module @compiled_controlnet {
  util.global private @__auto.controlnet.time_embedding.linear_1.weight = #stream.parameter.named<"model"::"controlnet.time_embedding.linear_1.weight"> : tensor<1280x320xf16>
  util.global private @__auto.controlnet.time_embedding.linear_1.bias = #stream.parameter.named<"model"::"controlnet.time_embedding.linear_1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.time_embedding.linear_2.weight = #stream.parameter.named<"model"::"controlnet.time_embedding.linear_2.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.time_embedding.linear_2.bias = #stream.parameter.named<"model"::"controlnet.time_embedding.linear_2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.add_embedding.linear_1.weight = #stream.parameter.named<"model"::"controlnet.add_embedding.linear_1.weight"> : tensor<1280x2816xf16>
  util.global private @__auto.controlnet.add_embedding.linear_1.bias = #stream.parameter.named<"model"::"controlnet.add_embedding.linear_1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.add_embedding.linear_2.weight = #stream.parameter.named<"model"::"controlnet.add_embedding.linear_2.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.add_embedding.linear_2.bias = #stream.parameter.named<"model"::"controlnet.add_embedding.linear_2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.conv_in.weight = #stream.parameter.named<"model"::"controlnet.conv_in.weight"> : tensor<320x4x3x3xf16>
  util.global private @__auto.controlnet.conv_in.bias = #stream.parameter.named<"model"::"controlnet.conv_in.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.conv_in.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.conv_in.weight"> : tensor<16x3x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.conv_in.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.conv_in.bias"> : tensor<16xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.0.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.0.weight"> : tensor<16x16x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.0.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.0.bias"> : tensor<16xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.1.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.1.weight"> : tensor<32x16x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.1.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.1.bias"> : tensor<32xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.2.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.2.weight"> : tensor<32x32x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.2.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.2.bias"> : tensor<32xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.3.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.3.weight"> : tensor<96x32x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.3.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.3.bias"> : tensor<96xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.4.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.4.weight"> : tensor<96x96x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.4.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.4.bias"> : tensor<96xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.5.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.5.weight"> : tensor<256x96x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.blocks.5.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.blocks.5.bias"> : tensor<256xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.conv_out.weight = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.conv_out.weight"> : tensor<320x256x3x3xf16>
  util.global private @__auto.controlnet.controlnet_cond_embedding.conv_out.bias = #stream.parameter.named<"model"::"controlnet.controlnet_cond_embedding.conv_out.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.norm1.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.norm1.weight"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.conv1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.conv1.weight"> : tensor<320x320x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.conv1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.conv1.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.norm2.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.norm2.weight"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.conv2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.0.conv2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.0.conv2.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.norm1.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.norm1.weight"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.conv1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.conv1.weight"> : tensor<320x320x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.conv1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.conv1.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.time_emb_proj.weight"> : tensor<320x1280xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.time_emb_proj.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.norm2.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.norm2.weight"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.conv2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.conv2.weight"> : tensor<320x320x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.0.resnets.1.conv2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.resnets.1.conv2.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.0.downsamplers.0.conv.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.0.downsamplers.0.conv.weight"> : tensor<320x320x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.0.downsamplers.0.conv.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.0.downsamplers.0.conv.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.norm1.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.norm1.weight"> : tensor<320xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.conv1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.conv1.weight"> : tensor<640x320x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.conv1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.conv1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.norm2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.norm2.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.conv2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.conv2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.conv2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.conv_shortcut.weight"> : tensor<640x320x1x1xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.0.conv_shortcut.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.norm.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.norm.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.norm.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.norm.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.proj_in.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.proj_in.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.proj_in.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.proj_in.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.proj_out.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.proj_out.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.0.proj_out.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.0.proj_out.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.norm1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.norm1.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.conv1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.conv1.weight"> : tensor<640x640x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.conv1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.conv1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.time_emb_proj.weight"> : tensor<640x1280xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.time_emb_proj.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.norm2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.norm2.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.conv2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.conv2.weight"> : tensor<640x640x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.1.resnets.1.conv2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.resnets.1.conv2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.norm.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.norm.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.norm.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.norm.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.proj_in.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.proj_in.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.proj_in.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.proj_in.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<640x2048xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<5120x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<5120xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<640x2560xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.proj_out.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.proj_out.weight"> : tensor<640x640xf16>
  util.global private @__auto.controlnet.down_blocks.1.attentions.1.proj_out.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.attentions.1.proj_out.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.1.downsamplers.0.conv.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.1.downsamplers.0.conv.weight"> : tensor<640x640x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.1.downsamplers.0.conv.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.1.downsamplers.0.conv.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.norm1.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.norm1.weight"> : tensor<640xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.conv1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.conv1.weight"> : tensor<1280x640x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.conv1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.conv2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.conv2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.conv_shortcut.weight"> : tensor<1280x640x1x1xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.0.conv_shortcut.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.norm.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.norm.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.proj_in.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.proj_in.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.proj_out.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.0.proj_out.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.conv1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.conv1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.conv2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.down_blocks.2.resnets.1.conv2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.norm.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.norm.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.norm.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.norm.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.proj_in.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.proj_in.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.proj_in.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.proj_out.weight = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.down_blocks.2.attentions.1.proj_out.bias = #stream.parameter.named<"model"::"controlnet.down_blocks.2.attentions.1.proj_out.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.conv1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.conv1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.conv1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.conv2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.mid_block.resnets.0.conv2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.0.conv2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.norm.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.norm.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.norm.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.norm.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.proj_in.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.proj_in.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.proj_in.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.proj_in.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight"> : tensor<1280x2048xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight"> : tensor<10240x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias"> : tensor<10240xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight"> : tensor<1280x5120xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.proj_out.weight = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.proj_out.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.attentions.0.proj_out.bias = #stream.parameter.named<"model"::"controlnet.mid_block.attentions.0.proj_out.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.norm1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.norm1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.norm1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.norm1.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.conv1.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.conv1.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.conv1.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.conv1.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.time_emb_proj.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.time_emb_proj.weight"> : tensor<1280x1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.time_emb_proj.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.time_emb_proj.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.norm2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.norm2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.norm2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.norm2.weight"> : tensor<1280xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.conv2.weight = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.conv2.weight"> : tensor<1280x1280x3x3xf16>
  util.global private @__auto.controlnet.mid_block.resnets.1.conv2.bias = #stream.parameter.named<"model"::"controlnet.mid_block.resnets.1.conv2.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.0.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.0.weight"> : tensor<320x320x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.0.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.0.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.1.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.1.weight"> : tensor<320x320x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.1.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.1.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.2.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.2.weight"> : tensor<320x320x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.2.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.2.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.3.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.3.weight"> : tensor<320x320x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.3.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.3.bias"> : tensor<320xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.4.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.4.weight"> : tensor<640x640x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.4.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.4.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.5.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.5.weight"> : tensor<640x640x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.5.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.5.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.6.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.6.weight"> : tensor<640x640x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.6.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.6.bias"> : tensor<640xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.7.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.7.weight"> : tensor<1280x1280x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.7.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.7.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.8.weight = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.8.weight"> : tensor<1280x1280x1x1xf16>
  util.global private @__auto.controlnet.controlnet_down_blocks.8.bias = #stream.parameter.named<"model"::"controlnet.controlnet_down_blocks.8.bias"> : tensor<1280xf16>
  util.global private @__auto.controlnet.controlnet_mid_block.weight = #stream.parameter.named<"model"::"controlnet.controlnet_mid_block.weight"> : tensor<1280x1280x1x1xf16>
  util.global private @__auto.controlnet.controlnet_mid_block.bias = #stream.parameter.named<"model"::"controlnet.controlnet_mid_block.bias"> : tensor<1280xf16>
  func.func @run_forward(%arg0: !torch.vtensor<[1,4,120,128],f16>, %arg1: !torch.vtensor<[1],f16>, %arg2: !torch.vtensor<[2,3,960,1024],f16>, %arg3: !torch.vtensor<[2,64,2048],f16>, %arg4: !torch.vtensor<[2,1280],f16>, %arg5: !torch.vtensor<[2,6],f16>, %arg6: !torch.vtensor<[2,16,2048],f16>, %arg7: !torch.vtensor<[1],f16>, %arg8: !torch.vtensor<[1],f16>) -> (!torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>) attributes {torch.assume_strict_symbolic_shapes} {
    %0 = torch.prim.ListConstruct %arg0, %arg0 : (!torch.vtensor<[1,4,120,128],f16>, !torch.vtensor<[1,4,120,128],f16>) -> !torch.list<vtensor>
    %int0 = torch.constant.int 0
    %1 = torch.aten.cat %0, %int0 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,4,120,128],f16>
    %int2 = torch.constant.int 2
    %2 = torch.prim.ListConstruct %int2 : (!torch.int) -> !torch.list<int>
    %false = torch.constant.bool false
    %3 = torch.aten.expand %arg1, %2, %false : !torch.vtensor<[1],f16>, !torch.list<int>, !torch.bool -> !torch.vtensor<[2],f16>
    %int0_0 = torch.constant.int 0
    %int160 = torch.constant.int 160
    %int6 = torch.constant.int 6
    %none = torch.constant.none
    %cpu = torch.constant.device "cpu"
    %false_1 = torch.constant.bool false
    %4 = torch.aten.arange.start %int0_0, %int160, %int6, %none, %cpu, %false_1 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[160],f32>
    %float-9.210340e00 = torch.constant.float -9.2103403719761836
    %5 = torch.aten.mul.Scalar %4, %float-9.210340e00 : !torch.vtensor<[160],f32>, !torch.float -> !torch.vtensor<[160],f32>
    %int160_2 = torch.constant.int 160
    %6 = torch.aten.div.Scalar %5, %int160_2 : !torch.vtensor<[160],f32>, !torch.int -> !torch.vtensor<[160],f32>
    %7 = torch.aten.exp %6 : !torch.vtensor<[160],f32> -> !torch.vtensor<[160],f32>
    %int0_3 = torch.constant.int 0
    %int0_4 = torch.constant.int 0
    %int9223372036854775807 = torch.constant.int 9223372036854775807
    %int1 = torch.constant.int 1
    %8 = torch.aten.slice.Tensor %3, %int0_3, %int0_4, %int9223372036854775807, %int1 : !torch.vtensor<[2],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2],f16>
    %int1_5 = torch.constant.int 1
    %9 = torch.aten.unsqueeze %8, %int1_5 : !torch.vtensor<[2],f16>, !torch.int -> !torch.vtensor<[2,1],f16>
    %int6_6 = torch.constant.int 6
    %10 = torch.prims.convert_element_type %9, %int6_6 : !torch.vtensor<[2,1],f16>, !torch.int -> !torch.vtensor<[2,1],f32>
    %int0_7 = torch.constant.int 0
    %11 = torch.aten.unsqueeze %7, %int0_7 : !torch.vtensor<[160],f32>, !torch.int -> !torch.vtensor<[1,160],f32>
    %int1_8 = torch.constant.int 1
    %int0_9 = torch.constant.int 0
    %int9223372036854775807_10 = torch.constant.int 9223372036854775807
    %int1_11 = torch.constant.int 1
    %12 = torch.aten.slice.Tensor %11, %int1_8, %int0_9, %int9223372036854775807_10, %int1_11 : !torch.vtensor<[1,160],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,160],f32>
    %13 = torch.aten.mul.Tensor %10, %12 : !torch.vtensor<[2,1],f32>, !torch.vtensor<[1,160],f32> -> !torch.vtensor<[2,160],f32>
    %int1_12 = torch.constant.int 1
    %14 = torch.aten.mul.Scalar %13, %int1_12 : !torch.vtensor<[2,160],f32>, !torch.int -> !torch.vtensor<[2,160],f32>
    %15 = torch.aten.sin %14 : !torch.vtensor<[2,160],f32> -> !torch.vtensor<[2,160],f32>
    %16 = torch.aten.cos %14 : !torch.vtensor<[2,160],f32> -> !torch.vtensor<[2,160],f32>
    %17 = torch.prim.ListConstruct %15, %16 : (!torch.vtensor<[2,160],f32>, !torch.vtensor<[2,160],f32>) -> !torch.list<vtensor>
    %int-1 = torch.constant.int -1
    %18 = torch.aten.cat %17, %int-1 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int0_13 = torch.constant.int 0
    %int0_14 = torch.constant.int 0
    %int9223372036854775807_15 = torch.constant.int 9223372036854775807
    %int1_16 = torch.constant.int 1
    %19 = torch.aten.slice.Tensor %18, %int0_13, %int0_14, %int9223372036854775807_15, %int1_16 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,320],f32>
    %int1_17 = torch.constant.int 1
    %int160_18 = torch.constant.int 160
    %int9223372036854775807_19 = torch.constant.int 9223372036854775807
    %int1_20 = torch.constant.int 1
    %20 = torch.aten.slice.Tensor %19, %int1_17, %int160_18, %int9223372036854775807_19, %int1_20 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,160],f32>
    %int0_21 = torch.constant.int 0
    %int0_22 = torch.constant.int 0
    %int9223372036854775807_23 = torch.constant.int 9223372036854775807
    %int1_24 = torch.constant.int 1
    %21 = torch.aten.slice.Tensor %18, %int0_21, %int0_22, %int9223372036854775807_23, %int1_24 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,320],f32>
    %int1_25 = torch.constant.int 1
    %int0_26 = torch.constant.int 0
    %int160_27 = torch.constant.int 160
    %int1_28 = torch.constant.int 1
    %22 = torch.aten.slice.Tensor %21, %int1_25, %int0_26, %int160_27, %int1_28 : !torch.vtensor<[2,320],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,160],f32>
    %23 = torch.prim.ListConstruct %20, %22 : (!torch.vtensor<[2,160],f32>, !torch.vtensor<[2,160],f32>) -> !torch.list<vtensor>
    %int-1_29 = torch.constant.int -1
    %24 = torch.aten.cat %23, %int-1_29 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int5 = torch.constant.int 5
    %25 = torch.prims.convert_element_type %24, %int5 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %__auto.controlnet.time_embedding.linear_1.weight = util.global.load @__auto.controlnet.time_embedding.linear_1.weight : tensor<1280x320xf16>
    %26 = torch_c.from_builtin_tensor %__auto.controlnet.time_embedding.linear_1.weight : tensor<1280x320xf16> -> !torch.vtensor<[1280,320],f16>
    %int0_30 = torch.constant.int 0
    %int1_31 = torch.constant.int 1
    %27 = torch.aten.transpose.int %26, %int0_30, %int1_31 : !torch.vtensor<[1280,320],f16>, !torch.int, !torch.int -> !torch.vtensor<[320,1280],f16>
    %__auto.controlnet.time_embedding.linear_1.bias = util.global.load @__auto.controlnet.time_embedding.linear_1.bias : tensor<1280xf16>
    %28 = torch_c.from_builtin_tensor %__auto.controlnet.time_embedding.linear_1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_32 = torch.constant.int 6
    %29 = torch.prims.convert_element_type %28, %int6_32 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_33 = torch.constant.int 6
    %30 = torch.prims.convert_element_type %25, %int6_33 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int6_34 = torch.constant.int 6
    %31 = torch.prims.convert_element_type %27, %int6_34 : !torch.vtensor<[320,1280],f16>, !torch.int -> !torch.vtensor<[320,1280],f32>
    %32 = torch.aten.mm %30, %31 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_35 = torch.constant.int 1
    %33 = torch.aten.mul.Scalar %32, %int1_35 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_36 = torch.constant.int 1
    %34 = torch.aten.mul.Scalar %29, %int1_36 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_37 = torch.constant.int 1
    %35 = torch.aten.add.Tensor %33, %34, %int1_37 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_38 = torch.constant.int 5
    %36 = torch.prims.convert_element_type %35, %int5_38 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %37 = torch.aten.silu %36 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.time_embedding.linear_2.weight = util.global.load @__auto.controlnet.time_embedding.linear_2.weight : tensor<1280x1280xf16>
    %38 = torch_c.from_builtin_tensor %__auto.controlnet.time_embedding.linear_2.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_39 = torch.constant.int 0
    %int1_40 = torch.constant.int 1
    %39 = torch.aten.transpose.int %38, %int0_39, %int1_40 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.time_embedding.linear_2.bias = util.global.load @__auto.controlnet.time_embedding.linear_2.bias : tensor<1280xf16>
    %40 = torch_c.from_builtin_tensor %__auto.controlnet.time_embedding.linear_2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_41 = torch.constant.int 6
    %41 = torch.prims.convert_element_type %40, %int6_41 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_42 = torch.constant.int 6
    %42 = torch.prims.convert_element_type %37, %int6_42 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_43 = torch.constant.int 6
    %43 = torch.prims.convert_element_type %39, %int6_43 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %44 = torch.aten.mm %42, %43 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_44 = torch.constant.int 1
    %45 = torch.aten.mul.Scalar %44, %int1_44 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_45 = torch.constant.int 1
    %46 = torch.aten.mul.Scalar %41, %int1_45 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_46 = torch.constant.int 1
    %47 = torch.aten.add.Tensor %45, %46, %int1_46 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_47 = torch.constant.int 5
    %48 = torch.prims.convert_element_type %47, %int5_47 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int12 = torch.constant.int 12
    %49 = torch.prim.ListConstruct %int12 : (!torch.int) -> !torch.list<int>
    %50 = torch.aten.view %arg5, %49 : !torch.vtensor<[2,6],f16>, !torch.list<int> -> !torch.vtensor<[12],f16>
    %int0_48 = torch.constant.int 0
    %int128 = torch.constant.int 128
    %int6_49 = torch.constant.int 6
    %none_50 = torch.constant.none
    %cpu_51 = torch.constant.device "cpu"
    %false_52 = torch.constant.bool false
    %51 = torch.aten.arange.start %int0_48, %int128, %int6_49, %none_50, %cpu_51, %false_52 : !torch.int, !torch.int, !torch.int, !torch.none, !torch.Device, !torch.bool -> !torch.vtensor<[128],f32>
    %float-9.210340e00_53 = torch.constant.float -9.2103403719761836
    %52 = torch.aten.mul.Scalar %51, %float-9.210340e00_53 : !torch.vtensor<[128],f32>, !torch.float -> !torch.vtensor<[128],f32>
    %int128_54 = torch.constant.int 128
    %53 = torch.aten.div.Scalar %52, %int128_54 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[128],f32>
    %54 = torch.aten.exp %53 : !torch.vtensor<[128],f32> -> !torch.vtensor<[128],f32>
    %int0_55 = torch.constant.int 0
    %int0_56 = torch.constant.int 0
    %int9223372036854775807_57 = torch.constant.int 9223372036854775807
    %int1_58 = torch.constant.int 1
    %55 = torch.aten.slice.Tensor %50, %int0_55, %int0_56, %int9223372036854775807_57, %int1_58 : !torch.vtensor<[12],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12],f16>
    %int1_59 = torch.constant.int 1
    %56 = torch.aten.unsqueeze %55, %int1_59 : !torch.vtensor<[12],f16>, !torch.int -> !torch.vtensor<[12,1],f16>
    %int6_60 = torch.constant.int 6
    %57 = torch.prims.convert_element_type %56, %int6_60 : !torch.vtensor<[12,1],f16>, !torch.int -> !torch.vtensor<[12,1],f32>
    %int0_61 = torch.constant.int 0
    %58 = torch.aten.unsqueeze %54, %int0_61 : !torch.vtensor<[128],f32>, !torch.int -> !torch.vtensor<[1,128],f32>
    %int1_62 = torch.constant.int 1
    %int0_63 = torch.constant.int 0
    %int9223372036854775807_64 = torch.constant.int 9223372036854775807
    %int1_65 = torch.constant.int 1
    %59 = torch.aten.slice.Tensor %58, %int1_62, %int0_63, %int9223372036854775807_64, %int1_65 : !torch.vtensor<[1,128],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[1,128],f32>
    %60 = torch.aten.mul.Tensor %57, %59 : !torch.vtensor<[12,1],f32>, !torch.vtensor<[1,128],f32> -> !torch.vtensor<[12,128],f32>
    %int1_66 = torch.constant.int 1
    %61 = torch.aten.mul.Scalar %60, %int1_66 : !torch.vtensor<[12,128],f32>, !torch.int -> !torch.vtensor<[12,128],f32>
    %62 = torch.aten.sin %61 : !torch.vtensor<[12,128],f32> -> !torch.vtensor<[12,128],f32>
    %63 = torch.aten.cos %61 : !torch.vtensor<[12,128],f32> -> !torch.vtensor<[12,128],f32>
    %64 = torch.prim.ListConstruct %62, %63 : (!torch.vtensor<[12,128],f32>, !torch.vtensor<[12,128],f32>) -> !torch.list<vtensor>
    %int-1_67 = torch.constant.int -1
    %65 = torch.aten.cat %64, %int-1_67 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[12,256],f32>
    %int0_68 = torch.constant.int 0
    %int0_69 = torch.constant.int 0
    %int9223372036854775807_70 = torch.constant.int 9223372036854775807
    %int1_71 = torch.constant.int 1
    %66 = torch.aten.slice.Tensor %65, %int0_68, %int0_69, %int9223372036854775807_70, %int1_71 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,256],f32>
    %int1_72 = torch.constant.int 1
    %int128_73 = torch.constant.int 128
    %int9223372036854775807_74 = torch.constant.int 9223372036854775807
    %int1_75 = torch.constant.int 1
    %67 = torch.aten.slice.Tensor %66, %int1_72, %int128_73, %int9223372036854775807_74, %int1_75 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,128],f32>
    %int0_76 = torch.constant.int 0
    %int0_77 = torch.constant.int 0
    %int9223372036854775807_78 = torch.constant.int 9223372036854775807
    %int1_79 = torch.constant.int 1
    %68 = torch.aten.slice.Tensor %65, %int0_76, %int0_77, %int9223372036854775807_78, %int1_79 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,256],f32>
    %int1_80 = torch.constant.int 1
    %int0_81 = torch.constant.int 0
    %int128_82 = torch.constant.int 128
    %int1_83 = torch.constant.int 1
    %69 = torch.aten.slice.Tensor %68, %int1_80, %int0_81, %int128_82, %int1_83 : !torch.vtensor<[12,256],f32>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[12,128],f32>
    %70 = torch.prim.ListConstruct %67, %69 : (!torch.vtensor<[12,128],f32>, !torch.vtensor<[12,128],f32>) -> !torch.list<vtensor>
    %int-1_84 = torch.constant.int -1
    %71 = torch.aten.cat %70, %int-1_84 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[12,256],f32>
    %int2_85 = torch.constant.int 2
    %int-1_86 = torch.constant.int -1
    %72 = torch.prim.ListConstruct %int2_85, %int-1_86 : (!torch.int, !torch.int) -> !torch.list<int>
    %73 = torch.aten.view %71, %72 : !torch.vtensor<[12,256],f32>, !torch.list<int> -> !torch.vtensor<[2,1536],f32>
    %74 = torch.prim.ListConstruct %arg4, %73 : (!torch.vtensor<[2,1280],f16>, !torch.vtensor<[2,1536],f32>) -> !torch.list<vtensor>
    %int-1_87 = torch.constant.int -1
    %75 = torch.aten.cat %74, %int-1_87 : !torch.list<vtensor>, !torch.int -> !torch.vtensor<[2,2816],f32>
    %int5_88 = torch.constant.int 5
    %76 = torch.prims.convert_element_type %75, %int5_88 : !torch.vtensor<[2,2816],f32>, !torch.int -> !torch.vtensor<[2,2816],f16>
    %__auto.controlnet.add_embedding.linear_1.weight = util.global.load @__auto.controlnet.add_embedding.linear_1.weight : tensor<1280x2816xf16>
    %77 = torch_c.from_builtin_tensor %__auto.controlnet.add_embedding.linear_1.weight : tensor<1280x2816xf16> -> !torch.vtensor<[1280,2816],f16>
    %int0_89 = torch.constant.int 0
    %int1_90 = torch.constant.int 1
    %78 = torch.aten.transpose.int %77, %int0_89, %int1_90 : !torch.vtensor<[1280,2816],f16>, !torch.int, !torch.int -> !torch.vtensor<[2816,1280],f16>
    %__auto.controlnet.add_embedding.linear_1.bias = util.global.load @__auto.controlnet.add_embedding.linear_1.bias : tensor<1280xf16>
    %79 = torch_c.from_builtin_tensor %__auto.controlnet.add_embedding.linear_1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_91 = torch.constant.int 6
    %80 = torch.prims.convert_element_type %79, %int6_91 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_92 = torch.constant.int 6
    %81 = torch.prims.convert_element_type %76, %int6_92 : !torch.vtensor<[2,2816],f16>, !torch.int -> !torch.vtensor<[2,2816],f32>
    %int6_93 = torch.constant.int 6
    %82 = torch.prims.convert_element_type %78, %int6_93 : !torch.vtensor<[2816,1280],f16>, !torch.int -> !torch.vtensor<[2816,1280],f32>
    %83 = torch.aten.mm %81, %82 : !torch.vtensor<[2,2816],f32>, !torch.vtensor<[2816,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_94 = torch.constant.int 1
    %84 = torch.aten.mul.Scalar %83, %int1_94 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_95 = torch.constant.int 1
    %85 = torch.aten.mul.Scalar %80, %int1_95 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_96 = torch.constant.int 1
    %86 = torch.aten.add.Tensor %84, %85, %int1_96 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_97 = torch.constant.int 5
    %87 = torch.prims.convert_element_type %86, %int5_97 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %88 = torch.aten.silu %87 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.add_embedding.linear_2.weight = util.global.load @__auto.controlnet.add_embedding.linear_2.weight : tensor<1280x1280xf16>
    %89 = torch_c.from_builtin_tensor %__auto.controlnet.add_embedding.linear_2.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_98 = torch.constant.int 0
    %int1_99 = torch.constant.int 1
    %90 = torch.aten.transpose.int %89, %int0_98, %int1_99 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.add_embedding.linear_2.bias = util.global.load @__auto.controlnet.add_embedding.linear_2.bias : tensor<1280xf16>
    %91 = torch_c.from_builtin_tensor %__auto.controlnet.add_embedding.linear_2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_100 = torch.constant.int 6
    %92 = torch.prims.convert_element_type %91, %int6_100 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_101 = torch.constant.int 6
    %93 = torch.prims.convert_element_type %88, %int6_101 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_102 = torch.constant.int 6
    %94 = torch.prims.convert_element_type %90, %int6_102 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %95 = torch.aten.mm %93, %94 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_103 = torch.constant.int 1
    %96 = torch.aten.mul.Scalar %95, %int1_103 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_104 = torch.constant.int 1
    %97 = torch.aten.mul.Scalar %92, %int1_104 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_105 = torch.constant.int 1
    %98 = torch.aten.add.Tensor %96, %97, %int1_105 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_106 = torch.constant.int 5
    %99 = torch.prims.convert_element_type %98, %int5_106 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int1_107 = torch.constant.int 1
    %100 = torch.aten.add.Tensor %48, %99, %int1_107 : !torch.vtensor<[2,1280],f16>, !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.conv_in.weight = util.global.load @__auto.controlnet.conv_in.weight : tensor<320x4x3x3xf16>
    %101 = torch_c.from_builtin_tensor %__auto.controlnet.conv_in.weight : tensor<320x4x3x3xf16> -> !torch.vtensor<[320,4,3,3],f16>
    %__auto.controlnet.conv_in.bias = util.global.load @__auto.controlnet.conv_in.bias : tensor<320xf16>
    %102 = torch_c.from_builtin_tensor %__auto.controlnet.conv_in.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_108 = torch.constant.int 1
    %int1_109 = torch.constant.int 1
    %103 = torch.prim.ListConstruct %int1_108, %int1_109 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_110 = torch.constant.int 1
    %int1_111 = torch.constant.int 1
    %104 = torch.prim.ListConstruct %int1_110, %int1_111 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_112 = torch.constant.int 1
    %int1_113 = torch.constant.int 1
    %105 = torch.prim.ListConstruct %int1_112, %int1_113 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_114 = torch.constant.bool false
    %int0_115 = torch.constant.int 0
    %int0_116 = torch.constant.int 0
    %106 = torch.prim.ListConstruct %int0_115, %int0_116 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_117 = torch.constant.int 1
    %107 = torch.aten.convolution %1, %101, %102, %103, %104, %105, %false_114, %106, %int1_117 : !torch.vtensor<[2,4,120,128],f16>, !torch.vtensor<[320,4,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.controlnet_cond_embedding.conv_in.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.conv_in.weight : tensor<16x3x3x3xf16>
    %108 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.conv_in.weight : tensor<16x3x3x3xf16> -> !torch.vtensor<[16,3,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.conv_in.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.conv_in.bias : tensor<16xf16>
    %109 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.conv_in.bias : tensor<16xf16> -> !torch.vtensor<[16],f16>
    %int1_118 = torch.constant.int 1
    %int1_119 = torch.constant.int 1
    %110 = torch.prim.ListConstruct %int1_118, %int1_119 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_120 = torch.constant.int 1
    %int1_121 = torch.constant.int 1
    %111 = torch.prim.ListConstruct %int1_120, %int1_121 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_122 = torch.constant.int 1
    %int1_123 = torch.constant.int 1
    %112 = torch.prim.ListConstruct %int1_122, %int1_123 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_124 = torch.constant.bool false
    %int0_125 = torch.constant.int 0
    %int0_126 = torch.constant.int 0
    %113 = torch.prim.ListConstruct %int0_125, %int0_126 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_127 = torch.constant.int 1
    %114 = torch.aten.convolution %arg2, %108, %109, %110, %111, %112, %false_124, %113, %int1_127 : !torch.vtensor<[2,3,960,1024],f16>, !torch.vtensor<[16,3,3,3],f16>, !torch.vtensor<[16],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,16,960,1024],f16>
    %115 = torch.aten.silu %114 : !torch.vtensor<[2,16,960,1024],f16> -> !torch.vtensor<[2,16,960,1024],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.0.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.0.weight : tensor<16x16x3x3xf16>
    %116 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.0.weight : tensor<16x16x3x3xf16> -> !torch.vtensor<[16,16,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.0.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.0.bias : tensor<16xf16>
    %117 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.0.bias : tensor<16xf16> -> !torch.vtensor<[16],f16>
    %int1_128 = torch.constant.int 1
    %int1_129 = torch.constant.int 1
    %118 = torch.prim.ListConstruct %int1_128, %int1_129 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_130 = torch.constant.int 1
    %int1_131 = torch.constant.int 1
    %119 = torch.prim.ListConstruct %int1_130, %int1_131 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_132 = torch.constant.int 1
    %int1_133 = torch.constant.int 1
    %120 = torch.prim.ListConstruct %int1_132, %int1_133 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_134 = torch.constant.bool false
    %int0_135 = torch.constant.int 0
    %int0_136 = torch.constant.int 0
    %121 = torch.prim.ListConstruct %int0_135, %int0_136 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_137 = torch.constant.int 1
    %122 = torch.aten.convolution %115, %116, %117, %118, %119, %120, %false_134, %121, %int1_137 : !torch.vtensor<[2,16,960,1024],f16>, !torch.vtensor<[16,16,3,3],f16>, !torch.vtensor<[16],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,16,960,1024],f16>
    %123 = torch.aten.silu %122 : !torch.vtensor<[2,16,960,1024],f16> -> !torch.vtensor<[2,16,960,1024],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.1.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.1.weight : tensor<32x16x3x3xf16>
    %124 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.1.weight : tensor<32x16x3x3xf16> -> !torch.vtensor<[32,16,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.1.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.1.bias : tensor<32xf16>
    %125 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.1.bias : tensor<32xf16> -> !torch.vtensor<[32],f16>
    %int2_138 = torch.constant.int 2
    %int2_139 = torch.constant.int 2
    %126 = torch.prim.ListConstruct %int2_138, %int2_139 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_140 = torch.constant.int 1
    %int1_141 = torch.constant.int 1
    %127 = torch.prim.ListConstruct %int1_140, %int1_141 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_142 = torch.constant.int 1
    %int1_143 = torch.constant.int 1
    %128 = torch.prim.ListConstruct %int1_142, %int1_143 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_144 = torch.constant.bool false
    %int0_145 = torch.constant.int 0
    %int0_146 = torch.constant.int 0
    %129 = torch.prim.ListConstruct %int0_145, %int0_146 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_147 = torch.constant.int 1
    %130 = torch.aten.convolution %123, %124, %125, %126, %127, %128, %false_144, %129, %int1_147 : !torch.vtensor<[2,16,960,1024],f16>, !torch.vtensor<[32,16,3,3],f16>, !torch.vtensor<[32],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,32,480,512],f16>
    %131 = torch.aten.silu %130 : !torch.vtensor<[2,32,480,512],f16> -> !torch.vtensor<[2,32,480,512],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.2.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.2.weight : tensor<32x32x3x3xf16>
    %132 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.2.weight : tensor<32x32x3x3xf16> -> !torch.vtensor<[32,32,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.2.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.2.bias : tensor<32xf16>
    %133 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.2.bias : tensor<32xf16> -> !torch.vtensor<[32],f16>
    %int1_148 = torch.constant.int 1
    %int1_149 = torch.constant.int 1
    %134 = torch.prim.ListConstruct %int1_148, %int1_149 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_150 = torch.constant.int 1
    %int1_151 = torch.constant.int 1
    %135 = torch.prim.ListConstruct %int1_150, %int1_151 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_152 = torch.constant.int 1
    %int1_153 = torch.constant.int 1
    %136 = torch.prim.ListConstruct %int1_152, %int1_153 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_154 = torch.constant.bool false
    %int0_155 = torch.constant.int 0
    %int0_156 = torch.constant.int 0
    %137 = torch.prim.ListConstruct %int0_155, %int0_156 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_157 = torch.constant.int 1
    %138 = torch.aten.convolution %131, %132, %133, %134, %135, %136, %false_154, %137, %int1_157 : !torch.vtensor<[2,32,480,512],f16>, !torch.vtensor<[32,32,3,3],f16>, !torch.vtensor<[32],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,32,480,512],f16>
    %139 = torch.aten.silu %138 : !torch.vtensor<[2,32,480,512],f16> -> !torch.vtensor<[2,32,480,512],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.3.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.3.weight : tensor<96x32x3x3xf16>
    %140 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.3.weight : tensor<96x32x3x3xf16> -> !torch.vtensor<[96,32,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.3.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.3.bias : tensor<96xf16>
    %141 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.3.bias : tensor<96xf16> -> !torch.vtensor<[96],f16>
    %int2_158 = torch.constant.int 2
    %int2_159 = torch.constant.int 2
    %142 = torch.prim.ListConstruct %int2_158, %int2_159 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_160 = torch.constant.int 1
    %int1_161 = torch.constant.int 1
    %143 = torch.prim.ListConstruct %int1_160, %int1_161 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_162 = torch.constant.int 1
    %int1_163 = torch.constant.int 1
    %144 = torch.prim.ListConstruct %int1_162, %int1_163 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_164 = torch.constant.bool false
    %int0_165 = torch.constant.int 0
    %int0_166 = torch.constant.int 0
    %145 = torch.prim.ListConstruct %int0_165, %int0_166 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_167 = torch.constant.int 1
    %146 = torch.aten.convolution %139, %140, %141, %142, %143, %144, %false_164, %145, %int1_167 : !torch.vtensor<[2,32,480,512],f16>, !torch.vtensor<[96,32,3,3],f16>, !torch.vtensor<[96],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,96,240,256],f16>
    %147 = torch.aten.silu %146 : !torch.vtensor<[2,96,240,256],f16> -> !torch.vtensor<[2,96,240,256],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.4.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.4.weight : tensor<96x96x3x3xf16>
    %148 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.4.weight : tensor<96x96x3x3xf16> -> !torch.vtensor<[96,96,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.4.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.4.bias : tensor<96xf16>
    %149 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.4.bias : tensor<96xf16> -> !torch.vtensor<[96],f16>
    %int1_168 = torch.constant.int 1
    %int1_169 = torch.constant.int 1
    %150 = torch.prim.ListConstruct %int1_168, %int1_169 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_170 = torch.constant.int 1
    %int1_171 = torch.constant.int 1
    %151 = torch.prim.ListConstruct %int1_170, %int1_171 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_172 = torch.constant.int 1
    %int1_173 = torch.constant.int 1
    %152 = torch.prim.ListConstruct %int1_172, %int1_173 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_174 = torch.constant.bool false
    %int0_175 = torch.constant.int 0
    %int0_176 = torch.constant.int 0
    %153 = torch.prim.ListConstruct %int0_175, %int0_176 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_177 = torch.constant.int 1
    %154 = torch.aten.convolution %147, %148, %149, %150, %151, %152, %false_174, %153, %int1_177 : !torch.vtensor<[2,96,240,256],f16>, !torch.vtensor<[96,96,3,3],f16>, !torch.vtensor<[96],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,96,240,256],f16>
    %155 = torch.aten.silu %154 : !torch.vtensor<[2,96,240,256],f16> -> !torch.vtensor<[2,96,240,256],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.5.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.5.weight : tensor<256x96x3x3xf16>
    %156 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.5.weight : tensor<256x96x3x3xf16> -> !torch.vtensor<[256,96,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.blocks.5.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.blocks.5.bias : tensor<256xf16>
    %157 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.blocks.5.bias : tensor<256xf16> -> !torch.vtensor<[256],f16>
    %int2_178 = torch.constant.int 2
    %int2_179 = torch.constant.int 2
    %158 = torch.prim.ListConstruct %int2_178, %int2_179 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_180 = torch.constant.int 1
    %int1_181 = torch.constant.int 1
    %159 = torch.prim.ListConstruct %int1_180, %int1_181 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_182 = torch.constant.int 1
    %int1_183 = torch.constant.int 1
    %160 = torch.prim.ListConstruct %int1_182, %int1_183 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_184 = torch.constant.bool false
    %int0_185 = torch.constant.int 0
    %int0_186 = torch.constant.int 0
    %161 = torch.prim.ListConstruct %int0_185, %int0_186 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_187 = torch.constant.int 1
    %162 = torch.aten.convolution %155, %156, %157, %158, %159, %160, %false_184, %161, %int1_187 : !torch.vtensor<[2,96,240,256],f16>, !torch.vtensor<[256,96,3,3],f16>, !torch.vtensor<[256],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,256,120,128],f16>
    %163 = torch.aten.silu %162 : !torch.vtensor<[2,256,120,128],f16> -> !torch.vtensor<[2,256,120,128],f16>
    %__auto.controlnet.controlnet_cond_embedding.conv_out.weight = util.global.load @__auto.controlnet.controlnet_cond_embedding.conv_out.weight : tensor<320x256x3x3xf16>
    %164 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.conv_out.weight : tensor<320x256x3x3xf16> -> !torch.vtensor<[320,256,3,3],f16>
    %__auto.controlnet.controlnet_cond_embedding.conv_out.bias = util.global.load @__auto.controlnet.controlnet_cond_embedding.conv_out.bias : tensor<320xf16>
    %165 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_cond_embedding.conv_out.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_188 = torch.constant.int 1
    %int1_189 = torch.constant.int 1
    %166 = torch.prim.ListConstruct %int1_188, %int1_189 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_190 = torch.constant.int 1
    %int1_191 = torch.constant.int 1
    %167 = torch.prim.ListConstruct %int1_190, %int1_191 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_192 = torch.constant.int 1
    %int1_193 = torch.constant.int 1
    %168 = torch.prim.ListConstruct %int1_192, %int1_193 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_194 = torch.constant.bool false
    %int0_195 = torch.constant.int 0
    %int0_196 = torch.constant.int 0
    %169 = torch.prim.ListConstruct %int0_195, %int0_196 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_197 = torch.constant.int 1
    %170 = torch.aten.convolution %163, %164, %165, %166, %167, %168, %false_194, %169, %int1_197 : !torch.vtensor<[2,256,120,128],f16>, !torch.vtensor<[320,256,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %int1_198 = torch.constant.int 1
    %171 = torch.aten.add.Tensor %107, %170, %int1_198 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %int2_199 = torch.constant.int 2
    %int32 = torch.constant.int 32
    %int10 = torch.constant.int 10
    %int15360 = torch.constant.int 15360
    %172 = torch.prim.ListConstruct %int2_199, %int32, %int10, %int15360 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %173 = torch.aten.view %171, %172 : !torch.vtensor<[2,320,120,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,15360],f16>
    %int6_200 = torch.constant.int 6
    %174 = torch.prims.convert_element_type %173, %int6_200 : !torch.vtensor<[2,32,10,15360],f16>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_201 = torch.constant.int 2
    %int3 = torch.constant.int 3
    %175 = torch.prim.ListConstruct %int2_201, %int3 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_202 = torch.constant.int 0
    %true = torch.constant.bool true
    %result0, %result1 = torch.aten.var_mean.correction %174, %175, %int0_202, %true : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05 = torch.constant.float 1.000000e-05
    %int1_203 = torch.constant.int 1
    %176 = torch.aten.add.Scalar %result0, %float1.000000e-05, %int1_203 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %177 = torch.aten.rsqrt %176 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_204 = torch.constant.int 1
    %178 = torch.aten.sub.Tensor %173, %result1, %int1_204 : !torch.vtensor<[2,32,10,15360],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %179 = torch.aten.mul.Tensor %178, %177 : !torch.vtensor<[2,32,10,15360],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_205 = torch.constant.int 2
    %int320 = torch.constant.int 320
    %int120 = torch.constant.int 120
    %int128_206 = torch.constant.int 128
    %180 = torch.prim.ListConstruct %int2_205, %int320, %int120, %int128_206 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %181 = torch.aten.view %179, %180 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int> -> !torch.vtensor<[2,320,120,128],f32>
    %__auto.controlnet.down_blocks.0.resnets.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.norm1.bias : tensor<320xf16>
    %182 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_207 = torch.constant.int 0
    %183 = torch.aten.unsqueeze %182, %int0_207 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_208 = torch.constant.int 2
    %184 = torch.aten.unsqueeze %183, %int2_208 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_209 = torch.constant.int 3
    %185 = torch.aten.unsqueeze %184, %int3_209 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.norm1.weight : tensor<320xf16>
    %186 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_210 = torch.constant.int 0
    %187 = torch.aten.unsqueeze %186, %int0_210 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_211 = torch.constant.int 2
    %188 = torch.aten.unsqueeze %187, %int2_211 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_212 = torch.constant.int 3
    %189 = torch.aten.unsqueeze %188, %int3_212 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %190 = torch.aten.mul.Tensor %181, %189 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,120,128],f32>
    %int1_213 = torch.constant.int 1
    %191 = torch.aten.add.Tensor %190, %185, %int1_213 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f32>
    %int5_214 = torch.constant.int 5
    %192 = torch.prims.convert_element_type %191, %int5_214 : !torch.vtensor<[2,320,120,128],f32>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %193 = torch.aten.silu %192 : !torch.vtensor<[2,320,120,128],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.conv1.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.conv1.weight : tensor<320x320x3x3xf16>
    %194 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.conv1.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.conv1.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.conv1.bias : tensor<320xf16>
    %195 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_215 = torch.constant.int 1
    %int1_216 = torch.constant.int 1
    %196 = torch.prim.ListConstruct %int1_215, %int1_216 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_217 = torch.constant.int 1
    %int1_218 = torch.constant.int 1
    %197 = torch.prim.ListConstruct %int1_217, %int1_218 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_219 = torch.constant.int 1
    %int1_220 = torch.constant.int 1
    %198 = torch.prim.ListConstruct %int1_219, %int1_220 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_221 = torch.constant.bool false
    %int0_222 = torch.constant.int 0
    %int0_223 = torch.constant.int 0
    %199 = torch.prim.ListConstruct %int0_222, %int0_223 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_224 = torch.constant.int 1
    %200 = torch.aten.convolution %193, %194, %195, %196, %197, %198, %false_221, %199, %int1_224 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %201 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.weight : tensor<320x1280xf16>
    %202 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %int0_225 = torch.constant.int 0
    %int1_226 = torch.constant.int 1
    %203 = torch.aten.transpose.int %202, %int0_225, %int1_226 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.bias : tensor<320xf16>
    %204 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int6_227 = torch.constant.int 6
    %205 = torch.prims.convert_element_type %204, %int6_227 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %int6_228 = torch.constant.int 6
    %206 = torch.prims.convert_element_type %201, %int6_228 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_229 = torch.constant.int 6
    %207 = torch.prims.convert_element_type %203, %int6_229 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %208 = torch.aten.mm %206, %207 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %int1_230 = torch.constant.int 1
    %209 = torch.aten.mul.Scalar %208, %int1_230 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int1_231 = torch.constant.int 1
    %210 = torch.aten.mul.Scalar %205, %int1_231 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %int1_232 = torch.constant.int 1
    %211 = torch.aten.add.Tensor %209, %210, %int1_232 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int5_233 = torch.constant.int 5
    %212 = torch.prims.convert_element_type %211, %int5_233 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %int0_234 = torch.constant.int 0
    %int0_235 = torch.constant.int 0
    %int9223372036854775807_236 = torch.constant.int 9223372036854775807
    %int1_237 = torch.constant.int 1
    %213 = torch.aten.slice.Tensor %212, %int0_234, %int0_235, %int9223372036854775807_236, %int1_237 : !torch.vtensor<[2,320],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,320],f16>
    %int1_238 = torch.constant.int 1
    %int0_239 = torch.constant.int 0
    %int9223372036854775807_240 = torch.constant.int 9223372036854775807
    %int1_241 = torch.constant.int 1
    %214 = torch.aten.slice.Tensor %213, %int1_238, %int0_239, %int9223372036854775807_240, %int1_241 : !torch.vtensor<[2,320],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,320],f16>
    %int2_242 = torch.constant.int 2
    %215 = torch.aten.unsqueeze %214, %int2_242 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %int3_243 = torch.constant.int 3
    %216 = torch.aten.unsqueeze %215, %int3_243 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %int1_244 = torch.constant.int 1
    %217 = torch.aten.add.Tensor %200, %216, %int1_244 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %int2_245 = torch.constant.int 2
    %int32_246 = torch.constant.int 32
    %int10_247 = torch.constant.int 10
    %int15360_248 = torch.constant.int 15360
    %218 = torch.prim.ListConstruct %int2_245, %int32_246, %int10_247, %int15360_248 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %219 = torch.aten.view %217, %218 : !torch.vtensor<[2,320,120,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,15360],f16>
    %int6_249 = torch.constant.int 6
    %220 = torch.prims.convert_element_type %219, %int6_249 : !torch.vtensor<[2,32,10,15360],f16>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_250 = torch.constant.int 2
    %int3_251 = torch.constant.int 3
    %221 = torch.prim.ListConstruct %int2_250, %int3_251 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_252 = torch.constant.int 0
    %true_253 = torch.constant.bool true
    %result0_254, %result1_255 = torch.aten.var_mean.correction %220, %221, %int0_252, %true_253 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_256 = torch.constant.float 1.000000e-05
    %int1_257 = torch.constant.int 1
    %222 = torch.aten.add.Scalar %result0_254, %float1.000000e-05_256, %int1_257 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %223 = torch.aten.rsqrt %222 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_258 = torch.constant.int 1
    %224 = torch.aten.sub.Tensor %219, %result1_255, %int1_258 : !torch.vtensor<[2,32,10,15360],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %225 = torch.aten.mul.Tensor %224, %223 : !torch.vtensor<[2,32,10,15360],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_259 = torch.constant.int 2
    %int320_260 = torch.constant.int 320
    %int120_261 = torch.constant.int 120
    %int128_262 = torch.constant.int 128
    %226 = torch.prim.ListConstruct %int2_259, %int320_260, %int120_261, %int128_262 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %227 = torch.aten.view %225, %226 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int> -> !torch.vtensor<[2,320,120,128],f32>
    %__auto.controlnet.down_blocks.0.resnets.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.norm2.bias : tensor<320xf16>
    %228 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_263 = torch.constant.int 0
    %229 = torch.aten.unsqueeze %228, %int0_263 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_264 = torch.constant.int 2
    %230 = torch.aten.unsqueeze %229, %int2_264 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_265 = torch.constant.int 3
    %231 = torch.aten.unsqueeze %230, %int3_265 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.norm2.weight : tensor<320xf16>
    %232 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_266 = torch.constant.int 0
    %233 = torch.aten.unsqueeze %232, %int0_266 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_267 = torch.constant.int 2
    %234 = torch.aten.unsqueeze %233, %int2_267 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_268 = torch.constant.int 3
    %235 = torch.aten.unsqueeze %234, %int3_268 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %236 = torch.aten.mul.Tensor %227, %235 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,120,128],f32>
    %int1_269 = torch.constant.int 1
    %237 = torch.aten.add.Tensor %236, %231, %int1_269 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f32>
    %int5_270 = torch.constant.int 5
    %238 = torch.prims.convert_element_type %237, %int5_270 : !torch.vtensor<[2,320,120,128],f32>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %239 = torch.aten.silu %238 : !torch.vtensor<[2,320,120,128],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %none_271 = torch.constant.none
    %240 = torch.aten.clone %239, %none_271 : !torch.vtensor<[2,320,120,128],f16>, !torch.none -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.conv2.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.conv2.weight : tensor<320x320x3x3xf16>
    %241 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %__auto.controlnet.down_blocks.0.resnets.0.conv2.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.0.conv2.bias : tensor<320xf16>
    %242 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.0.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_272 = torch.constant.int 1
    %int1_273 = torch.constant.int 1
    %243 = torch.prim.ListConstruct %int1_272, %int1_273 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_274 = torch.constant.int 1
    %int1_275 = torch.constant.int 1
    %244 = torch.prim.ListConstruct %int1_274, %int1_275 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_276 = torch.constant.int 1
    %int1_277 = torch.constant.int 1
    %245 = torch.prim.ListConstruct %int1_276, %int1_277 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_278 = torch.constant.bool false
    %int0_279 = torch.constant.int 0
    %int0_280 = torch.constant.int 0
    %246 = torch.prim.ListConstruct %int0_279, %int0_280 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_281 = torch.constant.int 1
    %247 = torch.aten.convolution %240, %241, %242, %243, %244, %245, %false_278, %246, %int1_281 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %int1_282 = torch.constant.int 1
    %248 = torch.aten.add.Tensor %171, %247, %int1_282 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %float1.000000e00 = torch.constant.float 1.000000e+00
    %249 = torch.aten.div.Scalar %248, %float1.000000e00 : !torch.vtensor<[2,320,120,128],f16>, !torch.float -> !torch.vtensor<[2,320,120,128],f16>
    %int2_283 = torch.constant.int 2
    %int32_284 = torch.constant.int 32
    %int10_285 = torch.constant.int 10
    %int15360_286 = torch.constant.int 15360
    %250 = torch.prim.ListConstruct %int2_283, %int32_284, %int10_285, %int15360_286 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %251 = torch.aten.view %249, %250 : !torch.vtensor<[2,320,120,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,15360],f16>
    %int6_287 = torch.constant.int 6
    %252 = torch.prims.convert_element_type %251, %int6_287 : !torch.vtensor<[2,32,10,15360],f16>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_288 = torch.constant.int 2
    %int3_289 = torch.constant.int 3
    %253 = torch.prim.ListConstruct %int2_288, %int3_289 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_290 = torch.constant.int 0
    %true_291 = torch.constant.bool true
    %result0_292, %result1_293 = torch.aten.var_mean.correction %252, %253, %int0_290, %true_291 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_294 = torch.constant.float 1.000000e-05
    %int1_295 = torch.constant.int 1
    %254 = torch.aten.add.Scalar %result0_292, %float1.000000e-05_294, %int1_295 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %255 = torch.aten.rsqrt %254 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_296 = torch.constant.int 1
    %256 = torch.aten.sub.Tensor %251, %result1_293, %int1_296 : !torch.vtensor<[2,32,10,15360],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %257 = torch.aten.mul.Tensor %256, %255 : !torch.vtensor<[2,32,10,15360],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_297 = torch.constant.int 2
    %int320_298 = torch.constant.int 320
    %int120_299 = torch.constant.int 120
    %int128_300 = torch.constant.int 128
    %258 = torch.prim.ListConstruct %int2_297, %int320_298, %int120_299, %int128_300 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %259 = torch.aten.view %257, %258 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int> -> !torch.vtensor<[2,320,120,128],f32>
    %__auto.controlnet.down_blocks.0.resnets.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.norm1.bias : tensor<320xf16>
    %260 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_301 = torch.constant.int 0
    %261 = torch.aten.unsqueeze %260, %int0_301 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_302 = torch.constant.int 2
    %262 = torch.aten.unsqueeze %261, %int2_302 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_303 = torch.constant.int 3
    %263 = torch.aten.unsqueeze %262, %int3_303 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.norm1.weight : tensor<320xf16>
    %264 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_304 = torch.constant.int 0
    %265 = torch.aten.unsqueeze %264, %int0_304 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_305 = torch.constant.int 2
    %266 = torch.aten.unsqueeze %265, %int2_305 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_306 = torch.constant.int 3
    %267 = torch.aten.unsqueeze %266, %int3_306 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %268 = torch.aten.mul.Tensor %259, %267 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,120,128],f32>
    %int1_307 = torch.constant.int 1
    %269 = torch.aten.add.Tensor %268, %263, %int1_307 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f32>
    %int5_308 = torch.constant.int 5
    %270 = torch.prims.convert_element_type %269, %int5_308 : !torch.vtensor<[2,320,120,128],f32>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %271 = torch.aten.silu %270 : !torch.vtensor<[2,320,120,128],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.conv1.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.conv1.weight : tensor<320x320x3x3xf16>
    %272 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.conv1.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.conv1.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.conv1.bias : tensor<320xf16>
    %273 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.conv1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_309 = torch.constant.int 1
    %int1_310 = torch.constant.int 1
    %274 = torch.prim.ListConstruct %int1_309, %int1_310 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_311 = torch.constant.int 1
    %int1_312 = torch.constant.int 1
    %275 = torch.prim.ListConstruct %int1_311, %int1_312 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_313 = torch.constant.int 1
    %int1_314 = torch.constant.int 1
    %276 = torch.prim.ListConstruct %int1_313, %int1_314 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_315 = torch.constant.bool false
    %int0_316 = torch.constant.int 0
    %int0_317 = torch.constant.int 0
    %277 = torch.prim.ListConstruct %int0_316, %int0_317 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_318 = torch.constant.int 1
    %278 = torch.aten.convolution %271, %272, %273, %274, %275, %276, %false_315, %277, %int1_318 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %279 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.weight : tensor<320x1280xf16>
    %280 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.weight : tensor<320x1280xf16> -> !torch.vtensor<[320,1280],f16>
    %int0_319 = torch.constant.int 0
    %int1_320 = torch.constant.int 1
    %281 = torch.aten.transpose.int %280, %int0_319, %int1_320 : !torch.vtensor<[320,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,320],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.bias : tensor<320xf16>
    %282 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.time_emb_proj.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int6_321 = torch.constant.int 6
    %283 = torch.prims.convert_element_type %282, %int6_321 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[320],f32>
    %int6_322 = torch.constant.int 6
    %284 = torch.prims.convert_element_type %279, %int6_322 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_323 = torch.constant.int 6
    %285 = torch.prims.convert_element_type %281, %int6_323 : !torch.vtensor<[1280,320],f16>, !torch.int -> !torch.vtensor<[1280,320],f32>
    %286 = torch.aten.mm %284, %285 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,320],f32> -> !torch.vtensor<[2,320],f32>
    %int1_324 = torch.constant.int 1
    %287 = torch.aten.mul.Scalar %286, %int1_324 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int1_325 = torch.constant.int 1
    %288 = torch.aten.mul.Scalar %283, %int1_325 : !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[320],f32>
    %int1_326 = torch.constant.int 1
    %289 = torch.aten.add.Tensor %287, %288, %int1_326 : !torch.vtensor<[2,320],f32>, !torch.vtensor<[320],f32>, !torch.int -> !torch.vtensor<[2,320],f32>
    %int5_327 = torch.constant.int 5
    %290 = torch.prims.convert_element_type %289, %int5_327 : !torch.vtensor<[2,320],f32>, !torch.int -> !torch.vtensor<[2,320],f16>
    %int0_328 = torch.constant.int 0
    %int0_329 = torch.constant.int 0
    %int9223372036854775807_330 = torch.constant.int 9223372036854775807
    %int1_331 = torch.constant.int 1
    %291 = torch.aten.slice.Tensor %290, %int0_328, %int0_329, %int9223372036854775807_330, %int1_331 : !torch.vtensor<[2,320],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,320],f16>
    %int1_332 = torch.constant.int 1
    %int0_333 = torch.constant.int 0
    %int9223372036854775807_334 = torch.constant.int 9223372036854775807
    %int1_335 = torch.constant.int 1
    %292 = torch.aten.slice.Tensor %291, %int1_332, %int0_333, %int9223372036854775807_334, %int1_335 : !torch.vtensor<[2,320],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,320],f16>
    %int2_336 = torch.constant.int 2
    %293 = torch.aten.unsqueeze %292, %int2_336 : !torch.vtensor<[2,320],f16>, !torch.int -> !torch.vtensor<[2,320,1],f16>
    %int3_337 = torch.constant.int 3
    %294 = torch.aten.unsqueeze %293, %int3_337 : !torch.vtensor<[2,320,1],f16>, !torch.int -> !torch.vtensor<[2,320,1,1],f16>
    %int1_338 = torch.constant.int 1
    %295 = torch.aten.add.Tensor %278, %294, %int1_338 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %int2_339 = torch.constant.int 2
    %int32_340 = torch.constant.int 32
    %int10_341 = torch.constant.int 10
    %int15360_342 = torch.constant.int 15360
    %296 = torch.prim.ListConstruct %int2_339, %int32_340, %int10_341, %int15360_342 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %297 = torch.aten.view %295, %296 : !torch.vtensor<[2,320,120,128],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,15360],f16>
    %int6_343 = torch.constant.int 6
    %298 = torch.prims.convert_element_type %297, %int6_343 : !torch.vtensor<[2,32,10,15360],f16>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_344 = torch.constant.int 2
    %int3_345 = torch.constant.int 3
    %299 = torch.prim.ListConstruct %int2_344, %int3_345 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_346 = torch.constant.int 0
    %true_347 = torch.constant.bool true
    %result0_348, %result1_349 = torch.aten.var_mean.correction %298, %299, %int0_346, %true_347 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_350 = torch.constant.float 1.000000e-05
    %int1_351 = torch.constant.int 1
    %300 = torch.aten.add.Scalar %result0_348, %float1.000000e-05_350, %int1_351 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %301 = torch.aten.rsqrt %300 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_352 = torch.constant.int 1
    %302 = torch.aten.sub.Tensor %297, %result1_349, %int1_352 : !torch.vtensor<[2,32,10,15360],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,15360],f32>
    %303 = torch.aten.mul.Tensor %302, %301 : !torch.vtensor<[2,32,10,15360],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,15360],f32>
    %int2_353 = torch.constant.int 2
    %int320_354 = torch.constant.int 320
    %int120_355 = torch.constant.int 120
    %int128_356 = torch.constant.int 128
    %304 = torch.prim.ListConstruct %int2_353, %int320_354, %int120_355, %int128_356 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %305 = torch.aten.view %303, %304 : !torch.vtensor<[2,32,10,15360],f32>, !torch.list<int> -> !torch.vtensor<[2,320,120,128],f32>
    %__auto.controlnet.down_blocks.0.resnets.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.norm2.bias : tensor<320xf16>
    %306 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.norm2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_357 = torch.constant.int 0
    %307 = torch.aten.unsqueeze %306, %int0_357 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_358 = torch.constant.int 2
    %308 = torch.aten.unsqueeze %307, %int2_358 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_359 = torch.constant.int 3
    %309 = torch.aten.unsqueeze %308, %int3_359 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.norm2.weight : tensor<320xf16>
    %310 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.norm2.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_360 = torch.constant.int 0
    %311 = torch.aten.unsqueeze %310, %int0_360 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_361 = torch.constant.int 2
    %312 = torch.aten.unsqueeze %311, %int2_361 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_362 = torch.constant.int 3
    %313 = torch.aten.unsqueeze %312, %int3_362 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %314 = torch.aten.mul.Tensor %305, %313 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,120,128],f32>
    %int1_363 = torch.constant.int 1
    %315 = torch.aten.add.Tensor %314, %309, %int1_363 : !torch.vtensor<[2,320,120,128],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f32>
    %int5_364 = torch.constant.int 5
    %316 = torch.prims.convert_element_type %315, %int5_364 : !torch.vtensor<[2,320,120,128],f32>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %317 = torch.aten.silu %316 : !torch.vtensor<[2,320,120,128],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %none_365 = torch.constant.none
    %318 = torch.aten.clone %317, %none_365 : !torch.vtensor<[2,320,120,128],f16>, !torch.none -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.conv2.weight = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.conv2.weight : tensor<320x320x3x3xf16>
    %319 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.conv2.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %__auto.controlnet.down_blocks.0.resnets.1.conv2.bias = util.global.load @__auto.controlnet.down_blocks.0.resnets.1.conv2.bias : tensor<320xf16>
    %320 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.resnets.1.conv2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_366 = torch.constant.int 1
    %int1_367 = torch.constant.int 1
    %321 = torch.prim.ListConstruct %int1_366, %int1_367 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_368 = torch.constant.int 1
    %int1_369 = torch.constant.int 1
    %322 = torch.prim.ListConstruct %int1_368, %int1_369 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_370 = torch.constant.int 1
    %int1_371 = torch.constant.int 1
    %323 = torch.prim.ListConstruct %int1_370, %int1_371 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_372 = torch.constant.bool false
    %int0_373 = torch.constant.int 0
    %int0_374 = torch.constant.int 0
    %324 = torch.prim.ListConstruct %int0_373, %int0_374 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_375 = torch.constant.int 1
    %325 = torch.aten.convolution %318, %319, %320, %321, %322, %323, %false_372, %324, %int1_375 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %int1_376 = torch.constant.int 1
    %326 = torch.aten.add.Tensor %249, %325, %int1_376 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %float1.000000e00_377 = torch.constant.float 1.000000e+00
    %327 = torch.aten.div.Scalar %326, %float1.000000e00_377 : !torch.vtensor<[2,320,120,128],f16>, !torch.float -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.down_blocks.0.downsamplers.0.conv.weight = util.global.load @__auto.controlnet.down_blocks.0.downsamplers.0.conv.weight : tensor<320x320x3x3xf16>
    %328 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.downsamplers.0.conv.weight : tensor<320x320x3x3xf16> -> !torch.vtensor<[320,320,3,3],f16>
    %__auto.controlnet.down_blocks.0.downsamplers.0.conv.bias = util.global.load @__auto.controlnet.down_blocks.0.downsamplers.0.conv.bias : tensor<320xf16>
    %329 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.0.downsamplers.0.conv.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int2_378 = torch.constant.int 2
    %int2_379 = torch.constant.int 2
    %330 = torch.prim.ListConstruct %int2_378, %int2_379 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_380 = torch.constant.int 1
    %int1_381 = torch.constant.int 1
    %331 = torch.prim.ListConstruct %int1_380, %int1_381 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_382 = torch.constant.int 1
    %int1_383 = torch.constant.int 1
    %332 = torch.prim.ListConstruct %int1_382, %int1_383 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_384 = torch.constant.bool false
    %int0_385 = torch.constant.int 0
    %int0_386 = torch.constant.int 0
    %333 = torch.prim.ListConstruct %int0_385, %int0_386 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_387 = torch.constant.int 1
    %334 = torch.aten.convolution %327, %328, %329, %330, %331, %332, %false_384, %333, %int1_387 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,3,3],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,60,64],f16>
    %int2_388 = torch.constant.int 2
    %int32_389 = torch.constant.int 32
    %int10_390 = torch.constant.int 10
    %int3840 = torch.constant.int 3840
    %335 = torch.prim.ListConstruct %int2_388, %int32_389, %int10_390, %int3840 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %336 = torch.aten.view %334, %335 : !torch.vtensor<[2,320,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,10,3840],f16>
    %int6_391 = torch.constant.int 6
    %337 = torch.prims.convert_element_type %336, %int6_391 : !torch.vtensor<[2,32,10,3840],f16>, !torch.int -> !torch.vtensor<[2,32,10,3840],f32>
    %int2_392 = torch.constant.int 2
    %int3_393 = torch.constant.int 3
    %338 = torch.prim.ListConstruct %int2_392, %int3_393 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_394 = torch.constant.int 0
    %true_395 = torch.constant.bool true
    %result0_396, %result1_397 = torch.aten.var_mean.correction %337, %338, %int0_394, %true_395 : !torch.vtensor<[2,32,10,3840],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_398 = torch.constant.float 1.000000e-05
    %int1_399 = torch.constant.int 1
    %339 = torch.aten.add.Scalar %result0_396, %float1.000000e-05_398, %int1_399 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %340 = torch.aten.rsqrt %339 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_400 = torch.constant.int 1
    %341 = torch.aten.sub.Tensor %336, %result1_397, %int1_400 : !torch.vtensor<[2,32,10,3840],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,10,3840],f32>
    %342 = torch.aten.mul.Tensor %341, %340 : !torch.vtensor<[2,32,10,3840],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,10,3840],f32>
    %int2_401 = torch.constant.int 2
    %int320_402 = torch.constant.int 320
    %int60 = torch.constant.int 60
    %int64 = torch.constant.int 64
    %343 = torch.prim.ListConstruct %int2_401, %int320_402, %int60, %int64 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %344 = torch.aten.view %342, %343 : !torch.vtensor<[2,32,10,3840],f32>, !torch.list<int> -> !torch.vtensor<[2,320,60,64],f32>
    %__auto.controlnet.down_blocks.1.resnets.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.norm1.bias : tensor<320xf16>
    %345 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.norm1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_403 = torch.constant.int 0
    %346 = torch.aten.unsqueeze %345, %int0_403 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_404 = torch.constant.int 2
    %347 = torch.aten.unsqueeze %346, %int2_404 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_405 = torch.constant.int 3
    %348 = torch.aten.unsqueeze %347, %int3_405 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.norm1.weight : tensor<320xf16>
    %349 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.norm1.weight : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int0_406 = torch.constant.int 0
    %350 = torch.aten.unsqueeze %349, %int0_406 : !torch.vtensor<[320],f16>, !torch.int -> !torch.vtensor<[1,320],f16>
    %int2_407 = torch.constant.int 2
    %351 = torch.aten.unsqueeze %350, %int2_407 : !torch.vtensor<[1,320],f16>, !torch.int -> !torch.vtensor<[1,320,1],f16>
    %int3_408 = torch.constant.int 3
    %352 = torch.aten.unsqueeze %351, %int3_408 : !torch.vtensor<[1,320,1],f16>, !torch.int -> !torch.vtensor<[1,320,1,1],f16>
    %353 = torch.aten.mul.Tensor %344, %352 : !torch.vtensor<[2,320,60,64],f32>, !torch.vtensor<[1,320,1,1],f16> -> !torch.vtensor<[2,320,60,64],f32>
    %int1_409 = torch.constant.int 1
    %354 = torch.aten.add.Tensor %353, %348, %int1_409 : !torch.vtensor<[2,320,60,64],f32>, !torch.vtensor<[1,320,1,1],f16>, !torch.int -> !torch.vtensor<[2,320,60,64],f32>
    %int5_410 = torch.constant.int 5
    %355 = torch.prims.convert_element_type %354, %int5_410 : !torch.vtensor<[2,320,60,64],f32>, !torch.int -> !torch.vtensor<[2,320,60,64],f16>
    %356 = torch.aten.silu %355 : !torch.vtensor<[2,320,60,64],f16> -> !torch.vtensor<[2,320,60,64],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.conv1.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.conv1.weight : tensor<640x320x3x3xf16>
    %357 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.conv1.weight : tensor<640x320x3x3xf16> -> !torch.vtensor<[640,320,3,3],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.conv1.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.conv1.bias : tensor<640xf16>
    %358 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_411 = torch.constant.int 1
    %int1_412 = torch.constant.int 1
    %359 = torch.prim.ListConstruct %int1_411, %int1_412 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_413 = torch.constant.int 1
    %int1_414 = torch.constant.int 1
    %360 = torch.prim.ListConstruct %int1_413, %int1_414 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_415 = torch.constant.int 1
    %int1_416 = torch.constant.int 1
    %361 = torch.prim.ListConstruct %int1_415, %int1_416 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_417 = torch.constant.bool false
    %int0_418 = torch.constant.int 0
    %int0_419 = torch.constant.int 0
    %362 = torch.prim.ListConstruct %int0_418, %int0_419 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_420 = torch.constant.int 1
    %363 = torch.aten.convolution %356, %357, %358, %359, %360, %361, %false_417, %362, %int1_420 : !torch.vtensor<[2,320,60,64],f16>, !torch.vtensor<[640,320,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %364 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16>
    %365 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %int0_421 = torch.constant.int 0
    %int1_422 = torch.constant.int 1
    %366 = torch.aten.transpose.int %365, %int0_421, %int1_422 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16>
    %367 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_423 = torch.constant.int 6
    %368 = torch.prims.convert_element_type %367, %int6_423 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_424 = torch.constant.int 6
    %369 = torch.prims.convert_element_type %364, %int6_424 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_425 = torch.constant.int 6
    %370 = torch.prims.convert_element_type %366, %int6_425 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %371 = torch.aten.mm %369, %370 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %int1_426 = torch.constant.int 1
    %372 = torch.aten.mul.Scalar %371, %int1_426 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %int1_427 = torch.constant.int 1
    %373 = torch.aten.mul.Scalar %368, %int1_427 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_428 = torch.constant.int 1
    %374 = torch.aten.add.Tensor %372, %373, %int1_428 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %int5_429 = torch.constant.int 5
    %375 = torch.prims.convert_element_type %374, %int5_429 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %int0_430 = torch.constant.int 0
    %int0_431 = torch.constant.int 0
    %int9223372036854775807_432 = torch.constant.int 9223372036854775807
    %int1_433 = torch.constant.int 1
    %376 = torch.aten.slice.Tensor %375, %int0_430, %int0_431, %int9223372036854775807_432, %int1_433 : !torch.vtensor<[2,640],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,640],f16>
    %int1_434 = torch.constant.int 1
    %int0_435 = torch.constant.int 0
    %int9223372036854775807_436 = torch.constant.int 9223372036854775807
    %int1_437 = torch.constant.int 1
    %377 = torch.aten.slice.Tensor %376, %int1_434, %int0_435, %int9223372036854775807_436, %int1_437 : !torch.vtensor<[2,640],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,640],f16>
    %int2_438 = torch.constant.int 2
    %378 = torch.aten.unsqueeze %377, %int2_438 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %int3_439 = torch.constant.int 3
    %379 = torch.aten.unsqueeze %378, %int3_439 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %int1_440 = torch.constant.int 1
    %380 = torch.aten.add.Tensor %363, %379, %int1_440 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int2_441 = torch.constant.int 2
    %int32_442 = torch.constant.int 32
    %int20 = torch.constant.int 20
    %int3840_443 = torch.constant.int 3840
    %381 = torch.prim.ListConstruct %int2_441, %int32_442, %int20, %int3840_443 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %382 = torch.aten.view %380, %381 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,3840],f16>
    %int6_444 = torch.constant.int 6
    %383 = torch.prims.convert_element_type %382, %int6_444 : !torch.vtensor<[2,32,20,3840],f16>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_445 = torch.constant.int 2
    %int3_446 = torch.constant.int 3
    %384 = torch.prim.ListConstruct %int2_445, %int3_446 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_447 = torch.constant.int 0
    %true_448 = torch.constant.bool true
    %result0_449, %result1_450 = torch.aten.var_mean.correction %383, %384, %int0_447, %true_448 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_451 = torch.constant.float 1.000000e-05
    %int1_452 = torch.constant.int 1
    %385 = torch.aten.add.Scalar %result0_449, %float1.000000e-05_451, %int1_452 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %386 = torch.aten.rsqrt %385 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_453 = torch.constant.int 1
    %387 = torch.aten.sub.Tensor %382, %result1_450, %int1_453 : !torch.vtensor<[2,32,20,3840],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %388 = torch.aten.mul.Tensor %387, %386 : !torch.vtensor<[2,32,20,3840],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_454 = torch.constant.int 2
    %int640 = torch.constant.int 640
    %int60_455 = torch.constant.int 60
    %int64_456 = torch.constant.int 64
    %389 = torch.prim.ListConstruct %int2_454, %int640, %int60_455, %int64_456 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %390 = torch.aten.view %388, %389 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f32>
    %__auto.controlnet.down_blocks.1.resnets.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.norm2.bias : tensor<640xf16>
    %391 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_457 = torch.constant.int 0
    %392 = torch.aten.unsqueeze %391, %int0_457 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_458 = torch.constant.int 2
    %393 = torch.aten.unsqueeze %392, %int2_458 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_459 = torch.constant.int 3
    %394 = torch.aten.unsqueeze %393, %int3_459 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.norm2.weight : tensor<640xf16>
    %395 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_460 = torch.constant.int 0
    %396 = torch.aten.unsqueeze %395, %int0_460 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_461 = torch.constant.int 2
    %397 = torch.aten.unsqueeze %396, %int2_461 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_462 = torch.constant.int 3
    %398 = torch.aten.unsqueeze %397, %int3_462 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %399 = torch.aten.mul.Tensor %390, %398 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,60,64],f32>
    %int1_463 = torch.constant.int 1
    %400 = torch.aten.add.Tensor %399, %394, %int1_463 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f32>
    %int5_464 = torch.constant.int 5
    %401 = torch.prims.convert_element_type %400, %int5_464 : !torch.vtensor<[2,640,60,64],f32>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %402 = torch.aten.silu %401 : !torch.vtensor<[2,640,60,64],f16> -> !torch.vtensor<[2,640,60,64],f16>
    %none_465 = torch.constant.none
    %403 = torch.aten.clone %402, %none_465 : !torch.vtensor<[2,640,60,64],f16>, !torch.none -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.conv2.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16>
    %404 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.conv2.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.conv2.bias : tensor<640xf16>
    %405 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_466 = torch.constant.int 1
    %int1_467 = torch.constant.int 1
    %406 = torch.prim.ListConstruct %int1_466, %int1_467 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_468 = torch.constant.int 1
    %int1_469 = torch.constant.int 1
    %407 = torch.prim.ListConstruct %int1_468, %int1_469 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_470 = torch.constant.int 1
    %int1_471 = torch.constant.int 1
    %408 = torch.prim.ListConstruct %int1_470, %int1_471 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_472 = torch.constant.bool false
    %int0_473 = torch.constant.int 0
    %int0_474 = torch.constant.int 0
    %409 = torch.prim.ListConstruct %int0_473, %int0_474 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_475 = torch.constant.int 1
    %410 = torch.aten.convolution %403, %404, %405, %406, %407, %408, %false_472, %409, %int1_475 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x320x1x1xf16>
    %411 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.weight : tensor<640x320x1x1xf16> -> !torch.vtensor<[640,320,1,1],f16>
    %__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16>
    %412 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.0.conv_shortcut.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_476 = torch.constant.int 1
    %int1_477 = torch.constant.int 1
    %413 = torch.prim.ListConstruct %int1_476, %int1_477 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_478 = torch.constant.int 0
    %int0_479 = torch.constant.int 0
    %414 = torch.prim.ListConstruct %int0_478, %int0_479 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_480 = torch.constant.int 1
    %int1_481 = torch.constant.int 1
    %415 = torch.prim.ListConstruct %int1_480, %int1_481 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_482 = torch.constant.bool false
    %int0_483 = torch.constant.int 0
    %int0_484 = torch.constant.int 0
    %416 = torch.prim.ListConstruct %int0_483, %int0_484 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_485 = torch.constant.int 1
    %417 = torch.aten.convolution %334, %411, %412, %413, %414, %415, %false_482, %416, %int1_485 : !torch.vtensor<[2,320,60,64],f16>, !torch.vtensor<[640,320,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int1_486 = torch.constant.int 1
    %418 = torch.aten.add.Tensor %417, %410, %int1_486 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %float1.000000e00_487 = torch.constant.float 1.000000e+00
    %419 = torch.aten.div.Scalar %418, %float1.000000e00_487 : !torch.vtensor<[2,640,60,64],f16>, !torch.float -> !torch.vtensor<[2,640,60,64],f16>
    %int2_488 = torch.constant.int 2
    %int32_489 = torch.constant.int 32
    %int20_490 = torch.constant.int 20
    %int3840_491 = torch.constant.int 3840
    %420 = torch.prim.ListConstruct %int2_488, %int32_489, %int20_490, %int3840_491 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %421 = torch.aten.view %419, %420 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,3840],f16>
    %int6_492 = torch.constant.int 6
    %422 = torch.prims.convert_element_type %421, %int6_492 : !torch.vtensor<[2,32,20,3840],f16>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_493 = torch.constant.int 2
    %int3_494 = torch.constant.int 3
    %423 = torch.prim.ListConstruct %int2_493, %int3_494 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_495 = torch.constant.int 0
    %true_496 = torch.constant.bool true
    %result0_497, %result1_498 = torch.aten.var_mean.correction %422, %423, %int0_495, %true_496 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float9.999990e-07 = torch.constant.float 9.9999999999999995E-7
    %int1_499 = torch.constant.int 1
    %424 = torch.aten.add.Scalar %result0_497, %float9.999990e-07, %int1_499 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %425 = torch.aten.rsqrt %424 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_500 = torch.constant.int 1
    %426 = torch.aten.sub.Tensor %421, %result1_498, %int1_500 : !torch.vtensor<[2,32,20,3840],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %427 = torch.aten.mul.Tensor %426, %425 : !torch.vtensor<[2,32,20,3840],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_501 = torch.constant.int 2
    %int640_502 = torch.constant.int 640
    %int60_503 = torch.constant.int 60
    %int64_504 = torch.constant.int 64
    %428 = torch.prim.ListConstruct %int2_501, %int640_502, %int60_503, %int64_504 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %429 = torch.aten.view %427, %428 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.norm.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.norm.bias : tensor<640xf16>
    %430 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_505 = torch.constant.int 0
    %431 = torch.aten.unsqueeze %430, %int0_505 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_506 = torch.constant.int 2
    %432 = torch.aten.unsqueeze %431, %int2_506 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_507 = torch.constant.int 3
    %433 = torch.aten.unsqueeze %432, %int3_507 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.norm.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.norm.weight : tensor<640xf16>
    %434 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_508 = torch.constant.int 0
    %435 = torch.aten.unsqueeze %434, %int0_508 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_509 = torch.constant.int 2
    %436 = torch.aten.unsqueeze %435, %int2_509 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_510 = torch.constant.int 3
    %437 = torch.aten.unsqueeze %436, %int3_510 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %438 = torch.aten.mul.Tensor %429, %437 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,60,64],f32>
    %int1_511 = torch.constant.int 1
    %439 = torch.aten.add.Tensor %438, %433, %int1_511 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f32>
    %int5_512 = torch.constant.int 5
    %440 = torch.prims.convert_element_type %439, %int5_512 : !torch.vtensor<[2,640,60,64],f32>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int0_513 = torch.constant.int 0
    %int2_514 = torch.constant.int 2
    %int3_515 = torch.constant.int 3
    %int1_516 = torch.constant.int 1
    %441 = torch.prim.ListConstruct %int0_513, %int2_514, %int3_515, %int1_516 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %442 = torch.aten.permute %440, %441 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,60,64,640],f16>
    %int2_517 = torch.constant.int 2
    %int3840_518 = torch.constant.int 3840
    %int640_519 = torch.constant.int 640
    %443 = torch.prim.ListConstruct %int2_517, %int3840_518, %int640_519 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %444 = torch.aten.view %442, %443 : !torch.vtensor<[2,60,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.proj_in.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16>
    %445 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_520 = torch.constant.int 0
    %int1_521 = torch.constant.int 1
    %446 = torch.aten.transpose.int %445, %int0_520, %int1_521 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int0_522 = torch.constant.int 0
    %447 = torch.aten.clone %444, %int0_522 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680 = torch.constant.int 7680
    %int640_523 = torch.constant.int 640
    %448 = torch.prim.ListConstruct %int7680, %int640_523 : (!torch.int, !torch.int) -> !torch.list<int>
    %449 = torch.aten._unsafe_view %447, %448 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %450 = torch.aten.mm %449, %446 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_524 = torch.constant.int 2
    %int3840_525 = torch.constant.int 3840
    %int640_526 = torch.constant.int 640
    %451 = torch.prim.ListConstruct %int2_524, %int3840_525, %int640_526 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %452 = torch.aten.view %450, %451 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.proj_in.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.proj_in.bias : tensor<640xf16>
    %453 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_527 = torch.constant.int 1
    %454 = torch.aten.add.Tensor %452, %453, %int1_527 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_528 = torch.constant.int 6
    %455 = torch.prims.convert_element_type %454, %int6_528 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_529 = torch.constant.int 2
    %456 = torch.prim.ListConstruct %int2_529 : (!torch.int) -> !torch.list<int>
    %int0_530 = torch.constant.int 0
    %true_531 = torch.constant.bool true
    %result0_532, %result1_533 = torch.aten.var_mean.correction %455, %456, %int0_530, %true_531 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_534 = torch.constant.float 1.000000e-05
    %int1_535 = torch.constant.int 1
    %457 = torch.aten.add.Scalar %result0_532, %float1.000000e-05_534, %int1_535 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %458 = torch.aten.rsqrt %457 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_536 = torch.constant.int 1
    %459 = torch.aten.sub.Tensor %454, %result1_533, %int1_536 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %460 = torch.aten.mul.Tensor %459, %458 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %461 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %462 = torch.aten.mul.Tensor %460, %461 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %463 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_537 = torch.constant.int 1
    %464 = torch.aten.add.Tensor %462, %463, %int1_537 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_538 = torch.constant.int 5
    %465 = torch.prims.convert_element_type %464, %int5_538 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %466 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_539 = torch.constant.int 0
    %int1_540 = torch.constant.int 1
    %467 = torch.aten.transpose.int %466, %int0_539, %int1_540 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_541 = torch.constant.int 7680
    %int640_542 = torch.constant.int 640
    %468 = torch.prim.ListConstruct %int7680_541, %int640_542 : (!torch.int, !torch.int) -> !torch.list<int>
    %469 = torch.aten.view %465, %468 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %470 = torch.aten.mm %469, %467 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_543 = torch.constant.int 2
    %int3840_544 = torch.constant.int 3840
    %int640_545 = torch.constant.int 640
    %471 = torch.prim.ListConstruct %int2_543, %int3840_544, %int640_545 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %472 = torch.aten.view %470, %471 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %473 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_546 = torch.constant.int 0
    %int1_547 = torch.constant.int 1
    %474 = torch.aten.transpose.int %473, %int0_546, %int1_547 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_548 = torch.constant.int 7680
    %int640_549 = torch.constant.int 640
    %475 = torch.prim.ListConstruct %int7680_548, %int640_549 : (!torch.int, !torch.int) -> !torch.list<int>
    %476 = torch.aten.view %465, %475 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %477 = torch.aten.mm %476, %474 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_550 = torch.constant.int 2
    %int3840_551 = torch.constant.int 3840
    %int640_552 = torch.constant.int 640
    %478 = torch.prim.ListConstruct %int2_550, %int3840_551, %int640_552 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %479 = torch.aten.view %477, %478 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %480 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_553 = torch.constant.int 0
    %int1_554 = torch.constant.int 1
    %481 = torch.aten.transpose.int %480, %int0_553, %int1_554 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_555 = torch.constant.int 7680
    %int640_556 = torch.constant.int 640
    %482 = torch.prim.ListConstruct %int7680_555, %int640_556 : (!torch.int, !torch.int) -> !torch.list<int>
    %483 = torch.aten.view %465, %482 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %484 = torch.aten.mm %483, %481 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_557 = torch.constant.int 2
    %int3840_558 = torch.constant.int 3840
    %int640_559 = torch.constant.int 640
    %485 = torch.prim.ListConstruct %int2_557, %int3840_558, %int640_559 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %486 = torch.aten.view %484, %485 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int2_560 = torch.constant.int 2
    %int-1_561 = torch.constant.int -1
    %int10_562 = torch.constant.int 10
    %int64_563 = torch.constant.int 64
    %487 = torch.prim.ListConstruct %int2_560, %int-1_561, %int10_562, %int64_563 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %488 = torch.aten.view %472, %487 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_564 = torch.constant.int 1
    %int2_565 = torch.constant.int 2
    %489 = torch.aten.transpose.int %488, %int1_564, %int2_565 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_566 = torch.constant.int 2
    %int-1_567 = torch.constant.int -1
    %int10_568 = torch.constant.int 10
    %int64_569 = torch.constant.int 64
    %490 = torch.prim.ListConstruct %int2_566, %int-1_567, %int10_568, %int64_569 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %491 = torch.aten.view %479, %490 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_570 = torch.constant.int 1
    %int2_571 = torch.constant.int 2
    %492 = torch.aten.transpose.int %491, %int1_570, %int2_571 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_572 = torch.constant.int 2
    %int-1_573 = torch.constant.int -1
    %int10_574 = torch.constant.int 10
    %int64_575 = torch.constant.int 64
    %493 = torch.prim.ListConstruct %int2_572, %int-1_573, %int10_574, %int64_575 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %494 = torch.aten.view %486, %493 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_576 = torch.constant.int 1
    %int2_577 = torch.constant.int 2
    %495 = torch.aten.transpose.int %494, %int1_576, %int2_577 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %float0.000000e00 = torch.constant.float 0.000000e+00
    %false_578 = torch.constant.bool false
    %none_579 = torch.constant.none
    %none_580 = torch.constant.none
    %496:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%489, %492, %495, %float0.000000e00, %false_578, %none_579, %none_580) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_581 = torch.constant.int 1
    %int2_582 = torch.constant.int 2
    %497 = torch.aten.transpose.int %496#0, %int1_581, %int2_582 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_583 = torch.constant.int 2
    %int-1_584 = torch.constant.int -1
    %int640_585 = torch.constant.int 640
    %498 = torch.prim.ListConstruct %int2_583, %int-1_584, %int640_585 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %499 = torch.aten.view %497, %498 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_586 = torch.constant.int 5
    %500 = torch.prims.convert_element_type %499, %int5_586 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_587 = torch.constant.int 7680
    %int640_588 = torch.constant.int 640
    %501 = torch.prim.ListConstruct %int7680_587, %int640_588 : (!torch.int, !torch.int) -> !torch.list<int>
    %502 = torch.aten.view %500, %501 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %503 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_589 = torch.constant.int 0
    %int1_590 = torch.constant.int 1
    %504 = torch.aten.transpose.int %503, %int0_589, %int1_590 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %505 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_591 = torch.constant.int 6
    %506 = torch.prims.convert_element_type %505, %int6_591 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_592 = torch.constant.int 6
    %507 = torch.prims.convert_element_type %502, %int6_592 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_593 = torch.constant.int 6
    %508 = torch.prims.convert_element_type %504, %int6_593 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %509 = torch.aten.mm %507, %508 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_594 = torch.constant.int 1
    %510 = torch.aten.mul.Scalar %509, %int1_594 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_595 = torch.constant.int 1
    %511 = torch.aten.mul.Scalar %506, %int1_595 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_596 = torch.constant.int 1
    %512 = torch.aten.add.Tensor %510, %511, %int1_596 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_597 = torch.constant.int 5
    %513 = torch.prims.convert_element_type %512, %int5_597 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_598 = torch.constant.int 2
    %int3840_599 = torch.constant.int 3840
    %int640_600 = torch.constant.int 640
    %514 = torch.prim.ListConstruct %int2_598, %int3840_599, %int640_600 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %515 = torch.aten.view %513, %514 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_601 = torch.constant.none
    %516 = torch.aten.clone %515, %none_601 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_602 = torch.constant.float 1.000000e+00
    %517 = torch.aten.div.Scalar %516, %float1.000000e00_602 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_603 = torch.constant.int 1
    %518 = torch.aten.add.Tensor %517, %454, %int1_603 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_604 = torch.constant.int 6
    %519 = torch.prims.convert_element_type %518, %int6_604 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_605 = torch.constant.int 2
    %520 = torch.prim.ListConstruct %int2_605 : (!torch.int) -> !torch.list<int>
    %int0_606 = torch.constant.int 0
    %true_607 = torch.constant.bool true
    %result0_608, %result1_609 = torch.aten.var_mean.correction %519, %520, %int0_606, %true_607 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_610 = torch.constant.float 1.000000e-05
    %int1_611 = torch.constant.int 1
    %521 = torch.aten.add.Scalar %result0_608, %float1.000000e-05_610, %int1_611 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %522 = torch.aten.rsqrt %521 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_612 = torch.constant.int 1
    %523 = torch.aten.sub.Tensor %518, %result1_609, %int1_612 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %524 = torch.aten.mul.Tensor %523, %522 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %525 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %526 = torch.aten.mul.Tensor %524, %525 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %527 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_613 = torch.constant.int 1
    %528 = torch.aten.add.Tensor %526, %527, %int1_613 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_614 = torch.constant.int 5
    %529 = torch.prims.convert_element_type %528, %int5_614 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %530 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_615 = torch.constant.int 0
    %int1_616 = torch.constant.int 1
    %531 = torch.aten.transpose.int %530, %int0_615, %int1_616 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_617 = torch.constant.int 7680
    %int640_618 = torch.constant.int 640
    %532 = torch.prim.ListConstruct %int7680_617, %int640_618 : (!torch.int, !torch.int) -> !torch.list<int>
    %533 = torch.aten.view %529, %532 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %534 = torch.aten.mm %533, %531 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_619 = torch.constant.int 2
    %int3840_620 = torch.constant.int 3840
    %int640_621 = torch.constant.int 640
    %535 = torch.prim.ListConstruct %int2_619, %int3840_620, %int640_621 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %536 = torch.aten.view %534, %535 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %537 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_622 = torch.constant.int 0
    %int1_623 = torch.constant.int 1
    %538 = torch.aten.transpose.int %537, %int0_622, %int1_623 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_624 = torch.constant.int 32
    %int2048 = torch.constant.int 2048
    %539 = torch.prim.ListConstruct %int32_624, %int2048 : (!torch.int, !torch.int) -> !torch.list<int>
    %540 = torch.aten.view %arg6, %539 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %541 = torch.aten.mm %540, %538 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_625 = torch.constant.int 2
    %int16 = torch.constant.int 16
    %int640_626 = torch.constant.int 640
    %542 = torch.prim.ListConstruct %int2_625, %int16, %int640_626 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %543 = torch.aten.view %541, %542 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %544 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_627 = torch.constant.int 0
    %int1_628 = torch.constant.int 1
    %545 = torch.aten.transpose.int %544, %int0_627, %int1_628 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_629 = torch.constant.int 32
    %int2048_630 = torch.constant.int 2048
    %546 = torch.prim.ListConstruct %int32_629, %int2048_630 : (!torch.int, !torch.int) -> !torch.list<int>
    %547 = torch.aten.view %arg6, %546 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %548 = torch.aten.mm %547, %545 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_631 = torch.constant.int 2
    %int16_632 = torch.constant.int 16
    %int640_633 = torch.constant.int 640
    %549 = torch.prim.ListConstruct %int2_631, %int16_632, %int640_633 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %550 = torch.aten.view %548, %549 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %int2_634 = torch.constant.int 2
    %int-1_635 = torch.constant.int -1
    %int10_636 = torch.constant.int 10
    %int64_637 = torch.constant.int 64
    %551 = torch.prim.ListConstruct %int2_634, %int-1_635, %int10_636, %int64_637 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %552 = torch.aten.view %536, %551 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_638 = torch.constant.int 1
    %int2_639 = torch.constant.int 2
    %553 = torch.aten.transpose.int %552, %int1_638, %int2_639 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_640 = torch.constant.int 2
    %int-1_641 = torch.constant.int -1
    %int10_642 = torch.constant.int 10
    %int64_643 = torch.constant.int 64
    %554 = torch.prim.ListConstruct %int2_640, %int-1_641, %int10_642, %int64_643 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %555 = torch.aten.view %543, %554 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_644 = torch.constant.int 1
    %int2_645 = torch.constant.int 2
    %556 = torch.aten.transpose.int %555, %int1_644, %int2_645 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %int2_646 = torch.constant.int 2
    %int-1_647 = torch.constant.int -1
    %int10_648 = torch.constant.int 10
    %int64_649 = torch.constant.int 64
    %557 = torch.prim.ListConstruct %int2_646, %int-1_647, %int10_648, %int64_649 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %558 = torch.aten.view %550, %557 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_650 = torch.constant.int 1
    %int2_651 = torch.constant.int 2
    %559 = torch.aten.transpose.int %558, %int1_650, %int2_651 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %float0.000000e00_652 = torch.constant.float 0.000000e+00
    %false_653 = torch.constant.bool false
    %none_654 = torch.constant.none
    %none_655 = torch.constant.none
    %560:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%553, %556, %559, %float0.000000e00_652, %false_653, %none_654, %none_655) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_656 = torch.constant.int 1
    %int2_657 = torch.constant.int 2
    %561 = torch.aten.transpose.int %560#0, %int1_656, %int2_657 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_658 = torch.constant.int 2
    %int-1_659 = torch.constant.int -1
    %int640_660 = torch.constant.int 640
    %562 = torch.prim.ListConstruct %int2_658, %int-1_659, %int640_660 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %563 = torch.aten.view %561, %562 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_661 = torch.constant.int 5
    %564 = torch.prims.convert_element_type %563, %int5_661 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_662 = torch.constant.int 7680
    %int640_663 = torch.constant.int 640
    %565 = torch.prim.ListConstruct %int7680_662, %int640_663 : (!torch.int, !torch.int) -> !torch.list<int>
    %566 = torch.aten.view %564, %565 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %567 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_664 = torch.constant.int 0
    %int1_665 = torch.constant.int 1
    %568 = torch.aten.transpose.int %567, %int0_664, %int1_665 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %569 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_666 = torch.constant.int 6
    %570 = torch.prims.convert_element_type %569, %int6_666 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_667 = torch.constant.int 6
    %571 = torch.prims.convert_element_type %566, %int6_667 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_668 = torch.constant.int 6
    %572 = torch.prims.convert_element_type %568, %int6_668 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %573 = torch.aten.mm %571, %572 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_669 = torch.constant.int 1
    %574 = torch.aten.mul.Scalar %573, %int1_669 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_670 = torch.constant.int 1
    %575 = torch.aten.mul.Scalar %570, %int1_670 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_671 = torch.constant.int 1
    %576 = torch.aten.add.Tensor %574, %575, %int1_671 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_672 = torch.constant.int 5
    %577 = torch.prims.convert_element_type %576, %int5_672 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_673 = torch.constant.int 2
    %int3840_674 = torch.constant.int 3840
    %int640_675 = torch.constant.int 640
    %578 = torch.prim.ListConstruct %int2_673, %int3840_674, %int640_675 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %579 = torch.aten.view %577, %578 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_676 = torch.constant.none
    %580 = torch.aten.clone %579, %none_676 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_677 = torch.constant.float 1.000000e+00
    %581 = torch.aten.div.Scalar %580, %float1.000000e00_677 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_678 = torch.constant.int 1
    %582 = torch.aten.add.Tensor %581, %518, %int1_678 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_679 = torch.constant.int 6
    %583 = torch.prims.convert_element_type %582, %int6_679 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_680 = torch.constant.int 2
    %584 = torch.prim.ListConstruct %int2_680 : (!torch.int) -> !torch.list<int>
    %int0_681 = torch.constant.int 0
    %true_682 = torch.constant.bool true
    %result0_683, %result1_684 = torch.aten.var_mean.correction %583, %584, %int0_681, %true_682 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_685 = torch.constant.float 1.000000e-05
    %int1_686 = torch.constant.int 1
    %585 = torch.aten.add.Scalar %result0_683, %float1.000000e-05_685, %int1_686 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %586 = torch.aten.rsqrt %585 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_687 = torch.constant.int 1
    %587 = torch.aten.sub.Tensor %582, %result1_684, %int1_687 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %588 = torch.aten.mul.Tensor %587, %586 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %589 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %590 = torch.aten.mul.Tensor %588, %589 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %591 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_688 = torch.constant.int 1
    %592 = torch.aten.add.Tensor %590, %591, %int1_688 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_689 = torch.constant.int 5
    %593 = torch.prims.convert_element_type %592, %int5_689 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_690 = torch.constant.int 7680
    %int640_691 = torch.constant.int 640
    %594 = torch.prim.ListConstruct %int7680_690, %int640_691 : (!torch.int, !torch.int) -> !torch.list<int>
    %595 = torch.aten.view %593, %594 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %596 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %int0_692 = torch.constant.int 0
    %int1_693 = torch.constant.int 1
    %597 = torch.aten.transpose.int %596, %int0_692, %int1_693 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %598 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_694 = torch.constant.int 6
    %599 = torch.prims.convert_element_type %598, %int6_694 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_695 = torch.constant.int 6
    %600 = torch.prims.convert_element_type %595, %int6_695 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_696 = torch.constant.int 6
    %601 = torch.prims.convert_element_type %597, %int6_696 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %602 = torch.aten.mm %600, %601 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[7680,5120],f32>
    %int1_697 = torch.constant.int 1
    %603 = torch.aten.mul.Scalar %602, %int1_697 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int1_698 = torch.constant.int 1
    %604 = torch.aten.mul.Scalar %599, %int1_698 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_699 = torch.constant.int 1
    %605 = torch.aten.add.Tensor %603, %604, %int1_699 : !torch.vtensor<[7680,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int5_700 = torch.constant.int 5
    %606 = torch.prims.convert_element_type %605, %int5_700 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f16>
    %int2_701 = torch.constant.int 2
    %int3840_702 = torch.constant.int 3840
    %int5120 = torch.constant.int 5120
    %607 = torch.prim.ListConstruct %int2_701, %int3840_702, %int5120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %608 = torch.aten.view %606, %607 : !torch.vtensor<[7680,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,5120],f16>
    %int-1_703 = torch.constant.int -1
    %int0_704 = torch.constant.int 0
    %int2560 = torch.constant.int 2560
    %int1_705 = torch.constant.int 1
    %609 = torch.aten.slice.Tensor %608, %int-1_703, %int0_704, %int2560, %int1_705 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %int-1_706 = torch.constant.int -1
    %int2560_707 = torch.constant.int 2560
    %int5120_708 = torch.constant.int 5120
    %int1_709 = torch.constant.int 1
    %610 = torch.aten.slice.Tensor %608, %int-1_706, %int2560_707, %int5120_708, %int1_709 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %str = torch.constant.str "none"
    %611 = torch.aten.gelu %610, %str : !torch.vtensor<[2,3840,2560],f16>, !torch.str -> !torch.vtensor<[2,3840,2560],f16>
    %612 = torch.aten.mul.Tensor %609, %611 : !torch.vtensor<[2,3840,2560],f16>, !torch.vtensor<[2,3840,2560],f16> -> !torch.vtensor<[2,3840,2560],f16>
    %none_710 = torch.constant.none
    %613 = torch.aten.clone %612, %none_710 : !torch.vtensor<[2,3840,2560],f16>, !torch.none -> !torch.vtensor<[2,3840,2560],f16>
    %int7680_711 = torch.constant.int 7680
    %int2560_712 = torch.constant.int 2560
    %614 = torch.prim.ListConstruct %int7680_711, %int2560_712 : (!torch.int, !torch.int) -> !torch.list<int>
    %615 = torch.aten.view %613, %614 : !torch.vtensor<[2,3840,2560],f16>, !torch.list<int> -> !torch.vtensor<[7680,2560],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %616 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %int0_713 = torch.constant.int 0
    %int1_714 = torch.constant.int 1
    %617 = torch.aten.transpose.int %616, %int0_713, %int1_714 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %618 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_715 = torch.constant.int 6
    %619 = torch.prims.convert_element_type %618, %int6_715 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_716 = torch.constant.int 6
    %620 = torch.prims.convert_element_type %615, %int6_716 : !torch.vtensor<[7680,2560],f16>, !torch.int -> !torch.vtensor<[7680,2560],f32>
    %int6_717 = torch.constant.int 6
    %621 = torch.prims.convert_element_type %617, %int6_717 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %622 = torch.aten.mm %620, %621 : !torch.vtensor<[7680,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_718 = torch.constant.int 1
    %623 = torch.aten.mul.Scalar %622, %int1_718 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_719 = torch.constant.int 1
    %624 = torch.aten.mul.Scalar %619, %int1_719 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_720 = torch.constant.int 1
    %625 = torch.aten.add.Tensor %623, %624, %int1_720 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_721 = torch.constant.int 5
    %626 = torch.prims.convert_element_type %625, %int5_721 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_722 = torch.constant.int 2
    %int3840_723 = torch.constant.int 3840
    %int640_724 = torch.constant.int 640
    %627 = torch.prim.ListConstruct %int2_722, %int3840_723, %int640_724 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %628 = torch.aten.view %626, %627 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int1_725 = torch.constant.int 1
    %629 = torch.aten.add.Tensor %628, %582, %int1_725 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_726 = torch.constant.int 6
    %630 = torch.prims.convert_element_type %629, %int6_726 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_727 = torch.constant.int 2
    %631 = torch.prim.ListConstruct %int2_727 : (!torch.int) -> !torch.list<int>
    %int0_728 = torch.constant.int 0
    %true_729 = torch.constant.bool true
    %result0_730, %result1_731 = torch.aten.var_mean.correction %630, %631, %int0_728, %true_729 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_732 = torch.constant.float 1.000000e-05
    %int1_733 = torch.constant.int 1
    %632 = torch.aten.add.Scalar %result0_730, %float1.000000e-05_732, %int1_733 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %633 = torch.aten.rsqrt %632 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_734 = torch.constant.int 1
    %634 = torch.aten.sub.Tensor %629, %result1_731, %int1_734 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %635 = torch.aten.mul.Tensor %634, %633 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %636 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %637 = torch.aten.mul.Tensor %635, %636 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %638 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_735 = torch.constant.int 1
    %639 = torch.aten.add.Tensor %637, %638, %int1_735 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_736 = torch.constant.int 5
    %640 = torch.prims.convert_element_type %639, %int5_736 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %641 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_737 = torch.constant.int 0
    %int1_738 = torch.constant.int 1
    %642 = torch.aten.transpose.int %641, %int0_737, %int1_738 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_739 = torch.constant.int 7680
    %int640_740 = torch.constant.int 640
    %643 = torch.prim.ListConstruct %int7680_739, %int640_740 : (!torch.int, !torch.int) -> !torch.list<int>
    %644 = torch.aten.view %640, %643 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %645 = torch.aten.mm %644, %642 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_741 = torch.constant.int 2
    %int3840_742 = torch.constant.int 3840
    %int640_743 = torch.constant.int 640
    %646 = torch.prim.ListConstruct %int2_741, %int3840_742, %int640_743 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %647 = torch.aten.view %645, %646 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %648 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_744 = torch.constant.int 0
    %int1_745 = torch.constant.int 1
    %649 = torch.aten.transpose.int %648, %int0_744, %int1_745 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_746 = torch.constant.int 7680
    %int640_747 = torch.constant.int 640
    %650 = torch.prim.ListConstruct %int7680_746, %int640_747 : (!torch.int, !torch.int) -> !torch.list<int>
    %651 = torch.aten.view %640, %650 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %652 = torch.aten.mm %651, %649 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_748 = torch.constant.int 2
    %int3840_749 = torch.constant.int 3840
    %int640_750 = torch.constant.int 640
    %653 = torch.prim.ListConstruct %int2_748, %int3840_749, %int640_750 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %654 = torch.aten.view %652, %653 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %655 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_751 = torch.constant.int 0
    %int1_752 = torch.constant.int 1
    %656 = torch.aten.transpose.int %655, %int0_751, %int1_752 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_753 = torch.constant.int 7680
    %int640_754 = torch.constant.int 640
    %657 = torch.prim.ListConstruct %int7680_753, %int640_754 : (!torch.int, !torch.int) -> !torch.list<int>
    %658 = torch.aten.view %640, %657 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %659 = torch.aten.mm %658, %656 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_755 = torch.constant.int 2
    %int3840_756 = torch.constant.int 3840
    %int640_757 = torch.constant.int 640
    %660 = torch.prim.ListConstruct %int2_755, %int3840_756, %int640_757 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %661 = torch.aten.view %659, %660 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int2_758 = torch.constant.int 2
    %int-1_759 = torch.constant.int -1
    %int10_760 = torch.constant.int 10
    %int64_761 = torch.constant.int 64
    %662 = torch.prim.ListConstruct %int2_758, %int-1_759, %int10_760, %int64_761 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %663 = torch.aten.view %647, %662 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_762 = torch.constant.int 1
    %int2_763 = torch.constant.int 2
    %664 = torch.aten.transpose.int %663, %int1_762, %int2_763 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_764 = torch.constant.int 2
    %int-1_765 = torch.constant.int -1
    %int10_766 = torch.constant.int 10
    %int64_767 = torch.constant.int 64
    %665 = torch.prim.ListConstruct %int2_764, %int-1_765, %int10_766, %int64_767 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %666 = torch.aten.view %654, %665 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_768 = torch.constant.int 1
    %int2_769 = torch.constant.int 2
    %667 = torch.aten.transpose.int %666, %int1_768, %int2_769 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_770 = torch.constant.int 2
    %int-1_771 = torch.constant.int -1
    %int10_772 = torch.constant.int 10
    %int64_773 = torch.constant.int 64
    %668 = torch.prim.ListConstruct %int2_770, %int-1_771, %int10_772, %int64_773 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %669 = torch.aten.view %661, %668 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_774 = torch.constant.int 1
    %int2_775 = torch.constant.int 2
    %670 = torch.aten.transpose.int %669, %int1_774, %int2_775 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %float0.000000e00_776 = torch.constant.float 0.000000e+00
    %false_777 = torch.constant.bool false
    %none_778 = torch.constant.none
    %none_779 = torch.constant.none
    %671:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%664, %667, %670, %float0.000000e00_776, %false_777, %none_778, %none_779) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_780 = torch.constant.int 1
    %int2_781 = torch.constant.int 2
    %672 = torch.aten.transpose.int %671#0, %int1_780, %int2_781 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_782 = torch.constant.int 2
    %int-1_783 = torch.constant.int -1
    %int640_784 = torch.constant.int 640
    %673 = torch.prim.ListConstruct %int2_782, %int-1_783, %int640_784 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %674 = torch.aten.view %672, %673 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_785 = torch.constant.int 5
    %675 = torch.prims.convert_element_type %674, %int5_785 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_786 = torch.constant.int 7680
    %int640_787 = torch.constant.int 640
    %676 = torch.prim.ListConstruct %int7680_786, %int640_787 : (!torch.int, !torch.int) -> !torch.list<int>
    %677 = torch.aten.view %675, %676 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %678 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_788 = torch.constant.int 0
    %int1_789 = torch.constant.int 1
    %679 = torch.aten.transpose.int %678, %int0_788, %int1_789 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %680 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_790 = torch.constant.int 6
    %681 = torch.prims.convert_element_type %680, %int6_790 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_791 = torch.constant.int 6
    %682 = torch.prims.convert_element_type %677, %int6_791 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_792 = torch.constant.int 6
    %683 = torch.prims.convert_element_type %679, %int6_792 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %684 = torch.aten.mm %682, %683 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_793 = torch.constant.int 1
    %685 = torch.aten.mul.Scalar %684, %int1_793 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_794 = torch.constant.int 1
    %686 = torch.aten.mul.Scalar %681, %int1_794 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_795 = torch.constant.int 1
    %687 = torch.aten.add.Tensor %685, %686, %int1_795 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_796 = torch.constant.int 5
    %688 = torch.prims.convert_element_type %687, %int5_796 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_797 = torch.constant.int 2
    %int3840_798 = torch.constant.int 3840
    %int640_799 = torch.constant.int 640
    %689 = torch.prim.ListConstruct %int2_797, %int3840_798, %int640_799 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %690 = torch.aten.view %688, %689 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_800 = torch.constant.none
    %691 = torch.aten.clone %690, %none_800 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_801 = torch.constant.float 1.000000e+00
    %692 = torch.aten.div.Scalar %691, %float1.000000e00_801 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_802 = torch.constant.int 1
    %693 = torch.aten.add.Tensor %692, %629, %int1_802 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_803 = torch.constant.int 6
    %694 = torch.prims.convert_element_type %693, %int6_803 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_804 = torch.constant.int 2
    %695 = torch.prim.ListConstruct %int2_804 : (!torch.int) -> !torch.list<int>
    %int0_805 = torch.constant.int 0
    %true_806 = torch.constant.bool true
    %result0_807, %result1_808 = torch.aten.var_mean.correction %694, %695, %int0_805, %true_806 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_809 = torch.constant.float 1.000000e-05
    %int1_810 = torch.constant.int 1
    %696 = torch.aten.add.Scalar %result0_807, %float1.000000e-05_809, %int1_810 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %697 = torch.aten.rsqrt %696 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_811 = torch.constant.int 1
    %698 = torch.aten.sub.Tensor %693, %result1_808, %int1_811 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %699 = torch.aten.mul.Tensor %698, %697 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %700 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %701 = torch.aten.mul.Tensor %699, %700 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %702 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_812 = torch.constant.int 1
    %703 = torch.aten.add.Tensor %701, %702, %int1_812 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_813 = torch.constant.int 5
    %704 = torch.prims.convert_element_type %703, %int5_813 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %705 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_814 = torch.constant.int 0
    %int1_815 = torch.constant.int 1
    %706 = torch.aten.transpose.int %705, %int0_814, %int1_815 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_816 = torch.constant.int 7680
    %int640_817 = torch.constant.int 640
    %707 = torch.prim.ListConstruct %int7680_816, %int640_817 : (!torch.int, !torch.int) -> !torch.list<int>
    %708 = torch.aten.view %704, %707 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %709 = torch.aten.mm %708, %706 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_818 = torch.constant.int 2
    %int3840_819 = torch.constant.int 3840
    %int640_820 = torch.constant.int 640
    %710 = torch.prim.ListConstruct %int2_818, %int3840_819, %int640_820 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %711 = torch.aten.view %709, %710 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %712 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_821 = torch.constant.int 0
    %int1_822 = torch.constant.int 1
    %713 = torch.aten.transpose.int %712, %int0_821, %int1_822 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_823 = torch.constant.int 32
    %int2048_824 = torch.constant.int 2048
    %714 = torch.prim.ListConstruct %int32_823, %int2048_824 : (!torch.int, !torch.int) -> !torch.list<int>
    %715 = torch.aten.view %arg6, %714 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %716 = torch.aten.mm %715, %713 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_825 = torch.constant.int 2
    %int16_826 = torch.constant.int 16
    %int640_827 = torch.constant.int 640
    %717 = torch.prim.ListConstruct %int2_825, %int16_826, %int640_827 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %718 = torch.aten.view %716, %717 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %719 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_828 = torch.constant.int 0
    %int1_829 = torch.constant.int 1
    %720 = torch.aten.transpose.int %719, %int0_828, %int1_829 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_830 = torch.constant.int 32
    %int2048_831 = torch.constant.int 2048
    %721 = torch.prim.ListConstruct %int32_830, %int2048_831 : (!torch.int, !torch.int) -> !torch.list<int>
    %722 = torch.aten.view %arg6, %721 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %723 = torch.aten.mm %722, %720 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_832 = torch.constant.int 2
    %int16_833 = torch.constant.int 16
    %int640_834 = torch.constant.int 640
    %724 = torch.prim.ListConstruct %int2_832, %int16_833, %int640_834 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %725 = torch.aten.view %723, %724 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %int2_835 = torch.constant.int 2
    %int-1_836 = torch.constant.int -1
    %int10_837 = torch.constant.int 10
    %int64_838 = torch.constant.int 64
    %726 = torch.prim.ListConstruct %int2_835, %int-1_836, %int10_837, %int64_838 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %727 = torch.aten.view %711, %726 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_839 = torch.constant.int 1
    %int2_840 = torch.constant.int 2
    %728 = torch.aten.transpose.int %727, %int1_839, %int2_840 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_841 = torch.constant.int 2
    %int-1_842 = torch.constant.int -1
    %int10_843 = torch.constant.int 10
    %int64_844 = torch.constant.int 64
    %729 = torch.prim.ListConstruct %int2_841, %int-1_842, %int10_843, %int64_844 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %730 = torch.aten.view %718, %729 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_845 = torch.constant.int 1
    %int2_846 = torch.constant.int 2
    %731 = torch.aten.transpose.int %730, %int1_845, %int2_846 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %int2_847 = torch.constant.int 2
    %int-1_848 = torch.constant.int -1
    %int10_849 = torch.constant.int 10
    %int64_850 = torch.constant.int 64
    %732 = torch.prim.ListConstruct %int2_847, %int-1_848, %int10_849, %int64_850 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %733 = torch.aten.view %725, %732 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_851 = torch.constant.int 1
    %int2_852 = torch.constant.int 2
    %734 = torch.aten.transpose.int %733, %int1_851, %int2_852 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %float0.000000e00_853 = torch.constant.float 0.000000e+00
    %false_854 = torch.constant.bool false
    %none_855 = torch.constant.none
    %none_856 = torch.constant.none
    %735:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%728, %731, %734, %float0.000000e00_853, %false_854, %none_855, %none_856) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_857 = torch.constant.int 1
    %int2_858 = torch.constant.int 2
    %736 = torch.aten.transpose.int %735#0, %int1_857, %int2_858 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_859 = torch.constant.int 2
    %int-1_860 = torch.constant.int -1
    %int640_861 = torch.constant.int 640
    %737 = torch.prim.ListConstruct %int2_859, %int-1_860, %int640_861 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %738 = torch.aten.view %736, %737 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_862 = torch.constant.int 5
    %739 = torch.prims.convert_element_type %738, %int5_862 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_863 = torch.constant.int 7680
    %int640_864 = torch.constant.int 640
    %740 = torch.prim.ListConstruct %int7680_863, %int640_864 : (!torch.int, !torch.int) -> !torch.list<int>
    %741 = torch.aten.view %739, %740 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %742 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_865 = torch.constant.int 0
    %int1_866 = torch.constant.int 1
    %743 = torch.aten.transpose.int %742, %int0_865, %int1_866 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %744 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_867 = torch.constant.int 6
    %745 = torch.prims.convert_element_type %744, %int6_867 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_868 = torch.constant.int 6
    %746 = torch.prims.convert_element_type %741, %int6_868 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_869 = torch.constant.int 6
    %747 = torch.prims.convert_element_type %743, %int6_869 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %748 = torch.aten.mm %746, %747 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_870 = torch.constant.int 1
    %749 = torch.aten.mul.Scalar %748, %int1_870 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_871 = torch.constant.int 1
    %750 = torch.aten.mul.Scalar %745, %int1_871 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_872 = torch.constant.int 1
    %751 = torch.aten.add.Tensor %749, %750, %int1_872 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_873 = torch.constant.int 5
    %752 = torch.prims.convert_element_type %751, %int5_873 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_874 = torch.constant.int 2
    %int3840_875 = torch.constant.int 3840
    %int640_876 = torch.constant.int 640
    %753 = torch.prim.ListConstruct %int2_874, %int3840_875, %int640_876 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %754 = torch.aten.view %752, %753 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_877 = torch.constant.none
    %755 = torch.aten.clone %754, %none_877 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_878 = torch.constant.float 1.000000e+00
    %756 = torch.aten.div.Scalar %755, %float1.000000e00_878 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_879 = torch.constant.int 1
    %757 = torch.aten.add.Tensor %756, %693, %int1_879 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_880 = torch.constant.int 6
    %758 = torch.prims.convert_element_type %757, %int6_880 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_881 = torch.constant.int 2
    %759 = torch.prim.ListConstruct %int2_881 : (!torch.int) -> !torch.list<int>
    %int0_882 = torch.constant.int 0
    %true_883 = torch.constant.bool true
    %result0_884, %result1_885 = torch.aten.var_mean.correction %758, %759, %int0_882, %true_883 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_886 = torch.constant.float 1.000000e-05
    %int1_887 = torch.constant.int 1
    %760 = torch.aten.add.Scalar %result0_884, %float1.000000e-05_886, %int1_887 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %761 = torch.aten.rsqrt %760 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_888 = torch.constant.int 1
    %762 = torch.aten.sub.Tensor %757, %result1_885, %int1_888 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %763 = torch.aten.mul.Tensor %762, %761 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %764 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %765 = torch.aten.mul.Tensor %763, %764 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %766 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_889 = torch.constant.int 1
    %767 = torch.aten.add.Tensor %765, %766, %int1_889 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_890 = torch.constant.int 5
    %768 = torch.prims.convert_element_type %767, %int5_890 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_891 = torch.constant.int 7680
    %int640_892 = torch.constant.int 640
    %769 = torch.prim.ListConstruct %int7680_891, %int640_892 : (!torch.int, !torch.int) -> !torch.list<int>
    %770 = torch.aten.view %768, %769 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %771 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %int0_893 = torch.constant.int 0
    %int1_894 = torch.constant.int 1
    %772 = torch.aten.transpose.int %771, %int0_893, %int1_894 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %773 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_895 = torch.constant.int 6
    %774 = torch.prims.convert_element_type %773, %int6_895 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_896 = torch.constant.int 6
    %775 = torch.prims.convert_element_type %770, %int6_896 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_897 = torch.constant.int 6
    %776 = torch.prims.convert_element_type %772, %int6_897 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %777 = torch.aten.mm %775, %776 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[7680,5120],f32>
    %int1_898 = torch.constant.int 1
    %778 = torch.aten.mul.Scalar %777, %int1_898 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int1_899 = torch.constant.int 1
    %779 = torch.aten.mul.Scalar %774, %int1_899 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_900 = torch.constant.int 1
    %780 = torch.aten.add.Tensor %778, %779, %int1_900 : !torch.vtensor<[7680,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int5_901 = torch.constant.int 5
    %781 = torch.prims.convert_element_type %780, %int5_901 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f16>
    %int2_902 = torch.constant.int 2
    %int3840_903 = torch.constant.int 3840
    %int5120_904 = torch.constant.int 5120
    %782 = torch.prim.ListConstruct %int2_902, %int3840_903, %int5120_904 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %783 = torch.aten.view %781, %782 : !torch.vtensor<[7680,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,5120],f16>
    %int-1_905 = torch.constant.int -1
    %int0_906 = torch.constant.int 0
    %int2560_907 = torch.constant.int 2560
    %int1_908 = torch.constant.int 1
    %784 = torch.aten.slice.Tensor %783, %int-1_905, %int0_906, %int2560_907, %int1_908 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %int-1_909 = torch.constant.int -1
    %int2560_910 = torch.constant.int 2560
    %int5120_911 = torch.constant.int 5120
    %int1_912 = torch.constant.int 1
    %785 = torch.aten.slice.Tensor %783, %int-1_909, %int2560_910, %int5120_911, %int1_912 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %str_913 = torch.constant.str "none"
    %786 = torch.aten.gelu %785, %str_913 : !torch.vtensor<[2,3840,2560],f16>, !torch.str -> !torch.vtensor<[2,3840,2560],f16>
    %787 = torch.aten.mul.Tensor %784, %786 : !torch.vtensor<[2,3840,2560],f16>, !torch.vtensor<[2,3840,2560],f16> -> !torch.vtensor<[2,3840,2560],f16>
    %none_914 = torch.constant.none
    %788 = torch.aten.clone %787, %none_914 : !torch.vtensor<[2,3840,2560],f16>, !torch.none -> !torch.vtensor<[2,3840,2560],f16>
    %int7680_915 = torch.constant.int 7680
    %int2560_916 = torch.constant.int 2560
    %789 = torch.prim.ListConstruct %int7680_915, %int2560_916 : (!torch.int, !torch.int) -> !torch.list<int>
    %790 = torch.aten.view %788, %789 : !torch.vtensor<[2,3840,2560],f16>, !torch.list<int> -> !torch.vtensor<[7680,2560],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %791 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %int0_917 = torch.constant.int 0
    %int1_918 = torch.constant.int 1
    %792 = torch.aten.transpose.int %791, %int0_917, %int1_918 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %793 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_919 = torch.constant.int 6
    %794 = torch.prims.convert_element_type %793, %int6_919 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_920 = torch.constant.int 6
    %795 = torch.prims.convert_element_type %790, %int6_920 : !torch.vtensor<[7680,2560],f16>, !torch.int -> !torch.vtensor<[7680,2560],f32>
    %int6_921 = torch.constant.int 6
    %796 = torch.prims.convert_element_type %792, %int6_921 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %797 = torch.aten.mm %795, %796 : !torch.vtensor<[7680,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_922 = torch.constant.int 1
    %798 = torch.aten.mul.Scalar %797, %int1_922 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_923 = torch.constant.int 1
    %799 = torch.aten.mul.Scalar %794, %int1_923 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_924 = torch.constant.int 1
    %800 = torch.aten.add.Tensor %798, %799, %int1_924 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_925 = torch.constant.int 5
    %801 = torch.prims.convert_element_type %800, %int5_925 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_926 = torch.constant.int 2
    %int3840_927 = torch.constant.int 3840
    %int640_928 = torch.constant.int 640
    %802 = torch.prim.ListConstruct %int2_926, %int3840_927, %int640_928 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %803 = torch.aten.view %801, %802 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int1_929 = torch.constant.int 1
    %804 = torch.aten.add.Tensor %803, %757, %int1_929 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_930 = torch.constant.int 7680
    %int640_931 = torch.constant.int 640
    %805 = torch.prim.ListConstruct %int7680_930, %int640_931 : (!torch.int, !torch.int) -> !torch.list<int>
    %806 = torch.aten.view %804, %805 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.proj_out.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16>
    %807 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_932 = torch.constant.int 0
    %int1_933 = torch.constant.int 1
    %808 = torch.aten.transpose.int %807, %int0_932, %int1_933 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.0.proj_out.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.0.proj_out.bias : tensor<640xf16>
    %809 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.0.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_934 = torch.constant.int 6
    %810 = torch.prims.convert_element_type %809, %int6_934 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_935 = torch.constant.int 6
    %811 = torch.prims.convert_element_type %806, %int6_935 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_936 = torch.constant.int 6
    %812 = torch.prims.convert_element_type %808, %int6_936 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %813 = torch.aten.mm %811, %812 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_937 = torch.constant.int 1
    %814 = torch.aten.mul.Scalar %813, %int1_937 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_938 = torch.constant.int 1
    %815 = torch.aten.mul.Scalar %810, %int1_938 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_939 = torch.constant.int 1
    %816 = torch.aten.add.Tensor %814, %815, %int1_939 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_940 = torch.constant.int 5
    %817 = torch.prims.convert_element_type %816, %int5_940 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_941 = torch.constant.int 2
    %int3840_942 = torch.constant.int 3840
    %int640_943 = torch.constant.int 640
    %818 = torch.prim.ListConstruct %int2_941, %int3840_942, %int640_943 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %819 = torch.aten.view %817, %818 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int2_944 = torch.constant.int 2
    %int60_945 = torch.constant.int 60
    %int64_946 = torch.constant.int 64
    %int640_947 = torch.constant.int 640
    %820 = torch.prim.ListConstruct %int2_944, %int60_945, %int64_946, %int640_947 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %821 = torch.aten.view %819, %820 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,60,64,640],f16>
    %int0_948 = torch.constant.int 0
    %int3_949 = torch.constant.int 3
    %int1_950 = torch.constant.int 1
    %int2_951 = torch.constant.int 2
    %822 = torch.prim.ListConstruct %int0_948, %int3_949, %int1_950, %int2_951 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %823 = torch.aten.permute %821, %822 : !torch.vtensor<[2,60,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f16>
    %int0_952 = torch.constant.int 0
    %824 = torch.aten.clone %823, %int0_952 : !torch.vtensor<[2,640,60,64],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int1_953 = torch.constant.int 1
    %825 = torch.aten.add.Tensor %824, %419, %int1_953 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int2_954 = torch.constant.int 2
    %int32_955 = torch.constant.int 32
    %int20_956 = torch.constant.int 20
    %int3840_957 = torch.constant.int 3840
    %826 = torch.prim.ListConstruct %int2_954, %int32_955, %int20_956, %int3840_957 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %827 = torch.aten.view %825, %826 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,3840],f16>
    %int6_958 = torch.constant.int 6
    %828 = torch.prims.convert_element_type %827, %int6_958 : !torch.vtensor<[2,32,20,3840],f16>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_959 = torch.constant.int 2
    %int3_960 = torch.constant.int 3
    %829 = torch.prim.ListConstruct %int2_959, %int3_960 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_961 = torch.constant.int 0
    %true_962 = torch.constant.bool true
    %result0_963, %result1_964 = torch.aten.var_mean.correction %828, %829, %int0_961, %true_962 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_965 = torch.constant.float 1.000000e-05
    %int1_966 = torch.constant.int 1
    %830 = torch.aten.add.Scalar %result0_963, %float1.000000e-05_965, %int1_966 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %831 = torch.aten.rsqrt %830 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_967 = torch.constant.int 1
    %832 = torch.aten.sub.Tensor %827, %result1_964, %int1_967 : !torch.vtensor<[2,32,20,3840],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %833 = torch.aten.mul.Tensor %832, %831 : !torch.vtensor<[2,32,20,3840],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_968 = torch.constant.int 2
    %int640_969 = torch.constant.int 640
    %int60_970 = torch.constant.int 60
    %int64_971 = torch.constant.int 64
    %834 = torch.prim.ListConstruct %int2_968, %int640_969, %int60_970, %int64_971 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %835 = torch.aten.view %833, %834 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f32>
    %__auto.controlnet.down_blocks.1.resnets.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.norm1.bias : tensor<640xf16>
    %836 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_972 = torch.constant.int 0
    %837 = torch.aten.unsqueeze %836, %int0_972 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_973 = torch.constant.int 2
    %838 = torch.aten.unsqueeze %837, %int2_973 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_974 = torch.constant.int 3
    %839 = torch.aten.unsqueeze %838, %int3_974 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.norm1.weight : tensor<640xf16>
    %840 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_975 = torch.constant.int 0
    %841 = torch.aten.unsqueeze %840, %int0_975 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_976 = torch.constant.int 2
    %842 = torch.aten.unsqueeze %841, %int2_976 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_977 = torch.constant.int 3
    %843 = torch.aten.unsqueeze %842, %int3_977 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %844 = torch.aten.mul.Tensor %835, %843 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,60,64],f32>
    %int1_978 = torch.constant.int 1
    %845 = torch.aten.add.Tensor %844, %839, %int1_978 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f32>
    %int5_979 = torch.constant.int 5
    %846 = torch.prims.convert_element_type %845, %int5_979 : !torch.vtensor<[2,640,60,64],f32>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %847 = torch.aten.silu %846 : !torch.vtensor<[2,640,60,64],f16> -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.conv1.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.conv1.weight : tensor<640x640x3x3xf16>
    %848 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.conv1.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.conv1.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.conv1.bias : tensor<640xf16>
    %849 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.conv1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_980 = torch.constant.int 1
    %int1_981 = torch.constant.int 1
    %850 = torch.prim.ListConstruct %int1_980, %int1_981 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_982 = torch.constant.int 1
    %int1_983 = torch.constant.int 1
    %851 = torch.prim.ListConstruct %int1_982, %int1_983 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_984 = torch.constant.int 1
    %int1_985 = torch.constant.int 1
    %852 = torch.prim.ListConstruct %int1_984, %int1_985 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_986 = torch.constant.bool false
    %int0_987 = torch.constant.int 0
    %int0_988 = torch.constant.int 0
    %853 = torch.prim.ListConstruct %int0_987, %int0_988 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_989 = torch.constant.int 1
    %854 = torch.aten.convolution %847, %848, %849, %850, %851, %852, %false_986, %853, %int1_989 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %855 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16>
    %856 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.weight : tensor<640x1280xf16> -> !torch.vtensor<[640,1280],f16>
    %int0_990 = torch.constant.int 0
    %int1_991 = torch.constant.int 1
    %857 = torch.aten.transpose.int %856, %int0_990, %int1_991 : !torch.vtensor<[640,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,640],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16>
    %858 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.time_emb_proj.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_992 = torch.constant.int 6
    %859 = torch.prims.convert_element_type %858, %int6_992 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_993 = torch.constant.int 6
    %860 = torch.prims.convert_element_type %855, %int6_993 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_994 = torch.constant.int 6
    %861 = torch.prims.convert_element_type %857, %int6_994 : !torch.vtensor<[1280,640],f16>, !torch.int -> !torch.vtensor<[1280,640],f32>
    %862 = torch.aten.mm %860, %861 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,640],f32> -> !torch.vtensor<[2,640],f32>
    %int1_995 = torch.constant.int 1
    %863 = torch.aten.mul.Scalar %862, %int1_995 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %int1_996 = torch.constant.int 1
    %864 = torch.aten.mul.Scalar %859, %int1_996 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_997 = torch.constant.int 1
    %865 = torch.aten.add.Tensor %863, %864, %int1_997 : !torch.vtensor<[2,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[2,640],f32>
    %int5_998 = torch.constant.int 5
    %866 = torch.prims.convert_element_type %865, %int5_998 : !torch.vtensor<[2,640],f32>, !torch.int -> !torch.vtensor<[2,640],f16>
    %int0_999 = torch.constant.int 0
    %int0_1000 = torch.constant.int 0
    %int9223372036854775807_1001 = torch.constant.int 9223372036854775807
    %int1_1002 = torch.constant.int 1
    %867 = torch.aten.slice.Tensor %866, %int0_999, %int0_1000, %int9223372036854775807_1001, %int1_1002 : !torch.vtensor<[2,640],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,640],f16>
    %int1_1003 = torch.constant.int 1
    %int0_1004 = torch.constant.int 0
    %int9223372036854775807_1005 = torch.constant.int 9223372036854775807
    %int1_1006 = torch.constant.int 1
    %868 = torch.aten.slice.Tensor %867, %int1_1003, %int0_1004, %int9223372036854775807_1005, %int1_1006 : !torch.vtensor<[2,640],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,640],f16>
    %int2_1007 = torch.constant.int 2
    %869 = torch.aten.unsqueeze %868, %int2_1007 : !torch.vtensor<[2,640],f16>, !torch.int -> !torch.vtensor<[2,640,1],f16>
    %int3_1008 = torch.constant.int 3
    %870 = torch.aten.unsqueeze %869, %int3_1008 : !torch.vtensor<[2,640,1],f16>, !torch.int -> !torch.vtensor<[2,640,1,1],f16>
    %int1_1009 = torch.constant.int 1
    %871 = torch.aten.add.Tensor %854, %870, %int1_1009 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int2_1010 = torch.constant.int 2
    %int32_1011 = torch.constant.int 32
    %int20_1012 = torch.constant.int 20
    %int3840_1013 = torch.constant.int 3840
    %872 = torch.prim.ListConstruct %int2_1010, %int32_1011, %int20_1012, %int3840_1013 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %873 = torch.aten.view %871, %872 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,3840],f16>
    %int6_1014 = torch.constant.int 6
    %874 = torch.prims.convert_element_type %873, %int6_1014 : !torch.vtensor<[2,32,20,3840],f16>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_1015 = torch.constant.int 2
    %int3_1016 = torch.constant.int 3
    %875 = torch.prim.ListConstruct %int2_1015, %int3_1016 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_1017 = torch.constant.int 0
    %true_1018 = torch.constant.bool true
    %result0_1019, %result1_1020 = torch.aten.var_mean.correction %874, %875, %int0_1017, %true_1018 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_1021 = torch.constant.float 1.000000e-05
    %int1_1022 = torch.constant.int 1
    %876 = torch.aten.add.Scalar %result0_1019, %float1.000000e-05_1021, %int1_1022 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %877 = torch.aten.rsqrt %876 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_1023 = torch.constant.int 1
    %878 = torch.aten.sub.Tensor %873, %result1_1020, %int1_1023 : !torch.vtensor<[2,32,20,3840],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %879 = torch.aten.mul.Tensor %878, %877 : !torch.vtensor<[2,32,20,3840],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_1024 = torch.constant.int 2
    %int640_1025 = torch.constant.int 640
    %int60_1026 = torch.constant.int 60
    %int64_1027 = torch.constant.int 64
    %880 = torch.prim.ListConstruct %int2_1024, %int640_1025, %int60_1026, %int64_1027 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %881 = torch.aten.view %879, %880 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f32>
    %__auto.controlnet.down_blocks.1.resnets.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.norm2.bias : tensor<640xf16>
    %882 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_1028 = torch.constant.int 0
    %883 = torch.aten.unsqueeze %882, %int0_1028 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_1029 = torch.constant.int 2
    %884 = torch.aten.unsqueeze %883, %int2_1029 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_1030 = torch.constant.int 3
    %885 = torch.aten.unsqueeze %884, %int3_1030 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.norm2.weight : tensor<640xf16>
    %886 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_1031 = torch.constant.int 0
    %887 = torch.aten.unsqueeze %886, %int0_1031 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_1032 = torch.constant.int 2
    %888 = torch.aten.unsqueeze %887, %int2_1032 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_1033 = torch.constant.int 3
    %889 = torch.aten.unsqueeze %888, %int3_1033 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %890 = torch.aten.mul.Tensor %881, %889 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,60,64],f32>
    %int1_1034 = torch.constant.int 1
    %891 = torch.aten.add.Tensor %890, %885, %int1_1034 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f32>
    %int5_1035 = torch.constant.int 5
    %892 = torch.prims.convert_element_type %891, %int5_1035 : !torch.vtensor<[2,640,60,64],f32>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %893 = torch.aten.silu %892 : !torch.vtensor<[2,640,60,64],f16> -> !torch.vtensor<[2,640,60,64],f16>
    %none_1036 = torch.constant.none
    %894 = torch.aten.clone %893, %none_1036 : !torch.vtensor<[2,640,60,64],f16>, !torch.none -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.conv2.weight = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16>
    %895 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.conv2.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %__auto.controlnet.down_blocks.1.resnets.1.conv2.bias = util.global.load @__auto.controlnet.down_blocks.1.resnets.1.conv2.bias : tensor<640xf16>
    %896 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.resnets.1.conv2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1037 = torch.constant.int 1
    %int1_1038 = torch.constant.int 1
    %897 = torch.prim.ListConstruct %int1_1037, %int1_1038 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1039 = torch.constant.int 1
    %int1_1040 = torch.constant.int 1
    %898 = torch.prim.ListConstruct %int1_1039, %int1_1040 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1041 = torch.constant.int 1
    %int1_1042 = torch.constant.int 1
    %899 = torch.prim.ListConstruct %int1_1041, %int1_1042 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_1043 = torch.constant.bool false
    %int0_1044 = torch.constant.int 0
    %int0_1045 = torch.constant.int 0
    %900 = torch.prim.ListConstruct %int0_1044, %int0_1045 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1046 = torch.constant.int 1
    %901 = torch.aten.convolution %894, %895, %896, %897, %898, %899, %false_1043, %900, %int1_1046 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int1_1047 = torch.constant.int 1
    %902 = torch.aten.add.Tensor %825, %901, %int1_1047 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %float1.000000e00_1048 = torch.constant.float 1.000000e+00
    %903 = torch.aten.div.Scalar %902, %float1.000000e00_1048 : !torch.vtensor<[2,640,60,64],f16>, !torch.float -> !torch.vtensor<[2,640,60,64],f16>
    %int2_1049 = torch.constant.int 2
    %int32_1050 = torch.constant.int 32
    %int20_1051 = torch.constant.int 20
    %int3840_1052 = torch.constant.int 3840
    %904 = torch.prim.ListConstruct %int2_1049, %int32_1050, %int20_1051, %int3840_1052 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %905 = torch.aten.view %903, %904 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,3840],f16>
    %int6_1053 = torch.constant.int 6
    %906 = torch.prims.convert_element_type %905, %int6_1053 : !torch.vtensor<[2,32,20,3840],f16>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_1054 = torch.constant.int 2
    %int3_1055 = torch.constant.int 3
    %907 = torch.prim.ListConstruct %int2_1054, %int3_1055 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_1056 = torch.constant.int 0
    %true_1057 = torch.constant.bool true
    %result0_1058, %result1_1059 = torch.aten.var_mean.correction %906, %907, %int0_1056, %true_1057 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float9.999990e-07_1060 = torch.constant.float 9.9999999999999995E-7
    %int1_1061 = torch.constant.int 1
    %908 = torch.aten.add.Scalar %result0_1058, %float9.999990e-07_1060, %int1_1061 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %909 = torch.aten.rsqrt %908 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_1062 = torch.constant.int 1
    %910 = torch.aten.sub.Tensor %905, %result1_1059, %int1_1062 : !torch.vtensor<[2,32,20,3840],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,3840],f32>
    %911 = torch.aten.mul.Tensor %910, %909 : !torch.vtensor<[2,32,20,3840],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,3840],f32>
    %int2_1063 = torch.constant.int 2
    %int640_1064 = torch.constant.int 640
    %int60_1065 = torch.constant.int 60
    %int64_1066 = torch.constant.int 64
    %912 = torch.prim.ListConstruct %int2_1063, %int640_1064, %int60_1065, %int64_1066 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %913 = torch.aten.view %911, %912 : !torch.vtensor<[2,32,20,3840],f32>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.norm.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.norm.bias : tensor<640xf16>
    %914 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.norm.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_1067 = torch.constant.int 0
    %915 = torch.aten.unsqueeze %914, %int0_1067 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_1068 = torch.constant.int 2
    %916 = torch.aten.unsqueeze %915, %int2_1068 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_1069 = torch.constant.int 3
    %917 = torch.aten.unsqueeze %916, %int3_1069 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.norm.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.norm.weight : tensor<640xf16>
    %918 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.norm.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_1070 = torch.constant.int 0
    %919 = torch.aten.unsqueeze %918, %int0_1070 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_1071 = torch.constant.int 2
    %920 = torch.aten.unsqueeze %919, %int2_1071 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_1072 = torch.constant.int 3
    %921 = torch.aten.unsqueeze %920, %int3_1072 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %922 = torch.aten.mul.Tensor %913, %921 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,60,64],f32>
    %int1_1073 = torch.constant.int 1
    %923 = torch.aten.add.Tensor %922, %917, %int1_1073 : !torch.vtensor<[2,640,60,64],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f32>
    %int5_1074 = torch.constant.int 5
    %924 = torch.prims.convert_element_type %923, %int5_1074 : !torch.vtensor<[2,640,60,64],f32>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int0_1075 = torch.constant.int 0
    %int2_1076 = torch.constant.int 2
    %int3_1077 = torch.constant.int 3
    %int1_1078 = torch.constant.int 1
    %925 = torch.prim.ListConstruct %int0_1075, %int2_1076, %int3_1077, %int1_1078 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %926 = torch.aten.permute %924, %925 : !torch.vtensor<[2,640,60,64],f16>, !torch.list<int> -> !torch.vtensor<[2,60,64,640],f16>
    %int2_1079 = torch.constant.int 2
    %int3840_1080 = torch.constant.int 3840
    %int640_1081 = torch.constant.int 640
    %927 = torch.prim.ListConstruct %int2_1079, %int3840_1080, %int640_1081 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %928 = torch.aten.view %926, %927 : !torch.vtensor<[2,60,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.proj_in.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16>
    %929 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.proj_in.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1082 = torch.constant.int 0
    %int1_1083 = torch.constant.int 1
    %930 = torch.aten.transpose.int %929, %int0_1082, %int1_1083 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int0_1084 = torch.constant.int 0
    %931 = torch.aten.clone %928, %int0_1084 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1085 = torch.constant.int 7680
    %int640_1086 = torch.constant.int 640
    %932 = torch.prim.ListConstruct %int7680_1085, %int640_1086 : (!torch.int, !torch.int) -> !torch.list<int>
    %933 = torch.aten._unsafe_view %931, %932 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %934 = torch.aten.mm %933, %930 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1087 = torch.constant.int 2
    %int3840_1088 = torch.constant.int 3840
    %int640_1089 = torch.constant.int 640
    %935 = torch.prim.ListConstruct %int2_1087, %int3840_1088, %int640_1089 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %936 = torch.aten.view %934, %935 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.proj_in.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.proj_in.bias : tensor<640xf16>
    %937 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.proj_in.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1090 = torch.constant.int 1
    %938 = torch.aten.add.Tensor %936, %937, %int1_1090 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_1091 = torch.constant.int 6
    %939 = torch.prims.convert_element_type %938, %int6_1091 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_1092 = torch.constant.int 2
    %940 = torch.prim.ListConstruct %int2_1092 : (!torch.int) -> !torch.list<int>
    %int0_1093 = torch.constant.int 0
    %true_1094 = torch.constant.bool true
    %result0_1095, %result1_1096 = torch.aten.var_mean.correction %939, %940, %int0_1093, %true_1094 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_1097 = torch.constant.float 1.000000e-05
    %int1_1098 = torch.constant.int 1
    %941 = torch.aten.add.Scalar %result0_1095, %float1.000000e-05_1097, %int1_1098 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %942 = torch.aten.rsqrt %941 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_1099 = torch.constant.int 1
    %943 = torch.aten.sub.Tensor %938, %result1_1096, %int1_1099 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %944 = torch.aten.mul.Tensor %943, %942 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16>
    %945 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %946 = torch.aten.mul.Tensor %944, %945 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16>
    %947 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1100 = torch.constant.int 1
    %948 = torch.aten.add.Tensor %946, %947, %int1_1100 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_1101 = torch.constant.int 5
    %949 = torch.prims.convert_element_type %948, %int5_1101 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16>
    %950 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1102 = torch.constant.int 0
    %int1_1103 = torch.constant.int 1
    %951 = torch.aten.transpose.int %950, %int0_1102, %int1_1103 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1104 = torch.constant.int 7680
    %int640_1105 = torch.constant.int 640
    %952 = torch.prim.ListConstruct %int7680_1104, %int640_1105 : (!torch.int, !torch.int) -> !torch.list<int>
    %953 = torch.aten.view %949, %952 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %954 = torch.aten.mm %953, %951 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1106 = torch.constant.int 2
    %int3840_1107 = torch.constant.int 3840
    %int640_1108 = torch.constant.int 640
    %955 = torch.prim.ListConstruct %int2_1106, %int3840_1107, %int640_1108 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %956 = torch.aten.view %954, %955 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16>
    %957 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1109 = torch.constant.int 0
    %int1_1110 = torch.constant.int 1
    %958 = torch.aten.transpose.int %957, %int0_1109, %int1_1110 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1111 = torch.constant.int 7680
    %int640_1112 = torch.constant.int 640
    %959 = torch.prim.ListConstruct %int7680_1111, %int640_1112 : (!torch.int, !torch.int) -> !torch.list<int>
    %960 = torch.aten.view %949, %959 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %961 = torch.aten.mm %960, %958 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1113 = torch.constant.int 2
    %int3840_1114 = torch.constant.int 3840
    %int640_1115 = torch.constant.int 640
    %962 = torch.prim.ListConstruct %int2_1113, %int3840_1114, %int640_1115 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %963 = torch.aten.view %961, %962 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16>
    %964 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1116 = torch.constant.int 0
    %int1_1117 = torch.constant.int 1
    %965 = torch.aten.transpose.int %964, %int0_1116, %int1_1117 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1118 = torch.constant.int 7680
    %int640_1119 = torch.constant.int 640
    %966 = torch.prim.ListConstruct %int7680_1118, %int640_1119 : (!torch.int, !torch.int) -> !torch.list<int>
    %967 = torch.aten.view %949, %966 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %968 = torch.aten.mm %967, %965 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1120 = torch.constant.int 2
    %int3840_1121 = torch.constant.int 3840
    %int640_1122 = torch.constant.int 640
    %969 = torch.prim.ListConstruct %int2_1120, %int3840_1121, %int640_1122 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %970 = torch.aten.view %968, %969 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int2_1123 = torch.constant.int 2
    %int-1_1124 = torch.constant.int -1
    %int10_1125 = torch.constant.int 10
    %int64_1126 = torch.constant.int 64
    %971 = torch.prim.ListConstruct %int2_1123, %int-1_1124, %int10_1125, %int64_1126 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %972 = torch.aten.view %956, %971 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1127 = torch.constant.int 1
    %int2_1128 = torch.constant.int 2
    %973 = torch.aten.transpose.int %972, %int1_1127, %int2_1128 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_1129 = torch.constant.int 2
    %int-1_1130 = torch.constant.int -1
    %int10_1131 = torch.constant.int 10
    %int64_1132 = torch.constant.int 64
    %974 = torch.prim.ListConstruct %int2_1129, %int-1_1130, %int10_1131, %int64_1132 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %975 = torch.aten.view %963, %974 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1133 = torch.constant.int 1
    %int2_1134 = torch.constant.int 2
    %976 = torch.aten.transpose.int %975, %int1_1133, %int2_1134 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_1135 = torch.constant.int 2
    %int-1_1136 = torch.constant.int -1
    %int10_1137 = torch.constant.int 10
    %int64_1138 = torch.constant.int 64
    %977 = torch.prim.ListConstruct %int2_1135, %int-1_1136, %int10_1137, %int64_1138 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %978 = torch.aten.view %970, %977 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1139 = torch.constant.int 1
    %int2_1140 = torch.constant.int 2
    %979 = torch.aten.transpose.int %978, %int1_1139, %int2_1140 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %float0.000000e00_1141 = torch.constant.float 0.000000e+00
    %false_1142 = torch.constant.bool false
    %none_1143 = torch.constant.none
    %none_1144 = torch.constant.none
    %980:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%973, %976, %979, %float0.000000e00_1141, %false_1142, %none_1143, %none_1144) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_1145 = torch.constant.int 1
    %int2_1146 = torch.constant.int 2
    %981 = torch.aten.transpose.int %980#0, %int1_1145, %int2_1146 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_1147 = torch.constant.int 2
    %int-1_1148 = torch.constant.int -1
    %int640_1149 = torch.constant.int 640
    %982 = torch.prim.ListConstruct %int2_1147, %int-1_1148, %int640_1149 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %983 = torch.aten.view %981, %982 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_1150 = torch.constant.int 5
    %984 = torch.prims.convert_element_type %983, %int5_1150 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1151 = torch.constant.int 7680
    %int640_1152 = torch.constant.int 640
    %985 = torch.prim.ListConstruct %int7680_1151, %int640_1152 : (!torch.int, !torch.int) -> !torch.list<int>
    %986 = torch.aten.view %984, %985 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16>
    %987 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1153 = torch.constant.int 0
    %int1_1154 = torch.constant.int 1
    %988 = torch.aten.transpose.int %987, %int0_1153, %int1_1154 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16>
    %989 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1155 = torch.constant.int 6
    %990 = torch.prims.convert_element_type %989, %int6_1155 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1156 = torch.constant.int 6
    %991 = torch.prims.convert_element_type %986, %int6_1156 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1157 = torch.constant.int 6
    %992 = torch.prims.convert_element_type %988, %int6_1157 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %993 = torch.aten.mm %991, %992 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1158 = torch.constant.int 1
    %994 = torch.aten.mul.Scalar %993, %int1_1158 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1159 = torch.constant.int 1
    %995 = torch.aten.mul.Scalar %990, %int1_1159 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1160 = torch.constant.int 1
    %996 = torch.aten.add.Tensor %994, %995, %int1_1160 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1161 = torch.constant.int 5
    %997 = torch.prims.convert_element_type %996, %int5_1161 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1162 = torch.constant.int 2
    %int3840_1163 = torch.constant.int 3840
    %int640_1164 = torch.constant.int 640
    %998 = torch.prim.ListConstruct %int2_1162, %int3840_1163, %int640_1164 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %999 = torch.aten.view %997, %998 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_1165 = torch.constant.none
    %1000 = torch.aten.clone %999, %none_1165 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_1166 = torch.constant.float 1.000000e+00
    %1001 = torch.aten.div.Scalar %1000, %float1.000000e00_1166 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_1167 = torch.constant.int 1
    %1002 = torch.aten.add.Tensor %1001, %938, %int1_1167 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_1168 = torch.constant.int 6
    %1003 = torch.prims.convert_element_type %1002, %int6_1168 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_1169 = torch.constant.int 2
    %1004 = torch.prim.ListConstruct %int2_1169 : (!torch.int) -> !torch.list<int>
    %int0_1170 = torch.constant.int 0
    %true_1171 = torch.constant.bool true
    %result0_1172, %result1_1173 = torch.aten.var_mean.correction %1003, %1004, %int0_1170, %true_1171 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_1174 = torch.constant.float 1.000000e-05
    %int1_1175 = torch.constant.int 1
    %1005 = torch.aten.add.Scalar %result0_1172, %float1.000000e-05_1174, %int1_1175 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %1006 = torch.aten.rsqrt %1005 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_1176 = torch.constant.int 1
    %1007 = torch.aten.sub.Tensor %1002, %result1_1173, %int1_1176 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %1008 = torch.aten.mul.Tensor %1007, %1006 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16>
    %1009 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1010 = torch.aten.mul.Tensor %1008, %1009 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16>
    %1011 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1177 = torch.constant.int 1
    %1012 = torch.aten.add.Tensor %1010, %1011, %int1_1177 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_1178 = torch.constant.int 5
    %1013 = torch.prims.convert_element_type %1012, %int5_1178 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16>
    %1014 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1179 = torch.constant.int 0
    %int1_1180 = torch.constant.int 1
    %1015 = torch.aten.transpose.int %1014, %int0_1179, %int1_1180 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1181 = torch.constant.int 7680
    %int640_1182 = torch.constant.int 640
    %1016 = torch.prim.ListConstruct %int7680_1181, %int640_1182 : (!torch.int, !torch.int) -> !torch.list<int>
    %1017 = torch.aten.view %1013, %1016 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %1018 = torch.aten.mm %1017, %1015 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1183 = torch.constant.int 2
    %int3840_1184 = torch.constant.int 3840
    %int640_1185 = torch.constant.int 640
    %1019 = torch.prim.ListConstruct %int2_1183, %int3840_1184, %int640_1185 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1020 = torch.aten.view %1018, %1019 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16>
    %1021 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_1186 = torch.constant.int 0
    %int1_1187 = torch.constant.int 1
    %1022 = torch.aten.transpose.int %1021, %int0_1186, %int1_1187 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_1188 = torch.constant.int 32
    %int2048_1189 = torch.constant.int 2048
    %1023 = torch.prim.ListConstruct %int32_1188, %int2048_1189 : (!torch.int, !torch.int) -> !torch.list<int>
    %1024 = torch.aten.view %arg6, %1023 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1025 = torch.aten.mm %1024, %1022 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_1190 = torch.constant.int 2
    %int16_1191 = torch.constant.int 16
    %int640_1192 = torch.constant.int 640
    %1026 = torch.prim.ListConstruct %int2_1190, %int16_1191, %int640_1192 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1027 = torch.aten.view %1025, %1026 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16>
    %1028 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_1193 = torch.constant.int 0
    %int1_1194 = torch.constant.int 1
    %1029 = torch.aten.transpose.int %1028, %int0_1193, %int1_1194 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_1195 = torch.constant.int 32
    %int2048_1196 = torch.constant.int 2048
    %1030 = torch.prim.ListConstruct %int32_1195, %int2048_1196 : (!torch.int, !torch.int) -> !torch.list<int>
    %1031 = torch.aten.view %arg6, %1030 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1032 = torch.aten.mm %1031, %1029 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_1197 = torch.constant.int 2
    %int16_1198 = torch.constant.int 16
    %int640_1199 = torch.constant.int 640
    %1033 = torch.prim.ListConstruct %int2_1197, %int16_1198, %int640_1199 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1034 = torch.aten.view %1032, %1033 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %int2_1200 = torch.constant.int 2
    %int-1_1201 = torch.constant.int -1
    %int10_1202 = torch.constant.int 10
    %int64_1203 = torch.constant.int 64
    %1035 = torch.prim.ListConstruct %int2_1200, %int-1_1201, %int10_1202, %int64_1203 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1036 = torch.aten.view %1020, %1035 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1204 = torch.constant.int 1
    %int2_1205 = torch.constant.int 2
    %1037 = torch.aten.transpose.int %1036, %int1_1204, %int2_1205 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_1206 = torch.constant.int 2
    %int-1_1207 = torch.constant.int -1
    %int10_1208 = torch.constant.int 10
    %int64_1209 = torch.constant.int 64
    %1038 = torch.prim.ListConstruct %int2_1206, %int-1_1207, %int10_1208, %int64_1209 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1039 = torch.aten.view %1027, %1038 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_1210 = torch.constant.int 1
    %int2_1211 = torch.constant.int 2
    %1040 = torch.aten.transpose.int %1039, %int1_1210, %int2_1211 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %int2_1212 = torch.constant.int 2
    %int-1_1213 = torch.constant.int -1
    %int10_1214 = torch.constant.int 10
    %int64_1215 = torch.constant.int 64
    %1041 = torch.prim.ListConstruct %int2_1212, %int-1_1213, %int10_1214, %int64_1215 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1042 = torch.aten.view %1034, %1041 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_1216 = torch.constant.int 1
    %int2_1217 = torch.constant.int 2
    %1043 = torch.aten.transpose.int %1042, %int1_1216, %int2_1217 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %float0.000000e00_1218 = torch.constant.float 0.000000e+00
    %false_1219 = torch.constant.bool false
    %none_1220 = torch.constant.none
    %none_1221 = torch.constant.none
    %1044:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1037, %1040, %1043, %float0.000000e00_1218, %false_1219, %none_1220, %none_1221) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_1222 = torch.constant.int 1
    %int2_1223 = torch.constant.int 2
    %1045 = torch.aten.transpose.int %1044#0, %int1_1222, %int2_1223 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_1224 = torch.constant.int 2
    %int-1_1225 = torch.constant.int -1
    %int640_1226 = torch.constant.int 640
    %1046 = torch.prim.ListConstruct %int2_1224, %int-1_1225, %int640_1226 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1047 = torch.aten.view %1045, %1046 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_1227 = torch.constant.int 5
    %1048 = torch.prims.convert_element_type %1047, %int5_1227 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1228 = torch.constant.int 7680
    %int640_1229 = torch.constant.int 640
    %1049 = torch.prim.ListConstruct %int7680_1228, %int640_1229 : (!torch.int, !torch.int) -> !torch.list<int>
    %1050 = torch.aten.view %1048, %1049 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16>
    %1051 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1230 = torch.constant.int 0
    %int1_1231 = torch.constant.int 1
    %1052 = torch.aten.transpose.int %1051, %int0_1230, %int1_1231 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16>
    %1053 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1232 = torch.constant.int 6
    %1054 = torch.prims.convert_element_type %1053, %int6_1232 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1233 = torch.constant.int 6
    %1055 = torch.prims.convert_element_type %1050, %int6_1233 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1234 = torch.constant.int 6
    %1056 = torch.prims.convert_element_type %1052, %int6_1234 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1057 = torch.aten.mm %1055, %1056 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1235 = torch.constant.int 1
    %1058 = torch.aten.mul.Scalar %1057, %int1_1235 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1236 = torch.constant.int 1
    %1059 = torch.aten.mul.Scalar %1054, %int1_1236 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1237 = torch.constant.int 1
    %1060 = torch.aten.add.Tensor %1058, %1059, %int1_1237 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1238 = torch.constant.int 5
    %1061 = torch.prims.convert_element_type %1060, %int5_1238 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1239 = torch.constant.int 2
    %int3840_1240 = torch.constant.int 3840
    %int640_1241 = torch.constant.int 640
    %1062 = torch.prim.ListConstruct %int2_1239, %int3840_1240, %int640_1241 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1063 = torch.aten.view %1061, %1062 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_1242 = torch.constant.none
    %1064 = torch.aten.clone %1063, %none_1242 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_1243 = torch.constant.float 1.000000e+00
    %1065 = torch.aten.div.Scalar %1064, %float1.000000e00_1243 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_1244 = torch.constant.int 1
    %1066 = torch.aten.add.Tensor %1065, %1002, %int1_1244 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_1245 = torch.constant.int 6
    %1067 = torch.prims.convert_element_type %1066, %int6_1245 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_1246 = torch.constant.int 2
    %1068 = torch.prim.ListConstruct %int2_1246 : (!torch.int) -> !torch.list<int>
    %int0_1247 = torch.constant.int 0
    %true_1248 = torch.constant.bool true
    %result0_1249, %result1_1250 = torch.aten.var_mean.correction %1067, %1068, %int0_1247, %true_1248 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_1251 = torch.constant.float 1.000000e-05
    %int1_1252 = torch.constant.int 1
    %1069 = torch.aten.add.Scalar %result0_1249, %float1.000000e-05_1251, %int1_1252 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %1070 = torch.aten.rsqrt %1069 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_1253 = torch.constant.int 1
    %1071 = torch.aten.sub.Tensor %1066, %result1_1250, %int1_1253 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %1072 = torch.aten.mul.Tensor %1071, %1070 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16>
    %1073 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1074 = torch.aten.mul.Tensor %1072, %1073 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16>
    %1075 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1254 = torch.constant.int 1
    %1076 = torch.aten.add.Tensor %1074, %1075, %int1_1254 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_1255 = torch.constant.int 5
    %1077 = torch.prims.convert_element_type %1076, %int5_1255 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1256 = torch.constant.int 7680
    %int640_1257 = torch.constant.int 640
    %1078 = torch.prim.ListConstruct %int7680_1256, %int640_1257 : (!torch.int, !torch.int) -> !torch.list<int>
    %1079 = torch.aten.view %1077, %1078 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16>
    %1080 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %int0_1258 = torch.constant.int 0
    %int1_1259 = torch.constant.int 1
    %1081 = torch.aten.transpose.int %1080, %int0_1258, %int1_1259 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16>
    %1082 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1260 = torch.constant.int 6
    %1083 = torch.prims.convert_element_type %1082, %int6_1260 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1261 = torch.constant.int 6
    %1084 = torch.prims.convert_element_type %1079, %int6_1261 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1262 = torch.constant.int 6
    %1085 = torch.prims.convert_element_type %1081, %int6_1262 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %1086 = torch.aten.mm %1084, %1085 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[7680,5120],f32>
    %int1_1263 = torch.constant.int 1
    %1087 = torch.aten.mul.Scalar %1086, %int1_1263 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int1_1264 = torch.constant.int 1
    %1088 = torch.aten.mul.Scalar %1083, %int1_1264 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1265 = torch.constant.int 1
    %1089 = torch.aten.add.Tensor %1087, %1088, %int1_1265 : !torch.vtensor<[7680,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int5_1266 = torch.constant.int 5
    %1090 = torch.prims.convert_element_type %1089, %int5_1266 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f16>
    %int2_1267 = torch.constant.int 2
    %int3840_1268 = torch.constant.int 3840
    %int5120_1269 = torch.constant.int 5120
    %1091 = torch.prim.ListConstruct %int2_1267, %int3840_1268, %int5120_1269 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1092 = torch.aten.view %1090, %1091 : !torch.vtensor<[7680,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,5120],f16>
    %int-1_1270 = torch.constant.int -1
    %int0_1271 = torch.constant.int 0
    %int2560_1272 = torch.constant.int 2560
    %int1_1273 = torch.constant.int 1
    %1093 = torch.aten.slice.Tensor %1092, %int-1_1270, %int0_1271, %int2560_1272, %int1_1273 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %int-1_1274 = torch.constant.int -1
    %int2560_1275 = torch.constant.int 2560
    %int5120_1276 = torch.constant.int 5120
    %int1_1277 = torch.constant.int 1
    %1094 = torch.aten.slice.Tensor %1092, %int-1_1274, %int2560_1275, %int5120_1276, %int1_1277 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %str_1278 = torch.constant.str "none"
    %1095 = torch.aten.gelu %1094, %str_1278 : !torch.vtensor<[2,3840,2560],f16>, !torch.str -> !torch.vtensor<[2,3840,2560],f16>
    %1096 = torch.aten.mul.Tensor %1093, %1095 : !torch.vtensor<[2,3840,2560],f16>, !torch.vtensor<[2,3840,2560],f16> -> !torch.vtensor<[2,3840,2560],f16>
    %none_1279 = torch.constant.none
    %1097 = torch.aten.clone %1096, %none_1279 : !torch.vtensor<[2,3840,2560],f16>, !torch.none -> !torch.vtensor<[2,3840,2560],f16>
    %int7680_1280 = torch.constant.int 7680
    %int2560_1281 = torch.constant.int 2560
    %1098 = torch.prim.ListConstruct %int7680_1280, %int2560_1281 : (!torch.int, !torch.int) -> !torch.list<int>
    %1099 = torch.aten.view %1097, %1098 : !torch.vtensor<[2,3840,2560],f16>, !torch.list<int> -> !torch.vtensor<[7680,2560],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16>
    %1100 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %int0_1282 = torch.constant.int 0
    %int1_1283 = torch.constant.int 1
    %1101 = torch.aten.transpose.int %1100, %int0_1282, %int1_1283 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16>
    %1102 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1284 = torch.constant.int 6
    %1103 = torch.prims.convert_element_type %1102, %int6_1284 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1285 = torch.constant.int 6
    %1104 = torch.prims.convert_element_type %1099, %int6_1285 : !torch.vtensor<[7680,2560],f16>, !torch.int -> !torch.vtensor<[7680,2560],f32>
    %int6_1286 = torch.constant.int 6
    %1105 = torch.prims.convert_element_type %1101, %int6_1286 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %1106 = torch.aten.mm %1104, %1105 : !torch.vtensor<[7680,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1287 = torch.constant.int 1
    %1107 = torch.aten.mul.Scalar %1106, %int1_1287 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1288 = torch.constant.int 1
    %1108 = torch.aten.mul.Scalar %1103, %int1_1288 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1289 = torch.constant.int 1
    %1109 = torch.aten.add.Tensor %1107, %1108, %int1_1289 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1290 = torch.constant.int 5
    %1110 = torch.prims.convert_element_type %1109, %int5_1290 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1291 = torch.constant.int 2
    %int3840_1292 = torch.constant.int 3840
    %int640_1293 = torch.constant.int 640
    %1111 = torch.prim.ListConstruct %int2_1291, %int3840_1292, %int640_1293 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1112 = torch.aten.view %1110, %1111 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int1_1294 = torch.constant.int 1
    %1113 = torch.aten.add.Tensor %1112, %1066, %int1_1294 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_1295 = torch.constant.int 6
    %1114 = torch.prims.convert_element_type %1113, %int6_1295 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_1296 = torch.constant.int 2
    %1115 = torch.prim.ListConstruct %int2_1296 : (!torch.int) -> !torch.list<int>
    %int0_1297 = torch.constant.int 0
    %true_1298 = torch.constant.bool true
    %result0_1299, %result1_1300 = torch.aten.var_mean.correction %1114, %1115, %int0_1297, %true_1298 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_1301 = torch.constant.float 1.000000e-05
    %int1_1302 = torch.constant.int 1
    %1116 = torch.aten.add.Scalar %result0_1299, %float1.000000e-05_1301, %int1_1302 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %1117 = torch.aten.rsqrt %1116 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_1303 = torch.constant.int 1
    %1118 = torch.aten.sub.Tensor %1113, %result1_1300, %int1_1303 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %1119 = torch.aten.mul.Tensor %1118, %1117 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16>
    %1120 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1121 = torch.aten.mul.Tensor %1119, %1120 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16>
    %1122 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1304 = torch.constant.int 1
    %1123 = torch.aten.add.Tensor %1121, %1122, %int1_1304 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_1305 = torch.constant.int 5
    %1124 = torch.prims.convert_element_type %1123, %int5_1305 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16>
    %1125 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1306 = torch.constant.int 0
    %int1_1307 = torch.constant.int 1
    %1126 = torch.aten.transpose.int %1125, %int0_1306, %int1_1307 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1308 = torch.constant.int 7680
    %int640_1309 = torch.constant.int 640
    %1127 = torch.prim.ListConstruct %int7680_1308, %int640_1309 : (!torch.int, !torch.int) -> !torch.list<int>
    %1128 = torch.aten.view %1124, %1127 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %1129 = torch.aten.mm %1128, %1126 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1310 = torch.constant.int 2
    %int3840_1311 = torch.constant.int 3840
    %int640_1312 = torch.constant.int 640
    %1130 = torch.prim.ListConstruct %int2_1310, %int3840_1311, %int640_1312 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1131 = torch.aten.view %1129, %1130 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16>
    %1132 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1313 = torch.constant.int 0
    %int1_1314 = torch.constant.int 1
    %1133 = torch.aten.transpose.int %1132, %int0_1313, %int1_1314 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1315 = torch.constant.int 7680
    %int640_1316 = torch.constant.int 640
    %1134 = torch.prim.ListConstruct %int7680_1315, %int640_1316 : (!torch.int, !torch.int) -> !torch.list<int>
    %1135 = torch.aten.view %1124, %1134 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %1136 = torch.aten.mm %1135, %1133 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1317 = torch.constant.int 2
    %int3840_1318 = torch.constant.int 3840
    %int640_1319 = torch.constant.int 640
    %1137 = torch.prim.ListConstruct %int2_1317, %int3840_1318, %int640_1319 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1138 = torch.aten.view %1136, %1137 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16>
    %1139 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1320 = torch.constant.int 0
    %int1_1321 = torch.constant.int 1
    %1140 = torch.aten.transpose.int %1139, %int0_1320, %int1_1321 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1322 = torch.constant.int 7680
    %int640_1323 = torch.constant.int 640
    %1141 = torch.prim.ListConstruct %int7680_1322, %int640_1323 : (!torch.int, !torch.int) -> !torch.list<int>
    %1142 = torch.aten.view %1124, %1141 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %1143 = torch.aten.mm %1142, %1140 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1324 = torch.constant.int 2
    %int3840_1325 = torch.constant.int 3840
    %int640_1326 = torch.constant.int 640
    %1144 = torch.prim.ListConstruct %int2_1324, %int3840_1325, %int640_1326 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1145 = torch.aten.view %1143, %1144 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int2_1327 = torch.constant.int 2
    %int-1_1328 = torch.constant.int -1
    %int10_1329 = torch.constant.int 10
    %int64_1330 = torch.constant.int 64
    %1146 = torch.prim.ListConstruct %int2_1327, %int-1_1328, %int10_1329, %int64_1330 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1147 = torch.aten.view %1131, %1146 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1331 = torch.constant.int 1
    %int2_1332 = torch.constant.int 2
    %1148 = torch.aten.transpose.int %1147, %int1_1331, %int2_1332 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_1333 = torch.constant.int 2
    %int-1_1334 = torch.constant.int -1
    %int10_1335 = torch.constant.int 10
    %int64_1336 = torch.constant.int 64
    %1149 = torch.prim.ListConstruct %int2_1333, %int-1_1334, %int10_1335, %int64_1336 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1150 = torch.aten.view %1138, %1149 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1337 = torch.constant.int 1
    %int2_1338 = torch.constant.int 2
    %1151 = torch.aten.transpose.int %1150, %int1_1337, %int2_1338 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_1339 = torch.constant.int 2
    %int-1_1340 = torch.constant.int -1
    %int10_1341 = torch.constant.int 10
    %int64_1342 = torch.constant.int 64
    %1152 = torch.prim.ListConstruct %int2_1339, %int-1_1340, %int10_1341, %int64_1342 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1153 = torch.aten.view %1145, %1152 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1343 = torch.constant.int 1
    %int2_1344 = torch.constant.int 2
    %1154 = torch.aten.transpose.int %1153, %int1_1343, %int2_1344 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %float0.000000e00_1345 = torch.constant.float 0.000000e+00
    %false_1346 = torch.constant.bool false
    %none_1347 = torch.constant.none
    %none_1348 = torch.constant.none
    %1155:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1148, %1151, %1154, %float0.000000e00_1345, %false_1346, %none_1347, %none_1348) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_1349 = torch.constant.int 1
    %int2_1350 = torch.constant.int 2
    %1156 = torch.aten.transpose.int %1155#0, %int1_1349, %int2_1350 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_1351 = torch.constant.int 2
    %int-1_1352 = torch.constant.int -1
    %int640_1353 = torch.constant.int 640
    %1157 = torch.prim.ListConstruct %int2_1351, %int-1_1352, %int640_1353 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1158 = torch.aten.view %1156, %1157 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_1354 = torch.constant.int 5
    %1159 = torch.prims.convert_element_type %1158, %int5_1354 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1355 = torch.constant.int 7680
    %int640_1356 = torch.constant.int 640
    %1160 = torch.prim.ListConstruct %int7680_1355, %int640_1356 : (!torch.int, !torch.int) -> !torch.list<int>
    %1161 = torch.aten.view %1159, %1160 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16>
    %1162 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1357 = torch.constant.int 0
    %int1_1358 = torch.constant.int 1
    %1163 = torch.aten.transpose.int %1162, %int0_1357, %int1_1358 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16>
    %1164 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1359 = torch.constant.int 6
    %1165 = torch.prims.convert_element_type %1164, %int6_1359 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1360 = torch.constant.int 6
    %1166 = torch.prims.convert_element_type %1161, %int6_1360 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1361 = torch.constant.int 6
    %1167 = torch.prims.convert_element_type %1163, %int6_1361 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1168 = torch.aten.mm %1166, %1167 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1362 = torch.constant.int 1
    %1169 = torch.aten.mul.Scalar %1168, %int1_1362 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1363 = torch.constant.int 1
    %1170 = torch.aten.mul.Scalar %1165, %int1_1363 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1364 = torch.constant.int 1
    %1171 = torch.aten.add.Tensor %1169, %1170, %int1_1364 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1365 = torch.constant.int 5
    %1172 = torch.prims.convert_element_type %1171, %int5_1365 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1366 = torch.constant.int 2
    %int3840_1367 = torch.constant.int 3840
    %int640_1368 = torch.constant.int 640
    %1173 = torch.prim.ListConstruct %int2_1366, %int3840_1367, %int640_1368 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1174 = torch.aten.view %1172, %1173 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_1369 = torch.constant.none
    %1175 = torch.aten.clone %1174, %none_1369 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_1370 = torch.constant.float 1.000000e+00
    %1176 = torch.aten.div.Scalar %1175, %float1.000000e00_1370 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_1371 = torch.constant.int 1
    %1177 = torch.aten.add.Tensor %1176, %1113, %int1_1371 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_1372 = torch.constant.int 6
    %1178 = torch.prims.convert_element_type %1177, %int6_1372 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_1373 = torch.constant.int 2
    %1179 = torch.prim.ListConstruct %int2_1373 : (!torch.int) -> !torch.list<int>
    %int0_1374 = torch.constant.int 0
    %true_1375 = torch.constant.bool true
    %result0_1376, %result1_1377 = torch.aten.var_mean.correction %1178, %1179, %int0_1374, %true_1375 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_1378 = torch.constant.float 1.000000e-05
    %int1_1379 = torch.constant.int 1
    %1180 = torch.aten.add.Scalar %result0_1376, %float1.000000e-05_1378, %int1_1379 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %1181 = torch.aten.rsqrt %1180 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_1380 = torch.constant.int 1
    %1182 = torch.aten.sub.Tensor %1177, %result1_1377, %int1_1380 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %1183 = torch.aten.mul.Tensor %1182, %1181 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16>
    %1184 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1185 = torch.aten.mul.Tensor %1183, %1184 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16>
    %1186 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1381 = torch.constant.int 1
    %1187 = torch.aten.add.Tensor %1185, %1186, %int1_1381 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_1382 = torch.constant.int 5
    %1188 = torch.prims.convert_element_type %1187, %int5_1382 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16>
    %1189 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1383 = torch.constant.int 0
    %int1_1384 = torch.constant.int 1
    %1190 = torch.aten.transpose.int %1189, %int0_1383, %int1_1384 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %int7680_1385 = torch.constant.int 7680
    %int640_1386 = torch.constant.int 640
    %1191 = torch.prim.ListConstruct %int7680_1385, %int640_1386 : (!torch.int, !torch.int) -> !torch.list<int>
    %1192 = torch.aten.view %1188, %1191 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %1193 = torch.aten.mm %1192, %1190 : !torch.vtensor<[7680,640],f16>, !torch.vtensor<[640,640],f16> -> !torch.vtensor<[7680,640],f16>
    %int2_1387 = torch.constant.int 2
    %int3840_1388 = torch.constant.int 3840
    %int640_1389 = torch.constant.int 640
    %1194 = torch.prim.ListConstruct %int2_1387, %int3840_1388, %int640_1389 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1195 = torch.aten.view %1193, %1194 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16>
    %1196 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_1390 = torch.constant.int 0
    %int1_1391 = torch.constant.int 1
    %1197 = torch.aten.transpose.int %1196, %int0_1390, %int1_1391 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_1392 = torch.constant.int 32
    %int2048_1393 = torch.constant.int 2048
    %1198 = torch.prim.ListConstruct %int32_1392, %int2048_1393 : (!torch.int, !torch.int) -> !torch.list<int>
    %1199 = torch.aten.view %arg6, %1198 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1200 = torch.aten.mm %1199, %1197 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_1394 = torch.constant.int 2
    %int16_1395 = torch.constant.int 16
    %int640_1396 = torch.constant.int 640
    %1201 = torch.prim.ListConstruct %int2_1394, %int16_1395, %int640_1396 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1202 = torch.aten.view %1200, %1201 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16>
    %1203 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<640x2048xf16> -> !torch.vtensor<[640,2048],f16>
    %int0_1397 = torch.constant.int 0
    %int1_1398 = torch.constant.int 1
    %1204 = torch.aten.transpose.int %1203, %int0_1397, %int1_1398 : !torch.vtensor<[640,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,640],f16>
    %int32_1399 = torch.constant.int 32
    %int2048_1400 = torch.constant.int 2048
    %1205 = torch.prim.ListConstruct %int32_1399, %int2048_1400 : (!torch.int, !torch.int) -> !torch.list<int>
    %1206 = torch.aten.view %arg6, %1205 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1207 = torch.aten.mm %1206, %1204 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,640],f16> -> !torch.vtensor<[32,640],f16>
    %int2_1401 = torch.constant.int 2
    %int16_1402 = torch.constant.int 16
    %int640_1403 = torch.constant.int 640
    %1208 = torch.prim.ListConstruct %int2_1401, %int16_1402, %int640_1403 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1209 = torch.aten.view %1207, %1208 : !torch.vtensor<[32,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,640],f16>
    %int2_1404 = torch.constant.int 2
    %int-1_1405 = torch.constant.int -1
    %int10_1406 = torch.constant.int 10
    %int64_1407 = torch.constant.int 64
    %1210 = torch.prim.ListConstruct %int2_1404, %int-1_1405, %int10_1406, %int64_1407 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1211 = torch.aten.view %1195, %1210 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,10,64],f16>
    %int1_1408 = torch.constant.int 1
    %int2_1409 = torch.constant.int 2
    %1212 = torch.aten.transpose.int %1211, %int1_1408, %int2_1409 : !torch.vtensor<[2,3840,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,3840,64],f16>
    %int2_1410 = torch.constant.int 2
    %int-1_1411 = torch.constant.int -1
    %int10_1412 = torch.constant.int 10
    %int64_1413 = torch.constant.int 64
    %1213 = torch.prim.ListConstruct %int2_1410, %int-1_1411, %int10_1412, %int64_1413 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1214 = torch.aten.view %1202, %1213 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_1414 = torch.constant.int 1
    %int2_1415 = torch.constant.int 2
    %1215 = torch.aten.transpose.int %1214, %int1_1414, %int2_1415 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %int2_1416 = torch.constant.int 2
    %int-1_1417 = torch.constant.int -1
    %int10_1418 = torch.constant.int 10
    %int64_1419 = torch.constant.int 64
    %1216 = torch.prim.ListConstruct %int2_1416, %int-1_1417, %int10_1418, %int64_1419 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1217 = torch.aten.view %1209, %1216 : !torch.vtensor<[2,16,640],f16>, !torch.list<int> -> !torch.vtensor<[2,16,10,64],f16>
    %int1_1420 = torch.constant.int 1
    %int2_1421 = torch.constant.int 2
    %1218 = torch.aten.transpose.int %1217, %int1_1420, %int2_1421 : !torch.vtensor<[2,16,10,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,10,16,64],f16>
    %float0.000000e00_1422 = torch.constant.float 0.000000e+00
    %false_1423 = torch.constant.bool false
    %none_1424 = torch.constant.none
    %none_1425 = torch.constant.none
    %1219:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1212, %1215, %1218, %float0.000000e00_1422, %false_1423, %none_1424, %none_1425) : (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.vtensor<[2,10,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,10,3840,64],f16>, !torch.vtensor<[2,10,3840],f32>) 
    %int1_1426 = torch.constant.int 1
    %int2_1427 = torch.constant.int 2
    %1220 = torch.aten.transpose.int %1219#0, %int1_1426, %int2_1427 : !torch.vtensor<[2,10,3840,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,3840,10,64],f16>
    %int2_1428 = torch.constant.int 2
    %int-1_1429 = torch.constant.int -1
    %int640_1430 = torch.constant.int 640
    %1221 = torch.prim.ListConstruct %int2_1428, %int-1_1429, %int640_1430 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1222 = torch.aten.view %1220, %1221 : !torch.vtensor<[2,3840,10,64],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int5_1431 = torch.constant.int 5
    %1223 = torch.prims.convert_element_type %1222, %int5_1431 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1432 = torch.constant.int 7680
    %int640_1433 = torch.constant.int 640
    %1224 = torch.prim.ListConstruct %int7680_1432, %int640_1433 : (!torch.int, !torch.int) -> !torch.list<int>
    %1225 = torch.aten.view %1223, %1224 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16>
    %1226 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1434 = torch.constant.int 0
    %int1_1435 = torch.constant.int 1
    %1227 = torch.aten.transpose.int %1226, %int0_1434, %int1_1435 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16>
    %1228 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1436 = torch.constant.int 6
    %1229 = torch.prims.convert_element_type %1228, %int6_1436 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1437 = torch.constant.int 6
    %1230 = torch.prims.convert_element_type %1225, %int6_1437 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1438 = torch.constant.int 6
    %1231 = torch.prims.convert_element_type %1227, %int6_1438 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1232 = torch.aten.mm %1230, %1231 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1439 = torch.constant.int 1
    %1233 = torch.aten.mul.Scalar %1232, %int1_1439 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1440 = torch.constant.int 1
    %1234 = torch.aten.mul.Scalar %1229, %int1_1440 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1441 = torch.constant.int 1
    %1235 = torch.aten.add.Tensor %1233, %1234, %int1_1441 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1442 = torch.constant.int 5
    %1236 = torch.prims.convert_element_type %1235, %int5_1442 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1443 = torch.constant.int 2
    %int3840_1444 = torch.constant.int 3840
    %int640_1445 = torch.constant.int 640
    %1237 = torch.prim.ListConstruct %int2_1443, %int3840_1444, %int640_1445 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1238 = torch.aten.view %1236, %1237 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %none_1446 = torch.constant.none
    %1239 = torch.aten.clone %1238, %none_1446 : !torch.vtensor<[2,3840,640],f16>, !torch.none -> !torch.vtensor<[2,3840,640],f16>
    %float1.000000e00_1447 = torch.constant.float 1.000000e+00
    %1240 = torch.aten.div.Scalar %1239, %float1.000000e00_1447 : !torch.vtensor<[2,3840,640],f16>, !torch.float -> !torch.vtensor<[2,3840,640],f16>
    %int1_1448 = torch.constant.int 1
    %1241 = torch.aten.add.Tensor %1240, %1177, %int1_1448 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int6_1449 = torch.constant.int 6
    %1242 = torch.prims.convert_element_type %1241, %int6_1449 : !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int2_1450 = torch.constant.int 2
    %1243 = torch.prim.ListConstruct %int2_1450 : (!torch.int) -> !torch.list<int>
    %int0_1451 = torch.constant.int 0
    %true_1452 = torch.constant.bool true
    %result0_1453, %result1_1454 = torch.aten.var_mean.correction %1242, %1243, %int0_1451, %true_1452 : !torch.vtensor<[2,3840,640],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,3840,1],f32>, !torch.vtensor<[2,3840,1],f32>
    %float1.000000e-05_1455 = torch.constant.float 1.000000e-05
    %int1_1456 = torch.constant.int 1
    %1244 = torch.aten.add.Scalar %result0_1453, %float1.000000e-05_1455, %int1_1456 : !torch.vtensor<[2,3840,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,3840,1],f32>
    %1245 = torch.aten.rsqrt %1244 : !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,1],f32>
    %int1_1457 = torch.constant.int 1
    %1246 = torch.aten.sub.Tensor %1241, %result1_1454, %int1_1457 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,1],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %1247 = torch.aten.mul.Tensor %1246, %1245 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[2,3840,1],f32> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16>
    %1248 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %1249 = torch.aten.mul.Tensor %1247, %1248 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16> -> !torch.vtensor<[2,3840,640],f32>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16>
    %1250 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.norm3.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_1458 = torch.constant.int 1
    %1251 = torch.aten.add.Tensor %1249, %1250, %int1_1458 : !torch.vtensor<[2,3840,640],f32>, !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f32>
    %int5_1459 = torch.constant.int 5
    %1252 = torch.prims.convert_element_type %1251, %int5_1459 : !torch.vtensor<[2,3840,640],f32>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1460 = torch.constant.int 7680
    %int640_1461 = torch.constant.int 640
    %1253 = torch.prim.ListConstruct %int7680_1460, %int640_1461 : (!torch.int, !torch.int) -> !torch.list<int>
    %1254 = torch.aten.view %1252, %1253 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16>
    %1255 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<5120x640xf16> -> !torch.vtensor<[5120,640],f16>
    %int0_1462 = torch.constant.int 0
    %int1_1463 = torch.constant.int 1
    %1256 = torch.aten.transpose.int %1255, %int0_1462, %int1_1463 : !torch.vtensor<[5120,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,5120],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16>
    %1257 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<5120xf16> -> !torch.vtensor<[5120],f16>
    %int6_1464 = torch.constant.int 6
    %1258 = torch.prims.convert_element_type %1257, %int6_1464 : !torch.vtensor<[5120],f16>, !torch.int -> !torch.vtensor<[5120],f32>
    %int6_1465 = torch.constant.int 6
    %1259 = torch.prims.convert_element_type %1254, %int6_1465 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1466 = torch.constant.int 6
    %1260 = torch.prims.convert_element_type %1256, %int6_1466 : !torch.vtensor<[640,5120],f16>, !torch.int -> !torch.vtensor<[640,5120],f32>
    %1261 = torch.aten.mm %1259, %1260 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,5120],f32> -> !torch.vtensor<[7680,5120],f32>
    %int1_1467 = torch.constant.int 1
    %1262 = torch.aten.mul.Scalar %1261, %int1_1467 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int1_1468 = torch.constant.int 1
    %1263 = torch.aten.mul.Scalar %1258, %int1_1468 : !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[5120],f32>
    %int1_1469 = torch.constant.int 1
    %1264 = torch.aten.add.Tensor %1262, %1263, %int1_1469 : !torch.vtensor<[7680,5120],f32>, !torch.vtensor<[5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f32>
    %int5_1470 = torch.constant.int 5
    %1265 = torch.prims.convert_element_type %1264, %int5_1470 : !torch.vtensor<[7680,5120],f32>, !torch.int -> !torch.vtensor<[7680,5120],f16>
    %int2_1471 = torch.constant.int 2
    %int3840_1472 = torch.constant.int 3840
    %int5120_1473 = torch.constant.int 5120
    %1266 = torch.prim.ListConstruct %int2_1471, %int3840_1472, %int5120_1473 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1267 = torch.aten.view %1265, %1266 : !torch.vtensor<[7680,5120],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,5120],f16>
    %int-1_1474 = torch.constant.int -1
    %int0_1475 = torch.constant.int 0
    %int2560_1476 = torch.constant.int 2560
    %int1_1477 = torch.constant.int 1
    %1268 = torch.aten.slice.Tensor %1267, %int-1_1474, %int0_1475, %int2560_1476, %int1_1477 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %int-1_1478 = torch.constant.int -1
    %int2560_1479 = torch.constant.int 2560
    %int5120_1480 = torch.constant.int 5120
    %int1_1481 = torch.constant.int 1
    %1269 = torch.aten.slice.Tensor %1267, %int-1_1478, %int2560_1479, %int5120_1480, %int1_1481 : !torch.vtensor<[2,3840,5120],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,3840,2560],f16>
    %str_1482 = torch.constant.str "none"
    %1270 = torch.aten.gelu %1269, %str_1482 : !torch.vtensor<[2,3840,2560],f16>, !torch.str -> !torch.vtensor<[2,3840,2560],f16>
    %1271 = torch.aten.mul.Tensor %1268, %1270 : !torch.vtensor<[2,3840,2560],f16>, !torch.vtensor<[2,3840,2560],f16> -> !torch.vtensor<[2,3840,2560],f16>
    %none_1483 = torch.constant.none
    %1272 = torch.aten.clone %1271, %none_1483 : !torch.vtensor<[2,3840,2560],f16>, !torch.none -> !torch.vtensor<[2,3840,2560],f16>
    %int7680_1484 = torch.constant.int 7680
    %int2560_1485 = torch.constant.int 2560
    %1273 = torch.prim.ListConstruct %int7680_1484, %int2560_1485 : (!torch.int, !torch.int) -> !torch.list<int>
    %1274 = torch.aten.view %1272, %1273 : !torch.vtensor<[2,3840,2560],f16>, !torch.list<int> -> !torch.vtensor<[7680,2560],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16>
    %1275 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<640x2560xf16> -> !torch.vtensor<[640,2560],f16>
    %int0_1486 = torch.constant.int 0
    %int1_1487 = torch.constant.int 1
    %1276 = torch.aten.transpose.int %1275, %int0_1486, %int1_1487 : !torch.vtensor<[640,2560],f16>, !torch.int, !torch.int -> !torch.vtensor<[2560,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16>
    %1277 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1488 = torch.constant.int 6
    %1278 = torch.prims.convert_element_type %1277, %int6_1488 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1489 = torch.constant.int 6
    %1279 = torch.prims.convert_element_type %1274, %int6_1489 : !torch.vtensor<[7680,2560],f16>, !torch.int -> !torch.vtensor<[7680,2560],f32>
    %int6_1490 = torch.constant.int 6
    %1280 = torch.prims.convert_element_type %1276, %int6_1490 : !torch.vtensor<[2560,640],f16>, !torch.int -> !torch.vtensor<[2560,640],f32>
    %1281 = torch.aten.mm %1279, %1280 : !torch.vtensor<[7680,2560],f32>, !torch.vtensor<[2560,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1491 = torch.constant.int 1
    %1282 = torch.aten.mul.Scalar %1281, %int1_1491 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1492 = torch.constant.int 1
    %1283 = torch.aten.mul.Scalar %1278, %int1_1492 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1493 = torch.constant.int 1
    %1284 = torch.aten.add.Tensor %1282, %1283, %int1_1493 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1494 = torch.constant.int 5
    %1285 = torch.prims.convert_element_type %1284, %int5_1494 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1495 = torch.constant.int 2
    %int3840_1496 = torch.constant.int 3840
    %int640_1497 = torch.constant.int 640
    %1286 = torch.prim.ListConstruct %int2_1495, %int3840_1496, %int640_1497 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1287 = torch.aten.view %1285, %1286 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int1_1498 = torch.constant.int 1
    %1288 = torch.aten.add.Tensor %1287, %1241, %int1_1498 : !torch.vtensor<[2,3840,640],f16>, !torch.vtensor<[2,3840,640],f16>, !torch.int -> !torch.vtensor<[2,3840,640],f16>
    %int7680_1499 = torch.constant.int 7680
    %int640_1500 = torch.constant.int 640
    %1289 = torch.prim.ListConstruct %int7680_1499, %int640_1500 : (!torch.int, !torch.int) -> !torch.list<int>
    %1290 = torch.aten.view %1288, %1289 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[7680,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.proj_out.weight = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16>
    %1291 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.proj_out.weight : tensor<640x640xf16> -> !torch.vtensor<[640,640],f16>
    %int0_1501 = torch.constant.int 0
    %int1_1502 = torch.constant.int 1
    %1292 = torch.aten.transpose.int %1291, %int0_1501, %int1_1502 : !torch.vtensor<[640,640],f16>, !torch.int, !torch.int -> !torch.vtensor<[640,640],f16>
    %__auto.controlnet.down_blocks.1.attentions.1.proj_out.bias = util.global.load @__auto.controlnet.down_blocks.1.attentions.1.proj_out.bias : tensor<640xf16>
    %1293 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.attentions.1.proj_out.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int6_1503 = torch.constant.int 6
    %1294 = torch.prims.convert_element_type %1293, %int6_1503 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[640],f32>
    %int6_1504 = torch.constant.int 6
    %1295 = torch.prims.convert_element_type %1290, %int6_1504 : !torch.vtensor<[7680,640],f16>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int6_1505 = torch.constant.int 6
    %1296 = torch.prims.convert_element_type %1292, %int6_1505 : !torch.vtensor<[640,640],f16>, !torch.int -> !torch.vtensor<[640,640],f32>
    %1297 = torch.aten.mm %1295, %1296 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640,640],f32> -> !torch.vtensor<[7680,640],f32>
    %int1_1506 = torch.constant.int 1
    %1298 = torch.aten.mul.Scalar %1297, %int1_1506 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int1_1507 = torch.constant.int 1
    %1299 = torch.aten.mul.Scalar %1294, %int1_1507 : !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[640],f32>
    %int1_1508 = torch.constant.int 1
    %1300 = torch.aten.add.Tensor %1298, %1299, %int1_1508 : !torch.vtensor<[7680,640],f32>, !torch.vtensor<[640],f32>, !torch.int -> !torch.vtensor<[7680,640],f32>
    %int5_1509 = torch.constant.int 5
    %1301 = torch.prims.convert_element_type %1300, %int5_1509 : !torch.vtensor<[7680,640],f32>, !torch.int -> !torch.vtensor<[7680,640],f16>
    %int2_1510 = torch.constant.int 2
    %int3840_1511 = torch.constant.int 3840
    %int640_1512 = torch.constant.int 640
    %1302 = torch.prim.ListConstruct %int2_1510, %int3840_1511, %int640_1512 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1303 = torch.aten.view %1301, %1302 : !torch.vtensor<[7680,640],f16>, !torch.list<int> -> !torch.vtensor<[2,3840,640],f16>
    %int2_1513 = torch.constant.int 2
    %int60_1514 = torch.constant.int 60
    %int64_1515 = torch.constant.int 64
    %int640_1516 = torch.constant.int 640
    %1304 = torch.prim.ListConstruct %int2_1513, %int60_1514, %int64_1515, %int640_1516 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1305 = torch.aten.view %1303, %1304 : !torch.vtensor<[2,3840,640],f16>, !torch.list<int> -> !torch.vtensor<[2,60,64,640],f16>
    %int0_1517 = torch.constant.int 0
    %int3_1518 = torch.constant.int 3
    %int1_1519 = torch.constant.int 1
    %int2_1520 = torch.constant.int 2
    %1306 = torch.prim.ListConstruct %int0_1517, %int3_1518, %int1_1519, %int2_1520 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1307 = torch.aten.permute %1305, %1306 : !torch.vtensor<[2,60,64,640],f16>, !torch.list<int> -> !torch.vtensor<[2,640,60,64],f16>
    %int0_1521 = torch.constant.int 0
    %1308 = torch.aten.clone %1307, %int0_1521 : !torch.vtensor<[2,640,60,64],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %int1_1522 = torch.constant.int 1
    %1309 = torch.aten.add.Tensor %1308, %903, %int1_1522 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.down_blocks.1.downsamplers.0.conv.weight = util.global.load @__auto.controlnet.down_blocks.1.downsamplers.0.conv.weight : tensor<640x640x3x3xf16>
    %1310 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.downsamplers.0.conv.weight : tensor<640x640x3x3xf16> -> !torch.vtensor<[640,640,3,3],f16>
    %__auto.controlnet.down_blocks.1.downsamplers.0.conv.bias = util.global.load @__auto.controlnet.down_blocks.1.downsamplers.0.conv.bias : tensor<640xf16>
    %1311 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.1.downsamplers.0.conv.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int2_1523 = torch.constant.int 2
    %int2_1524 = torch.constant.int 2
    %1312 = torch.prim.ListConstruct %int2_1523, %int2_1524 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1525 = torch.constant.int 1
    %int1_1526 = torch.constant.int 1
    %1313 = torch.prim.ListConstruct %int1_1525, %int1_1526 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1527 = torch.constant.int 1
    %int1_1528 = torch.constant.int 1
    %1314 = torch.prim.ListConstruct %int1_1527, %int1_1528 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_1529 = torch.constant.bool false
    %int0_1530 = torch.constant.int 0
    %int0_1531 = torch.constant.int 0
    %1315 = torch.prim.ListConstruct %int0_1530, %int0_1531 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1532 = torch.constant.int 1
    %1316 = torch.aten.convolution %1309, %1310, %1311, %1312, %1313, %1314, %false_1529, %1315, %int1_1532 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[640,640,3,3],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,30,32],f16>
    %int2_1533 = torch.constant.int 2
    %int32_1534 = torch.constant.int 32
    %int20_1535 = torch.constant.int 20
    %int960 = torch.constant.int 960
    %1317 = torch.prim.ListConstruct %int2_1533, %int32_1534, %int20_1535, %int960 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1318 = torch.aten.view %1316, %1317 : !torch.vtensor<[2,640,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,20,960],f16>
    %int6_1536 = torch.constant.int 6
    %1319 = torch.prims.convert_element_type %1318, %int6_1536 : !torch.vtensor<[2,32,20,960],f16>, !torch.int -> !torch.vtensor<[2,32,20,960],f32>
    %int2_1537 = torch.constant.int 2
    %int3_1538 = torch.constant.int 3
    %1320 = torch.prim.ListConstruct %int2_1537, %int3_1538 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_1539 = torch.constant.int 0
    %true_1540 = torch.constant.bool true
    %result0_1541, %result1_1542 = torch.aten.var_mean.correction %1319, %1320, %int0_1539, %true_1540 : !torch.vtensor<[2,32,20,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_1543 = torch.constant.float 1.000000e-05
    %int1_1544 = torch.constant.int 1
    %1321 = torch.aten.add.Scalar %result0_1541, %float1.000000e-05_1543, %int1_1544 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1322 = torch.aten.rsqrt %1321 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_1545 = torch.constant.int 1
    %1323 = torch.aten.sub.Tensor %1318, %result1_1542, %int1_1545 : !torch.vtensor<[2,32,20,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,20,960],f32>
    %1324 = torch.aten.mul.Tensor %1323, %1322 : !torch.vtensor<[2,32,20,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,20,960],f32>
    %int2_1546 = torch.constant.int 2
    %int640_1547 = torch.constant.int 640
    %int30 = torch.constant.int 30
    %int32_1548 = torch.constant.int 32
    %1325 = torch.prim.ListConstruct %int2_1546, %int640_1547, %int30, %int32_1548 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1326 = torch.aten.view %1324, %1325 : !torch.vtensor<[2,32,20,960],f32>, !torch.list<int> -> !torch.vtensor<[2,640,30,32],f32>
    %__auto.controlnet.down_blocks.2.resnets.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.norm1.bias : tensor<640xf16>
    %1327 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.norm1.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_1549 = torch.constant.int 0
    %1328 = torch.aten.unsqueeze %1327, %int0_1549 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_1550 = torch.constant.int 2
    %1329 = torch.aten.unsqueeze %1328, %int2_1550 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_1551 = torch.constant.int 3
    %1330 = torch.aten.unsqueeze %1329, %int3_1551 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.norm1.weight : tensor<640xf16>
    %1331 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.norm1.weight : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int0_1552 = torch.constant.int 0
    %1332 = torch.aten.unsqueeze %1331, %int0_1552 : !torch.vtensor<[640],f16>, !torch.int -> !torch.vtensor<[1,640],f16>
    %int2_1553 = torch.constant.int 2
    %1333 = torch.aten.unsqueeze %1332, %int2_1553 : !torch.vtensor<[1,640],f16>, !torch.int -> !torch.vtensor<[1,640,1],f16>
    %int3_1554 = torch.constant.int 3
    %1334 = torch.aten.unsqueeze %1333, %int3_1554 : !torch.vtensor<[1,640,1],f16>, !torch.int -> !torch.vtensor<[1,640,1,1],f16>
    %1335 = torch.aten.mul.Tensor %1326, %1334 : !torch.vtensor<[2,640,30,32],f32>, !torch.vtensor<[1,640,1,1],f16> -> !torch.vtensor<[2,640,30,32],f32>
    %int1_1555 = torch.constant.int 1
    %1336 = torch.aten.add.Tensor %1335, %1330, %int1_1555 : !torch.vtensor<[2,640,30,32],f32>, !torch.vtensor<[1,640,1,1],f16>, !torch.int -> !torch.vtensor<[2,640,30,32],f32>
    %int5_1556 = torch.constant.int 5
    %1337 = torch.prims.convert_element_type %1336, %int5_1556 : !torch.vtensor<[2,640,30,32],f32>, !torch.int -> !torch.vtensor<[2,640,30,32],f16>
    %1338 = torch.aten.silu %1337 : !torch.vtensor<[2,640,30,32],f16> -> !torch.vtensor<[2,640,30,32],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.conv1.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.conv1.weight : tensor<1280x640x3x3xf16>
    %1339 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.conv1.weight : tensor<1280x640x3x3xf16> -> !torch.vtensor<[1280,640,3,3],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.conv1.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.conv1.bias : tensor<1280xf16>
    %1340 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1557 = torch.constant.int 1
    %int1_1558 = torch.constant.int 1
    %1341 = torch.prim.ListConstruct %int1_1557, %int1_1558 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1559 = torch.constant.int 1
    %int1_1560 = torch.constant.int 1
    %1342 = torch.prim.ListConstruct %int1_1559, %int1_1560 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1561 = torch.constant.int 1
    %int1_1562 = torch.constant.int 1
    %1343 = torch.prim.ListConstruct %int1_1561, %int1_1562 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_1563 = torch.constant.bool false
    %int0_1564 = torch.constant.int 0
    %int0_1565 = torch.constant.int 0
    %1344 = torch.prim.ListConstruct %int0_1564, %int0_1565 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1566 = torch.constant.int 1
    %1345 = torch.aten.convolution %1338, %1339, %1340, %1341, %1342, %1343, %false_1563, %1344, %int1_1566 : !torch.vtensor<[2,640,30,32],f16>, !torch.vtensor<[1280,640,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %1346 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %1347 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1567 = torch.constant.int 0
    %int1_1568 = torch.constant.int 1
    %1348 = torch.aten.transpose.int %1347, %int0_1567, %int1_1568 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %1349 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1569 = torch.constant.int 6
    %1350 = torch.prims.convert_element_type %1349, %int6_1569 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1570 = torch.constant.int 6
    %1351 = torch.prims.convert_element_type %1346, %int6_1570 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_1571 = torch.constant.int 6
    %1352 = torch.prims.convert_element_type %1348, %int6_1571 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1353 = torch.aten.mm %1351, %1352 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_1572 = torch.constant.int 1
    %1354 = torch.aten.mul.Scalar %1353, %int1_1572 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_1573 = torch.constant.int 1
    %1355 = torch.aten.mul.Scalar %1350, %int1_1573 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1574 = torch.constant.int 1
    %1356 = torch.aten.add.Tensor %1354, %1355, %int1_1574 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_1575 = torch.constant.int 5
    %1357 = torch.prims.convert_element_type %1356, %int5_1575 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int0_1576 = torch.constant.int 0
    %int0_1577 = torch.constant.int 0
    %int9223372036854775807_1578 = torch.constant.int 9223372036854775807
    %int1_1579 = torch.constant.int 1
    %1358 = torch.aten.slice.Tensor %1357, %int0_1576, %int0_1577, %int9223372036854775807_1578, %int1_1579 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int1_1580 = torch.constant.int 1
    %int0_1581 = torch.constant.int 0
    %int9223372036854775807_1582 = torch.constant.int 9223372036854775807
    %int1_1583 = torch.constant.int 1
    %1359 = torch.aten.slice.Tensor %1358, %int1_1580, %int0_1581, %int9223372036854775807_1582, %int1_1583 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int2_1584 = torch.constant.int 2
    %1360 = torch.aten.unsqueeze %1359, %int2_1584 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %int3_1585 = torch.constant.int 3
    %1361 = torch.aten.unsqueeze %1360, %int3_1585 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %int1_1586 = torch.constant.int 1
    %1362 = torch.aten.add.Tensor %1345, %1361, %int1_1586 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_1587 = torch.constant.int 2
    %int32_1588 = torch.constant.int 32
    %int40 = torch.constant.int 40
    %int960_1589 = torch.constant.int 960
    %1363 = torch.prim.ListConstruct %int2_1587, %int32_1588, %int40, %int960_1589 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1364 = torch.aten.view %1362, %1363 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_1590 = torch.constant.int 6
    %1365 = torch.prims.convert_element_type %1364, %int6_1590 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_1591 = torch.constant.int 2
    %int3_1592 = torch.constant.int 3
    %1366 = torch.prim.ListConstruct %int2_1591, %int3_1592 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_1593 = torch.constant.int 0
    %true_1594 = torch.constant.bool true
    %result0_1595, %result1_1596 = torch.aten.var_mean.correction %1365, %1366, %int0_1593, %true_1594 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_1597 = torch.constant.float 1.000000e-05
    %int1_1598 = torch.constant.int 1
    %1367 = torch.aten.add.Scalar %result0_1595, %float1.000000e-05_1597, %int1_1598 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1368 = torch.aten.rsqrt %1367 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_1599 = torch.constant.int 1
    %1369 = torch.aten.sub.Tensor %1364, %result1_1596, %int1_1599 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %1370 = torch.aten.mul.Tensor %1369, %1368 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_1600 = torch.constant.int 2
    %int1280 = torch.constant.int 1280
    %int30_1601 = torch.constant.int 30
    %int32_1602 = torch.constant.int 32
    %1371 = torch.prim.ListConstruct %int2_1600, %int1280, %int30_1601, %int32_1602 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1372 = torch.aten.view %1370, %1371 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.down_blocks.2.resnets.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.norm2.bias : tensor<1280xf16>
    %1373 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_1603 = torch.constant.int 0
    %1374 = torch.aten.unsqueeze %1373, %int0_1603 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_1604 = torch.constant.int 2
    %1375 = torch.aten.unsqueeze %1374, %int2_1604 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_1605 = torch.constant.int 3
    %1376 = torch.aten.unsqueeze %1375, %int3_1605 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.norm2.weight : tensor<1280xf16>
    %1377 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_1606 = torch.constant.int 0
    %1378 = torch.aten.unsqueeze %1377, %int0_1606 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_1607 = torch.constant.int 2
    %1379 = torch.aten.unsqueeze %1378, %int2_1607 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_1608 = torch.constant.int 3
    %1380 = torch.aten.unsqueeze %1379, %int3_1608 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %1381 = torch.aten.mul.Tensor %1372, %1380 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_1609 = torch.constant.int 1
    %1382 = torch.aten.add.Tensor %1381, %1376, %int1_1609 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_1610 = torch.constant.int 5
    %1383 = torch.prims.convert_element_type %1382, %int5_1610 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %1384 = torch.aten.silu %1383 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %none_1611 = torch.constant.none
    %1385 = torch.aten.clone %1384, %none_1611 : !torch.vtensor<[2,1280,30,32],f16>, !torch.none -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.conv2.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %1386 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.conv2.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.conv2.bias : tensor<1280xf16>
    %1387 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1612 = torch.constant.int 1
    %int1_1613 = torch.constant.int 1
    %1388 = torch.prim.ListConstruct %int1_1612, %int1_1613 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1614 = torch.constant.int 1
    %int1_1615 = torch.constant.int 1
    %1389 = torch.prim.ListConstruct %int1_1614, %int1_1615 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1616 = torch.constant.int 1
    %int1_1617 = torch.constant.int 1
    %1390 = torch.prim.ListConstruct %int1_1616, %int1_1617 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_1618 = torch.constant.bool false
    %int0_1619 = torch.constant.int 0
    %int0_1620 = torch.constant.int 0
    %1391 = torch.prim.ListConstruct %int0_1619, %int0_1620 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1621 = torch.constant.int 1
    %1392 = torch.aten.convolution %1385, %1386, %1387, %1388, %1389, %1390, %false_1618, %1391, %int1_1621 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.weight : tensor<1280x640x1x1xf16>
    %1393 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.weight : tensor<1280x640x1x1xf16> -> !torch.vtensor<[1280,640,1,1],f16>
    %__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.bias : tensor<1280xf16>
    %1394 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.0.conv_shortcut.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1622 = torch.constant.int 1
    %int1_1623 = torch.constant.int 1
    %1395 = torch.prim.ListConstruct %int1_1622, %int1_1623 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_1624 = torch.constant.int 0
    %int0_1625 = torch.constant.int 0
    %1396 = torch.prim.ListConstruct %int0_1624, %int0_1625 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1626 = torch.constant.int 1
    %int1_1627 = torch.constant.int 1
    %1397 = torch.prim.ListConstruct %int1_1626, %int1_1627 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_1628 = torch.constant.bool false
    %int0_1629 = torch.constant.int 0
    %int0_1630 = torch.constant.int 0
    %1398 = torch.prim.ListConstruct %int0_1629, %int0_1630 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_1631 = torch.constant.int 1
    %1399 = torch.aten.convolution %1316, %1393, %1394, %1395, %1396, %1397, %false_1628, %1398, %int1_1631 : !torch.vtensor<[2,640,30,32],f16>, !torch.vtensor<[1280,640,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_1632 = torch.constant.int 1
    %1400 = torch.aten.add.Tensor %1399, %1392, %int1_1632 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %float1.000000e00_1633 = torch.constant.float 1.000000e+00
    %1401 = torch.aten.div.Scalar %1400, %float1.000000e00_1633 : !torch.vtensor<[2,1280,30,32],f16>, !torch.float -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_1634 = torch.constant.int 2
    %int32_1635 = torch.constant.int 32
    %int40_1636 = torch.constant.int 40
    %int960_1637 = torch.constant.int 960
    %1402 = torch.prim.ListConstruct %int2_1634, %int32_1635, %int40_1636, %int960_1637 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1403 = torch.aten.view %1401, %1402 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_1638 = torch.constant.int 6
    %1404 = torch.prims.convert_element_type %1403, %int6_1638 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_1639 = torch.constant.int 2
    %int3_1640 = torch.constant.int 3
    %1405 = torch.prim.ListConstruct %int2_1639, %int3_1640 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_1641 = torch.constant.int 0
    %true_1642 = torch.constant.bool true
    %result0_1643, %result1_1644 = torch.aten.var_mean.correction %1404, %1405, %int0_1641, %true_1642 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float9.999990e-07_1645 = torch.constant.float 9.9999999999999995E-7
    %int1_1646 = torch.constant.int 1
    %1406 = torch.aten.add.Scalar %result0_1643, %float9.999990e-07_1645, %int1_1646 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %1407 = torch.aten.rsqrt %1406 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_1647 = torch.constant.int 1
    %1408 = torch.aten.sub.Tensor %1403, %result1_1644, %int1_1647 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %1409 = torch.aten.mul.Tensor %1408, %1407 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_1648 = torch.constant.int 2
    %int1280_1649 = torch.constant.int 1280
    %int30_1650 = torch.constant.int 30
    %int32_1651 = torch.constant.int 32
    %1410 = torch.prim.ListConstruct %int2_1648, %int1280_1649, %int30_1650, %int32_1651 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1411 = torch.aten.view %1409, %1410 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.norm.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.norm.bias : tensor<1280xf16>
    %1412 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_1652 = torch.constant.int 0
    %1413 = torch.aten.unsqueeze %1412, %int0_1652 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_1653 = torch.constant.int 2
    %1414 = torch.aten.unsqueeze %1413, %int2_1653 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_1654 = torch.constant.int 3
    %1415 = torch.aten.unsqueeze %1414, %int3_1654 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.norm.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.norm.weight : tensor<1280xf16>
    %1416 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_1655 = torch.constant.int 0
    %1417 = torch.aten.unsqueeze %1416, %int0_1655 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_1656 = torch.constant.int 2
    %1418 = torch.aten.unsqueeze %1417, %int2_1656 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_1657 = torch.constant.int 3
    %1419 = torch.aten.unsqueeze %1418, %int3_1657 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %1420 = torch.aten.mul.Tensor %1411, %1419 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_1658 = torch.constant.int 1
    %1421 = torch.aten.add.Tensor %1420, %1415, %int1_1658 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_1659 = torch.constant.int 5
    %1422 = torch.prims.convert_element_type %1421, %int5_1659 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int0_1660 = torch.constant.int 0
    %int2_1661 = torch.constant.int 2
    %int3_1662 = torch.constant.int 3
    %int1_1663 = torch.constant.int 1
    %1423 = torch.prim.ListConstruct %int0_1660, %int2_1661, %int3_1662, %int1_1663 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1424 = torch.aten.permute %1422, %1423 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,30,32,1280],f16>
    %int2_1664 = torch.constant.int 2
    %int960_1665 = torch.constant.int 960
    %int1280_1666 = torch.constant.int 1280
    %1425 = torch.prim.ListConstruct %int2_1664, %int960_1665, %int1280_1666 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1426 = torch.aten.view %1424, %1425 : !torch.vtensor<[2,30,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.proj_in.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %1427 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1667 = torch.constant.int 0
    %int1_1668 = torch.constant.int 1
    %1428 = torch.aten.transpose.int %1427, %int0_1667, %int1_1668 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int0_1669 = torch.constant.int 0
    %1429 = torch.aten.clone %1426, %int0_1669 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920 = torch.constant.int 1920
    %int1280_1670 = torch.constant.int 1280
    %1430 = torch.prim.ListConstruct %int1920, %int1280_1670 : (!torch.int, !torch.int) -> !torch.list<int>
    %1431 = torch.aten._unsafe_view %1429, %1430 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1432 = torch.aten.mm %1431, %1428 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1671 = torch.constant.int 2
    %int960_1672 = torch.constant.int 960
    %int1280_1673 = torch.constant.int 1280
    %1433 = torch.prim.ListConstruct %int2_1671, %int960_1672, %int1280_1673 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1434 = torch.aten.view %1432, %1433 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.proj_in.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.proj_in.bias : tensor<1280xf16>
    %1435 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1674 = torch.constant.int 1
    %1436 = torch.aten.add.Tensor %1434, %1435, %int1_1674 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_1675 = torch.constant.int 6
    %1437 = torch.prims.convert_element_type %1436, %int6_1675 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_1676 = torch.constant.int 2
    %1438 = torch.prim.ListConstruct %int2_1676 : (!torch.int) -> !torch.list<int>
    %int0_1677 = torch.constant.int 0
    %true_1678 = torch.constant.bool true
    %result0_1679, %result1_1680 = torch.aten.var_mean.correction %1437, %1438, %int0_1677, %true_1678 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_1681 = torch.constant.float 1.000000e-05
    %int1_1682 = torch.constant.int 1
    %1439 = torch.aten.add.Scalar %result0_1679, %float1.000000e-05_1681, %int1_1682 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1440 = torch.aten.rsqrt %1439 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_1683 = torch.constant.int 1
    %1441 = torch.aten.sub.Tensor %1436, %result1_1680, %int1_1683 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1442 = torch.aten.mul.Tensor %1441, %1440 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %1443 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1444 = torch.aten.mul.Tensor %1442, %1443 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %1445 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1684 = torch.constant.int 1
    %1446 = torch.aten.add.Tensor %1444, %1445, %int1_1684 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_1685 = torch.constant.int 5
    %1447 = torch.prims.convert_element_type %1446, %int5_1685 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %1448 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1686 = torch.constant.int 0
    %int1_1687 = torch.constant.int 1
    %1449 = torch.aten.transpose.int %1448, %int0_1686, %int1_1687 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1688 = torch.constant.int 1920
    %int1280_1689 = torch.constant.int 1280
    %1450 = torch.prim.ListConstruct %int1920_1688, %int1280_1689 : (!torch.int, !torch.int) -> !torch.list<int>
    %1451 = torch.aten.view %1447, %1450 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1452 = torch.aten.mm %1451, %1449 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1690 = torch.constant.int 2
    %int960_1691 = torch.constant.int 960
    %int1280_1692 = torch.constant.int 1280
    %1453 = torch.prim.ListConstruct %int2_1690, %int960_1691, %int1280_1692 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1454 = torch.aten.view %1452, %1453 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %1455 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1693 = torch.constant.int 0
    %int1_1694 = torch.constant.int 1
    %1456 = torch.aten.transpose.int %1455, %int0_1693, %int1_1694 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1695 = torch.constant.int 1920
    %int1280_1696 = torch.constant.int 1280
    %1457 = torch.prim.ListConstruct %int1920_1695, %int1280_1696 : (!torch.int, !torch.int) -> !torch.list<int>
    %1458 = torch.aten.view %1447, %1457 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1459 = torch.aten.mm %1458, %1456 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1697 = torch.constant.int 2
    %int960_1698 = torch.constant.int 960
    %int1280_1699 = torch.constant.int 1280
    %1460 = torch.prim.ListConstruct %int2_1697, %int960_1698, %int1280_1699 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1461 = torch.aten.view %1459, %1460 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %1462 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1700 = torch.constant.int 0
    %int1_1701 = torch.constant.int 1
    %1463 = torch.aten.transpose.int %1462, %int0_1700, %int1_1701 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1702 = torch.constant.int 1920
    %int1280_1703 = torch.constant.int 1280
    %1464 = torch.prim.ListConstruct %int1920_1702, %int1280_1703 : (!torch.int, !torch.int) -> !torch.list<int>
    %1465 = torch.aten.view %1447, %1464 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1466 = torch.aten.mm %1465, %1463 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1704 = torch.constant.int 2
    %int960_1705 = torch.constant.int 960
    %int1280_1706 = torch.constant.int 1280
    %1467 = torch.prim.ListConstruct %int2_1704, %int960_1705, %int1280_1706 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1468 = torch.aten.view %1466, %1467 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_1707 = torch.constant.int 2
    %int-1_1708 = torch.constant.int -1
    %int20_1709 = torch.constant.int 20
    %int64_1710 = torch.constant.int 64
    %1469 = torch.prim.ListConstruct %int2_1707, %int-1_1708, %int20_1709, %int64_1710 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1470 = torch.aten.view %1454, %1469 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1711 = torch.constant.int 1
    %int2_1712 = torch.constant.int 2
    %1471 = torch.aten.transpose.int %1470, %int1_1711, %int2_1712 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_1713 = torch.constant.int 2
    %int-1_1714 = torch.constant.int -1
    %int20_1715 = torch.constant.int 20
    %int64_1716 = torch.constant.int 64
    %1472 = torch.prim.ListConstruct %int2_1713, %int-1_1714, %int20_1715, %int64_1716 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1473 = torch.aten.view %1461, %1472 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1717 = torch.constant.int 1
    %int2_1718 = torch.constant.int 2
    %1474 = torch.aten.transpose.int %1473, %int1_1717, %int2_1718 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_1719 = torch.constant.int 2
    %int-1_1720 = torch.constant.int -1
    %int20_1721 = torch.constant.int 20
    %int64_1722 = torch.constant.int 64
    %1475 = torch.prim.ListConstruct %int2_1719, %int-1_1720, %int20_1721, %int64_1722 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1476 = torch.aten.view %1468, %1475 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1723 = torch.constant.int 1
    %int2_1724 = torch.constant.int 2
    %1477 = torch.aten.transpose.int %1476, %int1_1723, %int2_1724 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_1725 = torch.constant.float 0.000000e+00
    %false_1726 = torch.constant.bool false
    %none_1727 = torch.constant.none
    %none_1728 = torch.constant.none
    %1478:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1471, %1474, %1477, %float0.000000e00_1725, %false_1726, %none_1727, %none_1728) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_1729 = torch.constant.int 1
    %int2_1730 = torch.constant.int 2
    %1479 = torch.aten.transpose.int %1478#0, %int1_1729, %int2_1730 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_1731 = torch.constant.int 2
    %int-1_1732 = torch.constant.int -1
    %int1280_1733 = torch.constant.int 1280
    %1480 = torch.prim.ListConstruct %int2_1731, %int-1_1732, %int1280_1733 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1481 = torch.aten.view %1479, %1480 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_1734 = torch.constant.int 5
    %1482 = torch.prims.convert_element_type %1481, %int5_1734 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_1735 = torch.constant.int 1920
    %int1280_1736 = torch.constant.int 1280
    %1483 = torch.prim.ListConstruct %int1920_1735, %int1280_1736 : (!torch.int, !torch.int) -> !torch.list<int>
    %1484 = torch.aten.view %1482, %1483 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1485 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1737 = torch.constant.int 0
    %int1_1738 = torch.constant.int 1
    %1486 = torch.aten.transpose.int %1485, %int0_1737, %int1_1738 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %1487 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1739 = torch.constant.int 6
    %1488 = torch.prims.convert_element_type %1487, %int6_1739 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1740 = torch.constant.int 6
    %1489 = torch.prims.convert_element_type %1484, %int6_1740 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_1741 = torch.constant.int 6
    %1490 = torch.prims.convert_element_type %1486, %int6_1741 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1491 = torch.aten.mm %1489, %1490 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_1742 = torch.constant.int 1
    %1492 = torch.aten.mul.Scalar %1491, %int1_1742 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_1743 = torch.constant.int 1
    %1493 = torch.aten.mul.Scalar %1488, %int1_1743 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1744 = torch.constant.int 1
    %1494 = torch.aten.add.Tensor %1492, %1493, %int1_1744 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_1745 = torch.constant.int 5
    %1495 = torch.prims.convert_element_type %1494, %int5_1745 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_1746 = torch.constant.int 2
    %int960_1747 = torch.constant.int 960
    %int1280_1748 = torch.constant.int 1280
    %1496 = torch.prim.ListConstruct %int2_1746, %int960_1747, %int1280_1748 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1497 = torch.aten.view %1495, %1496 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_1749 = torch.constant.none
    %1498 = torch.aten.clone %1497, %none_1749 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_1750 = torch.constant.float 1.000000e+00
    %1499 = torch.aten.div.Scalar %1498, %float1.000000e00_1750 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_1751 = torch.constant.int 1
    %1500 = torch.aten.add.Tensor %1499, %1436, %int1_1751 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_1752 = torch.constant.int 6
    %1501 = torch.prims.convert_element_type %1500, %int6_1752 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_1753 = torch.constant.int 2
    %1502 = torch.prim.ListConstruct %int2_1753 : (!torch.int) -> !torch.list<int>
    %int0_1754 = torch.constant.int 0
    %true_1755 = torch.constant.bool true
    %result0_1756, %result1_1757 = torch.aten.var_mean.correction %1501, %1502, %int0_1754, %true_1755 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_1758 = torch.constant.float 1.000000e-05
    %int1_1759 = torch.constant.int 1
    %1503 = torch.aten.add.Scalar %result0_1756, %float1.000000e-05_1758, %int1_1759 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1504 = torch.aten.rsqrt %1503 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_1760 = torch.constant.int 1
    %1505 = torch.aten.sub.Tensor %1500, %result1_1757, %int1_1760 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1506 = torch.aten.mul.Tensor %1505, %1504 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %1507 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1508 = torch.aten.mul.Tensor %1506, %1507 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %1509 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1761 = torch.constant.int 1
    %1510 = torch.aten.add.Tensor %1508, %1509, %int1_1761 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_1762 = torch.constant.int 5
    %1511 = torch.prims.convert_element_type %1510, %int5_1762 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %1512 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1763 = torch.constant.int 0
    %int1_1764 = torch.constant.int 1
    %1513 = torch.aten.transpose.int %1512, %int0_1763, %int1_1764 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1765 = torch.constant.int 1920
    %int1280_1766 = torch.constant.int 1280
    %1514 = torch.prim.ListConstruct %int1920_1765, %int1280_1766 : (!torch.int, !torch.int) -> !torch.list<int>
    %1515 = torch.aten.view %1511, %1514 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1516 = torch.aten.mm %1515, %1513 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1767 = torch.constant.int 2
    %int960_1768 = torch.constant.int 960
    %int1280_1769 = torch.constant.int 1280
    %1517 = torch.prim.ListConstruct %int2_1767, %int960_1768, %int1280_1769 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1518 = torch.aten.view %1516, %1517 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %1519 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_1770 = torch.constant.int 0
    %int1_1771 = torch.constant.int 1
    %1520 = torch.aten.transpose.int %1519, %int0_1770, %int1_1771 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_1772 = torch.constant.int 32
    %int2048_1773 = torch.constant.int 2048
    %1521 = torch.prim.ListConstruct %int32_1772, %int2048_1773 : (!torch.int, !torch.int) -> !torch.list<int>
    %1522 = torch.aten.view %arg6, %1521 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1523 = torch.aten.mm %1522, %1520 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_1774 = torch.constant.int 2
    %int16_1775 = torch.constant.int 16
    %int1280_1776 = torch.constant.int 1280
    %1524 = torch.prim.ListConstruct %int2_1774, %int16_1775, %int1280_1776 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1525 = torch.aten.view %1523, %1524 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %1526 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_1777 = torch.constant.int 0
    %int1_1778 = torch.constant.int 1
    %1527 = torch.aten.transpose.int %1526, %int0_1777, %int1_1778 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_1779 = torch.constant.int 32
    %int2048_1780 = torch.constant.int 2048
    %1528 = torch.prim.ListConstruct %int32_1779, %int2048_1780 : (!torch.int, !torch.int) -> !torch.list<int>
    %1529 = torch.aten.view %arg6, %1528 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1530 = torch.aten.mm %1529, %1527 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_1781 = torch.constant.int 2
    %int16_1782 = torch.constant.int 16
    %int1280_1783 = torch.constant.int 1280
    %1531 = torch.prim.ListConstruct %int2_1781, %int16_1782, %int1280_1783 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1532 = torch.aten.view %1530, %1531 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_1784 = torch.constant.int 2
    %int-1_1785 = torch.constant.int -1
    %int20_1786 = torch.constant.int 20
    %int64_1787 = torch.constant.int 64
    %1533 = torch.prim.ListConstruct %int2_1784, %int-1_1785, %int20_1786, %int64_1787 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1534 = torch.aten.view %1518, %1533 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1788 = torch.constant.int 1
    %int2_1789 = torch.constant.int 2
    %1535 = torch.aten.transpose.int %1534, %int1_1788, %int2_1789 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_1790 = torch.constant.int 2
    %int-1_1791 = torch.constant.int -1
    %int20_1792 = torch.constant.int 20
    %int64_1793 = torch.constant.int 64
    %1536 = torch.prim.ListConstruct %int2_1790, %int-1_1791, %int20_1792, %int64_1793 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1537 = torch.aten.view %1525, %1536 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_1794 = torch.constant.int 1
    %int2_1795 = torch.constant.int 2
    %1538 = torch.aten.transpose.int %1537, %int1_1794, %int2_1795 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_1796 = torch.constant.int 2
    %int-1_1797 = torch.constant.int -1
    %int20_1798 = torch.constant.int 20
    %int64_1799 = torch.constant.int 64
    %1539 = torch.prim.ListConstruct %int2_1796, %int-1_1797, %int20_1798, %int64_1799 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1540 = torch.aten.view %1532, %1539 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_1800 = torch.constant.int 1
    %int2_1801 = torch.constant.int 2
    %1541 = torch.aten.transpose.int %1540, %int1_1800, %int2_1801 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_1802 = torch.constant.float 0.000000e+00
    %false_1803 = torch.constant.bool false
    %none_1804 = torch.constant.none
    %none_1805 = torch.constant.none
    %1542:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1535, %1538, %1541, %float0.000000e00_1802, %false_1803, %none_1804, %none_1805) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_1806 = torch.constant.int 1
    %int2_1807 = torch.constant.int 2
    %1543 = torch.aten.transpose.int %1542#0, %int1_1806, %int2_1807 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_1808 = torch.constant.int 2
    %int-1_1809 = torch.constant.int -1
    %int1280_1810 = torch.constant.int 1280
    %1544 = torch.prim.ListConstruct %int2_1808, %int-1_1809, %int1280_1810 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1545 = torch.aten.view %1543, %1544 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_1811 = torch.constant.int 5
    %1546 = torch.prims.convert_element_type %1545, %int5_1811 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_1812 = torch.constant.int 1920
    %int1280_1813 = torch.constant.int 1280
    %1547 = torch.prim.ListConstruct %int1920_1812, %int1280_1813 : (!torch.int, !torch.int) -> !torch.list<int>
    %1548 = torch.aten.view %1546, %1547 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1549 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1814 = torch.constant.int 0
    %int1_1815 = torch.constant.int 1
    %1550 = torch.aten.transpose.int %1549, %int0_1814, %int1_1815 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %1551 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1816 = torch.constant.int 6
    %1552 = torch.prims.convert_element_type %1551, %int6_1816 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1817 = torch.constant.int 6
    %1553 = torch.prims.convert_element_type %1548, %int6_1817 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_1818 = torch.constant.int 6
    %1554 = torch.prims.convert_element_type %1550, %int6_1818 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1555 = torch.aten.mm %1553, %1554 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_1819 = torch.constant.int 1
    %1556 = torch.aten.mul.Scalar %1555, %int1_1819 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_1820 = torch.constant.int 1
    %1557 = torch.aten.mul.Scalar %1552, %int1_1820 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1821 = torch.constant.int 1
    %1558 = torch.aten.add.Tensor %1556, %1557, %int1_1821 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_1822 = torch.constant.int 5
    %1559 = torch.prims.convert_element_type %1558, %int5_1822 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_1823 = torch.constant.int 2
    %int960_1824 = torch.constant.int 960
    %int1280_1825 = torch.constant.int 1280
    %1560 = torch.prim.ListConstruct %int2_1823, %int960_1824, %int1280_1825 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1561 = torch.aten.view %1559, %1560 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_1826 = torch.constant.none
    %1562 = torch.aten.clone %1561, %none_1826 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_1827 = torch.constant.float 1.000000e+00
    %1563 = torch.aten.div.Scalar %1562, %float1.000000e00_1827 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_1828 = torch.constant.int 1
    %1564 = torch.aten.add.Tensor %1563, %1500, %int1_1828 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_1829 = torch.constant.int 6
    %1565 = torch.prims.convert_element_type %1564, %int6_1829 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_1830 = torch.constant.int 2
    %1566 = torch.prim.ListConstruct %int2_1830 : (!torch.int) -> !torch.list<int>
    %int0_1831 = torch.constant.int 0
    %true_1832 = torch.constant.bool true
    %result0_1833, %result1_1834 = torch.aten.var_mean.correction %1565, %1566, %int0_1831, %true_1832 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_1835 = torch.constant.float 1.000000e-05
    %int1_1836 = torch.constant.int 1
    %1567 = torch.aten.add.Scalar %result0_1833, %float1.000000e-05_1835, %int1_1836 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1568 = torch.aten.rsqrt %1567 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_1837 = torch.constant.int 1
    %1569 = torch.aten.sub.Tensor %1564, %result1_1834, %int1_1837 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1570 = torch.aten.mul.Tensor %1569, %1568 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %1571 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1572 = torch.aten.mul.Tensor %1570, %1571 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %1573 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1838 = torch.constant.int 1
    %1574 = torch.aten.add.Tensor %1572, %1573, %int1_1838 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_1839 = torch.constant.int 5
    %1575 = torch.prims.convert_element_type %1574, %int5_1839 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_1840 = torch.constant.int 1920
    %int1280_1841 = torch.constant.int 1280
    %1576 = torch.prim.ListConstruct %int1920_1840, %int1280_1841 : (!torch.int, !torch.int) -> !torch.list<int>
    %1577 = torch.aten.view %1575, %1576 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1578 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_1842 = torch.constant.int 0
    %int1_1843 = torch.constant.int 1
    %1579 = torch.aten.transpose.int %1578, %int0_1842, %int1_1843 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %1580 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_1844 = torch.constant.int 6
    %1581 = torch.prims.convert_element_type %1580, %int6_1844 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_1845 = torch.constant.int 6
    %1582 = torch.prims.convert_element_type %1577, %int6_1845 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_1846 = torch.constant.int 6
    %1583 = torch.prims.convert_element_type %1579, %int6_1846 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1584 = torch.aten.mm %1582, %1583 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_1847 = torch.constant.int 1
    %1585 = torch.aten.mul.Scalar %1584, %int1_1847 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_1848 = torch.constant.int 1
    %1586 = torch.aten.mul.Scalar %1581, %int1_1848 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_1849 = torch.constant.int 1
    %1587 = torch.aten.add.Tensor %1585, %1586, %int1_1849 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_1850 = torch.constant.int 5
    %1588 = torch.prims.convert_element_type %1587, %int5_1850 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_1851 = torch.constant.int 2
    %int960_1852 = torch.constant.int 960
    %int10240 = torch.constant.int 10240
    %1589 = torch.prim.ListConstruct %int2_1851, %int960_1852, %int10240 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1590 = torch.aten.view %1588, %1589 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_1853 = torch.constant.int -1
    %int0_1854 = torch.constant.int 0
    %int5120_1855 = torch.constant.int 5120
    %int1_1856 = torch.constant.int 1
    %1591 = torch.aten.slice.Tensor %1590, %int-1_1853, %int0_1854, %int5120_1855, %int1_1856 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_1857 = torch.constant.int -1
    %int5120_1858 = torch.constant.int 5120
    %int10240_1859 = torch.constant.int 10240
    %int1_1860 = torch.constant.int 1
    %1592 = torch.aten.slice.Tensor %1590, %int-1_1857, %int5120_1858, %int10240_1859, %int1_1860 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_1861 = torch.constant.str "none"
    %1593 = torch.aten.gelu %1592, %str_1861 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %1594 = torch.aten.mul.Tensor %1591, %1593 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_1862 = torch.constant.none
    %1595 = torch.aten.clone %1594, %none_1862 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_1863 = torch.constant.int 1920
    %int5120_1864 = torch.constant.int 5120
    %1596 = torch.prim.ListConstruct %int1920_1863, %int5120_1864 : (!torch.int, !torch.int) -> !torch.list<int>
    %1597 = torch.aten.view %1595, %1596 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %1598 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_1865 = torch.constant.int 0
    %int1_1866 = torch.constant.int 1
    %1599 = torch.aten.transpose.int %1598, %int0_1865, %int1_1866 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %1600 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1867 = torch.constant.int 6
    %1601 = torch.prims.convert_element_type %1600, %int6_1867 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1868 = torch.constant.int 6
    %1602 = torch.prims.convert_element_type %1597, %int6_1868 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_1869 = torch.constant.int 6
    %1603 = torch.prims.convert_element_type %1599, %int6_1869 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1604 = torch.aten.mm %1602, %1603 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_1870 = torch.constant.int 1
    %1605 = torch.aten.mul.Scalar %1604, %int1_1870 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_1871 = torch.constant.int 1
    %1606 = torch.aten.mul.Scalar %1601, %int1_1871 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1872 = torch.constant.int 1
    %1607 = torch.aten.add.Tensor %1605, %1606, %int1_1872 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_1873 = torch.constant.int 5
    %1608 = torch.prims.convert_element_type %1607, %int5_1873 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_1874 = torch.constant.int 2
    %int960_1875 = torch.constant.int 960
    %int1280_1876 = torch.constant.int 1280
    %1609 = torch.prim.ListConstruct %int2_1874, %int960_1875, %int1280_1876 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1610 = torch.aten.view %1608, %1609 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_1877 = torch.constant.int 1
    %1611 = torch.aten.add.Tensor %1610, %1564, %int1_1877 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_1878 = torch.constant.int 6
    %1612 = torch.prims.convert_element_type %1611, %int6_1878 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_1879 = torch.constant.int 2
    %1613 = torch.prim.ListConstruct %int2_1879 : (!torch.int) -> !torch.list<int>
    %int0_1880 = torch.constant.int 0
    %true_1881 = torch.constant.bool true
    %result0_1882, %result1_1883 = torch.aten.var_mean.correction %1612, %1613, %int0_1880, %true_1881 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_1884 = torch.constant.float 1.000000e-05
    %int1_1885 = torch.constant.int 1
    %1614 = torch.aten.add.Scalar %result0_1882, %float1.000000e-05_1884, %int1_1885 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1615 = torch.aten.rsqrt %1614 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_1886 = torch.constant.int 1
    %1616 = torch.aten.sub.Tensor %1611, %result1_1883, %int1_1886 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1617 = torch.aten.mul.Tensor %1616, %1615 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %1618 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1619 = torch.aten.mul.Tensor %1617, %1618 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %1620 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1887 = torch.constant.int 1
    %1621 = torch.aten.add.Tensor %1619, %1620, %int1_1887 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_1888 = torch.constant.int 5
    %1622 = torch.prims.convert_element_type %1621, %int5_1888 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %1623 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1889 = torch.constant.int 0
    %int1_1890 = torch.constant.int 1
    %1624 = torch.aten.transpose.int %1623, %int0_1889, %int1_1890 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1891 = torch.constant.int 1920
    %int1280_1892 = torch.constant.int 1280
    %1625 = torch.prim.ListConstruct %int1920_1891, %int1280_1892 : (!torch.int, !torch.int) -> !torch.list<int>
    %1626 = torch.aten.view %1622, %1625 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1627 = torch.aten.mm %1626, %1624 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1893 = torch.constant.int 2
    %int960_1894 = torch.constant.int 960
    %int1280_1895 = torch.constant.int 1280
    %1628 = torch.prim.ListConstruct %int2_1893, %int960_1894, %int1280_1895 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1629 = torch.aten.view %1627, %1628 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %1630 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1896 = torch.constant.int 0
    %int1_1897 = torch.constant.int 1
    %1631 = torch.aten.transpose.int %1630, %int0_1896, %int1_1897 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1898 = torch.constant.int 1920
    %int1280_1899 = torch.constant.int 1280
    %1632 = torch.prim.ListConstruct %int1920_1898, %int1280_1899 : (!torch.int, !torch.int) -> !torch.list<int>
    %1633 = torch.aten.view %1622, %1632 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1634 = torch.aten.mm %1633, %1631 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1900 = torch.constant.int 2
    %int960_1901 = torch.constant.int 960
    %int1280_1902 = torch.constant.int 1280
    %1635 = torch.prim.ListConstruct %int2_1900, %int960_1901, %int1280_1902 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1636 = torch.aten.view %1634, %1635 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %1637 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1903 = torch.constant.int 0
    %int1_1904 = torch.constant.int 1
    %1638 = torch.aten.transpose.int %1637, %int0_1903, %int1_1904 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1905 = torch.constant.int 1920
    %int1280_1906 = torch.constant.int 1280
    %1639 = torch.prim.ListConstruct %int1920_1905, %int1280_1906 : (!torch.int, !torch.int) -> !torch.list<int>
    %1640 = torch.aten.view %1622, %1639 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1641 = torch.aten.mm %1640, %1638 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1907 = torch.constant.int 2
    %int960_1908 = torch.constant.int 960
    %int1280_1909 = torch.constant.int 1280
    %1642 = torch.prim.ListConstruct %int2_1907, %int960_1908, %int1280_1909 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1643 = torch.aten.view %1641, %1642 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_1910 = torch.constant.int 2
    %int-1_1911 = torch.constant.int -1
    %int20_1912 = torch.constant.int 20
    %int64_1913 = torch.constant.int 64
    %1644 = torch.prim.ListConstruct %int2_1910, %int-1_1911, %int20_1912, %int64_1913 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1645 = torch.aten.view %1629, %1644 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1914 = torch.constant.int 1
    %int2_1915 = torch.constant.int 2
    %1646 = torch.aten.transpose.int %1645, %int1_1914, %int2_1915 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_1916 = torch.constant.int 2
    %int-1_1917 = torch.constant.int -1
    %int20_1918 = torch.constant.int 20
    %int64_1919 = torch.constant.int 64
    %1647 = torch.prim.ListConstruct %int2_1916, %int-1_1917, %int20_1918, %int64_1919 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1648 = torch.aten.view %1636, %1647 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1920 = torch.constant.int 1
    %int2_1921 = torch.constant.int 2
    %1649 = torch.aten.transpose.int %1648, %int1_1920, %int2_1921 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_1922 = torch.constant.int 2
    %int-1_1923 = torch.constant.int -1
    %int20_1924 = torch.constant.int 20
    %int64_1925 = torch.constant.int 64
    %1650 = torch.prim.ListConstruct %int2_1922, %int-1_1923, %int20_1924, %int64_1925 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1651 = torch.aten.view %1643, %1650 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1926 = torch.constant.int 1
    %int2_1927 = torch.constant.int 2
    %1652 = torch.aten.transpose.int %1651, %int1_1926, %int2_1927 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_1928 = torch.constant.float 0.000000e+00
    %false_1929 = torch.constant.bool false
    %none_1930 = torch.constant.none
    %none_1931 = torch.constant.none
    %1653:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1646, %1649, %1652, %float0.000000e00_1928, %false_1929, %none_1930, %none_1931) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_1932 = torch.constant.int 1
    %int2_1933 = torch.constant.int 2
    %1654 = torch.aten.transpose.int %1653#0, %int1_1932, %int2_1933 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_1934 = torch.constant.int 2
    %int-1_1935 = torch.constant.int -1
    %int1280_1936 = torch.constant.int 1280
    %1655 = torch.prim.ListConstruct %int2_1934, %int-1_1935, %int1280_1936 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1656 = torch.aten.view %1654, %1655 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_1937 = torch.constant.int 5
    %1657 = torch.prims.convert_element_type %1656, %int5_1937 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_1938 = torch.constant.int 1920
    %int1280_1939 = torch.constant.int 1280
    %1658 = torch.prim.ListConstruct %int1920_1938, %int1280_1939 : (!torch.int, !torch.int) -> !torch.list<int>
    %1659 = torch.aten.view %1657, %1658 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1660 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1940 = torch.constant.int 0
    %int1_1941 = torch.constant.int 1
    %1661 = torch.aten.transpose.int %1660, %int0_1940, %int1_1941 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %1662 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_1942 = torch.constant.int 6
    %1663 = torch.prims.convert_element_type %1662, %int6_1942 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_1943 = torch.constant.int 6
    %1664 = torch.prims.convert_element_type %1659, %int6_1943 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_1944 = torch.constant.int 6
    %1665 = torch.prims.convert_element_type %1661, %int6_1944 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1666 = torch.aten.mm %1664, %1665 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_1945 = torch.constant.int 1
    %1667 = torch.aten.mul.Scalar %1666, %int1_1945 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_1946 = torch.constant.int 1
    %1668 = torch.aten.mul.Scalar %1663, %int1_1946 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_1947 = torch.constant.int 1
    %1669 = torch.aten.add.Tensor %1667, %1668, %int1_1947 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_1948 = torch.constant.int 5
    %1670 = torch.prims.convert_element_type %1669, %int5_1948 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_1949 = torch.constant.int 2
    %int960_1950 = torch.constant.int 960
    %int1280_1951 = torch.constant.int 1280
    %1671 = torch.prim.ListConstruct %int2_1949, %int960_1950, %int1280_1951 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1672 = torch.aten.view %1670, %1671 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_1952 = torch.constant.none
    %1673 = torch.aten.clone %1672, %none_1952 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_1953 = torch.constant.float 1.000000e+00
    %1674 = torch.aten.div.Scalar %1673, %float1.000000e00_1953 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_1954 = torch.constant.int 1
    %1675 = torch.aten.add.Tensor %1674, %1611, %int1_1954 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_1955 = torch.constant.int 6
    %1676 = torch.prims.convert_element_type %1675, %int6_1955 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_1956 = torch.constant.int 2
    %1677 = torch.prim.ListConstruct %int2_1956 : (!torch.int) -> !torch.list<int>
    %int0_1957 = torch.constant.int 0
    %true_1958 = torch.constant.bool true
    %result0_1959, %result1_1960 = torch.aten.var_mean.correction %1676, %1677, %int0_1957, %true_1958 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_1961 = torch.constant.float 1.000000e-05
    %int1_1962 = torch.constant.int 1
    %1678 = torch.aten.add.Scalar %result0_1959, %float1.000000e-05_1961, %int1_1962 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1679 = torch.aten.rsqrt %1678 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_1963 = torch.constant.int 1
    %1680 = torch.aten.sub.Tensor %1675, %result1_1960, %int1_1963 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1681 = torch.aten.mul.Tensor %1680, %1679 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %1682 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1683 = torch.aten.mul.Tensor %1681, %1682 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %1684 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_1964 = torch.constant.int 1
    %1685 = torch.aten.add.Tensor %1683, %1684, %int1_1964 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_1965 = torch.constant.int 5
    %1686 = torch.prims.convert_element_type %1685, %int5_1965 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %1687 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_1966 = torch.constant.int 0
    %int1_1967 = torch.constant.int 1
    %1688 = torch.aten.transpose.int %1687, %int0_1966, %int1_1967 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_1968 = torch.constant.int 1920
    %int1280_1969 = torch.constant.int 1280
    %1689 = torch.prim.ListConstruct %int1920_1968, %int1280_1969 : (!torch.int, !torch.int) -> !torch.list<int>
    %1690 = torch.aten.view %1686, %1689 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1691 = torch.aten.mm %1690, %1688 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_1970 = torch.constant.int 2
    %int960_1971 = torch.constant.int 960
    %int1280_1972 = torch.constant.int 1280
    %1692 = torch.prim.ListConstruct %int2_1970, %int960_1971, %int1280_1972 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1693 = torch.aten.view %1691, %1692 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %1694 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_1973 = torch.constant.int 0
    %int1_1974 = torch.constant.int 1
    %1695 = torch.aten.transpose.int %1694, %int0_1973, %int1_1974 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_1975 = torch.constant.int 32
    %int2048_1976 = torch.constant.int 2048
    %1696 = torch.prim.ListConstruct %int32_1975, %int2048_1976 : (!torch.int, !torch.int) -> !torch.list<int>
    %1697 = torch.aten.view %arg6, %1696 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1698 = torch.aten.mm %1697, %1695 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_1977 = torch.constant.int 2
    %int16_1978 = torch.constant.int 16
    %int1280_1979 = torch.constant.int 1280
    %1699 = torch.prim.ListConstruct %int2_1977, %int16_1978, %int1280_1979 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1700 = torch.aten.view %1698, %1699 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %1701 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_1980 = torch.constant.int 0
    %int1_1981 = torch.constant.int 1
    %1702 = torch.aten.transpose.int %1701, %int0_1980, %int1_1981 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_1982 = torch.constant.int 32
    %int2048_1983 = torch.constant.int 2048
    %1703 = torch.prim.ListConstruct %int32_1982, %int2048_1983 : (!torch.int, !torch.int) -> !torch.list<int>
    %1704 = torch.aten.view %arg6, %1703 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1705 = torch.aten.mm %1704, %1702 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_1984 = torch.constant.int 2
    %int16_1985 = torch.constant.int 16
    %int1280_1986 = torch.constant.int 1280
    %1706 = torch.prim.ListConstruct %int2_1984, %int16_1985, %int1280_1986 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1707 = torch.aten.view %1705, %1706 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_1987 = torch.constant.int 2
    %int-1_1988 = torch.constant.int -1
    %int20_1989 = torch.constant.int 20
    %int64_1990 = torch.constant.int 64
    %1708 = torch.prim.ListConstruct %int2_1987, %int-1_1988, %int20_1989, %int64_1990 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1709 = torch.aten.view %1693, %1708 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_1991 = torch.constant.int 1
    %int2_1992 = torch.constant.int 2
    %1710 = torch.aten.transpose.int %1709, %int1_1991, %int2_1992 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_1993 = torch.constant.int 2
    %int-1_1994 = torch.constant.int -1
    %int20_1995 = torch.constant.int 20
    %int64_1996 = torch.constant.int 64
    %1711 = torch.prim.ListConstruct %int2_1993, %int-1_1994, %int20_1995, %int64_1996 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1712 = torch.aten.view %1700, %1711 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_1997 = torch.constant.int 1
    %int2_1998 = torch.constant.int 2
    %1713 = torch.aten.transpose.int %1712, %int1_1997, %int2_1998 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_1999 = torch.constant.int 2
    %int-1_2000 = torch.constant.int -1
    %int20_2001 = torch.constant.int 20
    %int64_2002 = torch.constant.int 64
    %1714 = torch.prim.ListConstruct %int2_1999, %int-1_2000, %int20_2001, %int64_2002 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1715 = torch.aten.view %1707, %1714 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2003 = torch.constant.int 1
    %int2_2004 = torch.constant.int 2
    %1716 = torch.aten.transpose.int %1715, %int1_2003, %int2_2004 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_2005 = torch.constant.float 0.000000e+00
    %false_2006 = torch.constant.bool false
    %none_2007 = torch.constant.none
    %none_2008 = torch.constant.none
    %1717:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1710, %1713, %1716, %float0.000000e00_2005, %false_2006, %none_2007, %none_2008) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2009 = torch.constant.int 1
    %int2_2010 = torch.constant.int 2
    %1718 = torch.aten.transpose.int %1717#0, %int1_2009, %int2_2010 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2011 = torch.constant.int 2
    %int-1_2012 = torch.constant.int -1
    %int1280_2013 = torch.constant.int 1280
    %1719 = torch.prim.ListConstruct %int2_2011, %int-1_2012, %int1280_2013 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1720 = torch.aten.view %1718, %1719 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2014 = torch.constant.int 5
    %1721 = torch.prims.convert_element_type %1720, %int5_2014 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2015 = torch.constant.int 1920
    %int1280_2016 = torch.constant.int 1280
    %1722 = torch.prim.ListConstruct %int1920_2015, %int1280_2016 : (!torch.int, !torch.int) -> !torch.list<int>
    %1723 = torch.aten.view %1721, %1722 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1724 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2017 = torch.constant.int 0
    %int1_2018 = torch.constant.int 1
    %1725 = torch.aten.transpose.int %1724, %int0_2017, %int1_2018 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %1726 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2019 = torch.constant.int 6
    %1727 = torch.prims.convert_element_type %1726, %int6_2019 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2020 = torch.constant.int 6
    %1728 = torch.prims.convert_element_type %1723, %int6_2020 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2021 = torch.constant.int 6
    %1729 = torch.prims.convert_element_type %1725, %int6_2021 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1730 = torch.aten.mm %1728, %1729 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2022 = torch.constant.int 1
    %1731 = torch.aten.mul.Scalar %1730, %int1_2022 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2023 = torch.constant.int 1
    %1732 = torch.aten.mul.Scalar %1727, %int1_2023 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2024 = torch.constant.int 1
    %1733 = torch.aten.add.Tensor %1731, %1732, %int1_2024 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2025 = torch.constant.int 5
    %1734 = torch.prims.convert_element_type %1733, %int5_2025 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2026 = torch.constant.int 2
    %int960_2027 = torch.constant.int 960
    %int1280_2028 = torch.constant.int 1280
    %1735 = torch.prim.ListConstruct %int2_2026, %int960_2027, %int1280_2028 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1736 = torch.aten.view %1734, %1735 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2029 = torch.constant.none
    %1737 = torch.aten.clone %1736, %none_2029 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2030 = torch.constant.float 1.000000e+00
    %1738 = torch.aten.div.Scalar %1737, %float1.000000e00_2030 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2031 = torch.constant.int 1
    %1739 = torch.aten.add.Tensor %1738, %1675, %int1_2031 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2032 = torch.constant.int 6
    %1740 = torch.prims.convert_element_type %1739, %int6_2032 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2033 = torch.constant.int 2
    %1741 = torch.prim.ListConstruct %int2_2033 : (!torch.int) -> !torch.list<int>
    %int0_2034 = torch.constant.int 0
    %true_2035 = torch.constant.bool true
    %result0_2036, %result1_2037 = torch.aten.var_mean.correction %1740, %1741, %int0_2034, %true_2035 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2038 = torch.constant.float 1.000000e-05
    %int1_2039 = torch.constant.int 1
    %1742 = torch.aten.add.Scalar %result0_2036, %float1.000000e-05_2038, %int1_2039 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1743 = torch.aten.rsqrt %1742 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2040 = torch.constant.int 1
    %1744 = torch.aten.sub.Tensor %1739, %result1_2037, %int1_2040 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1745 = torch.aten.mul.Tensor %1744, %1743 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %1746 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1747 = torch.aten.mul.Tensor %1745, %1746 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %1748 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2041 = torch.constant.int 1
    %1749 = torch.aten.add.Tensor %1747, %1748, %int1_2041 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2042 = torch.constant.int 5
    %1750 = torch.prims.convert_element_type %1749, %int5_2042 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2043 = torch.constant.int 1920
    %int1280_2044 = torch.constant.int 1280
    %1751 = torch.prim.ListConstruct %int1920_2043, %int1280_2044 : (!torch.int, !torch.int) -> !torch.list<int>
    %1752 = torch.aten.view %1750, %1751 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1753 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_2045 = torch.constant.int 0
    %int1_2046 = torch.constant.int 1
    %1754 = torch.aten.transpose.int %1753, %int0_2045, %int1_2046 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %1755 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_2047 = torch.constant.int 6
    %1756 = torch.prims.convert_element_type %1755, %int6_2047 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_2048 = torch.constant.int 6
    %1757 = torch.prims.convert_element_type %1752, %int6_2048 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2049 = torch.constant.int 6
    %1758 = torch.prims.convert_element_type %1754, %int6_2049 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1759 = torch.aten.mm %1757, %1758 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_2050 = torch.constant.int 1
    %1760 = torch.aten.mul.Scalar %1759, %int1_2050 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_2051 = torch.constant.int 1
    %1761 = torch.aten.mul.Scalar %1756, %int1_2051 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_2052 = torch.constant.int 1
    %1762 = torch.aten.add.Tensor %1760, %1761, %int1_2052 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_2053 = torch.constant.int 5
    %1763 = torch.prims.convert_element_type %1762, %int5_2053 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_2054 = torch.constant.int 2
    %int960_2055 = torch.constant.int 960
    %int10240_2056 = torch.constant.int 10240
    %1764 = torch.prim.ListConstruct %int2_2054, %int960_2055, %int10240_2056 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1765 = torch.aten.view %1763, %1764 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_2057 = torch.constant.int -1
    %int0_2058 = torch.constant.int 0
    %int5120_2059 = torch.constant.int 5120
    %int1_2060 = torch.constant.int 1
    %1766 = torch.aten.slice.Tensor %1765, %int-1_2057, %int0_2058, %int5120_2059, %int1_2060 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_2061 = torch.constant.int -1
    %int5120_2062 = torch.constant.int 5120
    %int10240_2063 = torch.constant.int 10240
    %int1_2064 = torch.constant.int 1
    %1767 = torch.aten.slice.Tensor %1765, %int-1_2061, %int5120_2062, %int10240_2063, %int1_2064 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_2065 = torch.constant.str "none"
    %1768 = torch.aten.gelu %1767, %str_2065 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %1769 = torch.aten.mul.Tensor %1766, %1768 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_2066 = torch.constant.none
    %1770 = torch.aten.clone %1769, %none_2066 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_2067 = torch.constant.int 1920
    %int5120_2068 = torch.constant.int 5120
    %1771 = torch.prim.ListConstruct %int1920_2067, %int5120_2068 : (!torch.int, !torch.int) -> !torch.list<int>
    %1772 = torch.aten.view %1770, %1771 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %1773 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2069 = torch.constant.int 0
    %int1_2070 = torch.constant.int 1
    %1774 = torch.aten.transpose.int %1773, %int0_2069, %int1_2070 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %1775 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2071 = torch.constant.int 6
    %1776 = torch.prims.convert_element_type %1775, %int6_2071 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2072 = torch.constant.int 6
    %1777 = torch.prims.convert_element_type %1772, %int6_2072 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_2073 = torch.constant.int 6
    %1778 = torch.prims.convert_element_type %1774, %int6_2073 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1779 = torch.aten.mm %1777, %1778 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2074 = torch.constant.int 1
    %1780 = torch.aten.mul.Scalar %1779, %int1_2074 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2075 = torch.constant.int 1
    %1781 = torch.aten.mul.Scalar %1776, %int1_2075 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2076 = torch.constant.int 1
    %1782 = torch.aten.add.Tensor %1780, %1781, %int1_2076 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2077 = torch.constant.int 5
    %1783 = torch.prims.convert_element_type %1782, %int5_2077 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2078 = torch.constant.int 2
    %int960_2079 = torch.constant.int 960
    %int1280_2080 = torch.constant.int 1280
    %1784 = torch.prim.ListConstruct %int2_2078, %int960_2079, %int1280_2080 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1785 = torch.aten.view %1783, %1784 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_2081 = torch.constant.int 1
    %1786 = torch.aten.add.Tensor %1785, %1739, %int1_2081 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2082 = torch.constant.int 6
    %1787 = torch.prims.convert_element_type %1786, %int6_2082 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2083 = torch.constant.int 2
    %1788 = torch.prim.ListConstruct %int2_2083 : (!torch.int) -> !torch.list<int>
    %int0_2084 = torch.constant.int 0
    %true_2085 = torch.constant.bool true
    %result0_2086, %result1_2087 = torch.aten.var_mean.correction %1787, %1788, %int0_2084, %true_2085 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2088 = torch.constant.float 1.000000e-05
    %int1_2089 = torch.constant.int 1
    %1789 = torch.aten.add.Scalar %result0_2086, %float1.000000e-05_2088, %int1_2089 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1790 = torch.aten.rsqrt %1789 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2090 = torch.constant.int 1
    %1791 = torch.aten.sub.Tensor %1786, %result1_2087, %int1_2090 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1792 = torch.aten.mul.Tensor %1791, %1790 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %1793 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1794 = torch.aten.mul.Tensor %1792, %1793 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %1795 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2091 = torch.constant.int 1
    %1796 = torch.aten.add.Tensor %1794, %1795, %int1_2091 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2092 = torch.constant.int 5
    %1797 = torch.prims.convert_element_type %1796, %int5_2092 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %1798 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2093 = torch.constant.int 0
    %int1_2094 = torch.constant.int 1
    %1799 = torch.aten.transpose.int %1798, %int0_2093, %int1_2094 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2095 = torch.constant.int 1920
    %int1280_2096 = torch.constant.int 1280
    %1800 = torch.prim.ListConstruct %int1920_2095, %int1280_2096 : (!torch.int, !torch.int) -> !torch.list<int>
    %1801 = torch.aten.view %1797, %1800 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1802 = torch.aten.mm %1801, %1799 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2097 = torch.constant.int 2
    %int960_2098 = torch.constant.int 960
    %int1280_2099 = torch.constant.int 1280
    %1803 = torch.prim.ListConstruct %int2_2097, %int960_2098, %int1280_2099 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1804 = torch.aten.view %1802, %1803 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %1805 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2100 = torch.constant.int 0
    %int1_2101 = torch.constant.int 1
    %1806 = torch.aten.transpose.int %1805, %int0_2100, %int1_2101 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2102 = torch.constant.int 1920
    %int1280_2103 = torch.constant.int 1280
    %1807 = torch.prim.ListConstruct %int1920_2102, %int1280_2103 : (!torch.int, !torch.int) -> !torch.list<int>
    %1808 = torch.aten.view %1797, %1807 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1809 = torch.aten.mm %1808, %1806 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2104 = torch.constant.int 2
    %int960_2105 = torch.constant.int 960
    %int1280_2106 = torch.constant.int 1280
    %1810 = torch.prim.ListConstruct %int2_2104, %int960_2105, %int1280_2106 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1811 = torch.aten.view %1809, %1810 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %1812 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2107 = torch.constant.int 0
    %int1_2108 = torch.constant.int 1
    %1813 = torch.aten.transpose.int %1812, %int0_2107, %int1_2108 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2109 = torch.constant.int 1920
    %int1280_2110 = torch.constant.int 1280
    %1814 = torch.prim.ListConstruct %int1920_2109, %int1280_2110 : (!torch.int, !torch.int) -> !torch.list<int>
    %1815 = torch.aten.view %1797, %1814 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1816 = torch.aten.mm %1815, %1813 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2111 = torch.constant.int 2
    %int960_2112 = torch.constant.int 960
    %int1280_2113 = torch.constant.int 1280
    %1817 = torch.prim.ListConstruct %int2_2111, %int960_2112, %int1280_2113 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1818 = torch.aten.view %1816, %1817 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_2114 = torch.constant.int 2
    %int-1_2115 = torch.constant.int -1
    %int20_2116 = torch.constant.int 20
    %int64_2117 = torch.constant.int 64
    %1819 = torch.prim.ListConstruct %int2_2114, %int-1_2115, %int20_2116, %int64_2117 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1820 = torch.aten.view %1804, %1819 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2118 = torch.constant.int 1
    %int2_2119 = torch.constant.int 2
    %1821 = torch.aten.transpose.int %1820, %int1_2118, %int2_2119 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2120 = torch.constant.int 2
    %int-1_2121 = torch.constant.int -1
    %int20_2122 = torch.constant.int 20
    %int64_2123 = torch.constant.int 64
    %1822 = torch.prim.ListConstruct %int2_2120, %int-1_2121, %int20_2122, %int64_2123 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1823 = torch.aten.view %1811, %1822 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2124 = torch.constant.int 1
    %int2_2125 = torch.constant.int 2
    %1824 = torch.aten.transpose.int %1823, %int1_2124, %int2_2125 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2126 = torch.constant.int 2
    %int-1_2127 = torch.constant.int -1
    %int20_2128 = torch.constant.int 20
    %int64_2129 = torch.constant.int 64
    %1825 = torch.prim.ListConstruct %int2_2126, %int-1_2127, %int20_2128, %int64_2129 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1826 = torch.aten.view %1818, %1825 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2130 = torch.constant.int 1
    %int2_2131 = torch.constant.int 2
    %1827 = torch.aten.transpose.int %1826, %int1_2130, %int2_2131 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_2132 = torch.constant.float 0.000000e+00
    %false_2133 = torch.constant.bool false
    %none_2134 = torch.constant.none
    %none_2135 = torch.constant.none
    %1828:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1821, %1824, %1827, %float0.000000e00_2132, %false_2133, %none_2134, %none_2135) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2136 = torch.constant.int 1
    %int2_2137 = torch.constant.int 2
    %1829 = torch.aten.transpose.int %1828#0, %int1_2136, %int2_2137 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2138 = torch.constant.int 2
    %int-1_2139 = torch.constant.int -1
    %int1280_2140 = torch.constant.int 1280
    %1830 = torch.prim.ListConstruct %int2_2138, %int-1_2139, %int1280_2140 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1831 = torch.aten.view %1829, %1830 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2141 = torch.constant.int 5
    %1832 = torch.prims.convert_element_type %1831, %int5_2141 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2142 = torch.constant.int 1920
    %int1280_2143 = torch.constant.int 1280
    %1833 = torch.prim.ListConstruct %int1920_2142, %int1280_2143 : (!torch.int, !torch.int) -> !torch.list<int>
    %1834 = torch.aten.view %1832, %1833 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %1835 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2144 = torch.constant.int 0
    %int1_2145 = torch.constant.int 1
    %1836 = torch.aten.transpose.int %1835, %int0_2144, %int1_2145 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %1837 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2146 = torch.constant.int 6
    %1838 = torch.prims.convert_element_type %1837, %int6_2146 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2147 = torch.constant.int 6
    %1839 = torch.prims.convert_element_type %1834, %int6_2147 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2148 = torch.constant.int 6
    %1840 = torch.prims.convert_element_type %1836, %int6_2148 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1841 = torch.aten.mm %1839, %1840 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2149 = torch.constant.int 1
    %1842 = torch.aten.mul.Scalar %1841, %int1_2149 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2150 = torch.constant.int 1
    %1843 = torch.aten.mul.Scalar %1838, %int1_2150 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2151 = torch.constant.int 1
    %1844 = torch.aten.add.Tensor %1842, %1843, %int1_2151 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2152 = torch.constant.int 5
    %1845 = torch.prims.convert_element_type %1844, %int5_2152 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2153 = torch.constant.int 2
    %int960_2154 = torch.constant.int 960
    %int1280_2155 = torch.constant.int 1280
    %1846 = torch.prim.ListConstruct %int2_2153, %int960_2154, %int1280_2155 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1847 = torch.aten.view %1845, %1846 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2156 = torch.constant.none
    %1848 = torch.aten.clone %1847, %none_2156 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2157 = torch.constant.float 1.000000e+00
    %1849 = torch.aten.div.Scalar %1848, %float1.000000e00_2157 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2158 = torch.constant.int 1
    %1850 = torch.aten.add.Tensor %1849, %1786, %int1_2158 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2159 = torch.constant.int 6
    %1851 = torch.prims.convert_element_type %1850, %int6_2159 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2160 = torch.constant.int 2
    %1852 = torch.prim.ListConstruct %int2_2160 : (!torch.int) -> !torch.list<int>
    %int0_2161 = torch.constant.int 0
    %true_2162 = torch.constant.bool true
    %result0_2163, %result1_2164 = torch.aten.var_mean.correction %1851, %1852, %int0_2161, %true_2162 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2165 = torch.constant.float 1.000000e-05
    %int1_2166 = torch.constant.int 1
    %1853 = torch.aten.add.Scalar %result0_2163, %float1.000000e-05_2165, %int1_2166 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1854 = torch.aten.rsqrt %1853 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2167 = torch.constant.int 1
    %1855 = torch.aten.sub.Tensor %1850, %result1_2164, %int1_2167 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1856 = torch.aten.mul.Tensor %1855, %1854 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %1857 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1858 = torch.aten.mul.Tensor %1856, %1857 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %1859 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2168 = torch.constant.int 1
    %1860 = torch.aten.add.Tensor %1858, %1859, %int1_2168 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2169 = torch.constant.int 5
    %1861 = torch.prims.convert_element_type %1860, %int5_2169 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %1862 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2170 = torch.constant.int 0
    %int1_2171 = torch.constant.int 1
    %1863 = torch.aten.transpose.int %1862, %int0_2170, %int1_2171 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2172 = torch.constant.int 1920
    %int1280_2173 = torch.constant.int 1280
    %1864 = torch.prim.ListConstruct %int1920_2172, %int1280_2173 : (!torch.int, !torch.int) -> !torch.list<int>
    %1865 = torch.aten.view %1861, %1864 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1866 = torch.aten.mm %1865, %1863 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2174 = torch.constant.int 2
    %int960_2175 = torch.constant.int 960
    %int1280_2176 = torch.constant.int 1280
    %1867 = torch.prim.ListConstruct %int2_2174, %int960_2175, %int1280_2176 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1868 = torch.aten.view %1866, %1867 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %1869 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2177 = torch.constant.int 0
    %int1_2178 = torch.constant.int 1
    %1870 = torch.aten.transpose.int %1869, %int0_2177, %int1_2178 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2179 = torch.constant.int 32
    %int2048_2180 = torch.constant.int 2048
    %1871 = torch.prim.ListConstruct %int32_2179, %int2048_2180 : (!torch.int, !torch.int) -> !torch.list<int>
    %1872 = torch.aten.view %arg6, %1871 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1873 = torch.aten.mm %1872, %1870 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2181 = torch.constant.int 2
    %int16_2182 = torch.constant.int 16
    %int1280_2183 = torch.constant.int 1280
    %1874 = torch.prim.ListConstruct %int2_2181, %int16_2182, %int1280_2183 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1875 = torch.aten.view %1873, %1874 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %1876 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2184 = torch.constant.int 0
    %int1_2185 = torch.constant.int 1
    %1877 = torch.aten.transpose.int %1876, %int0_2184, %int1_2185 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2186 = torch.constant.int 32
    %int2048_2187 = torch.constant.int 2048
    %1878 = torch.prim.ListConstruct %int32_2186, %int2048_2187 : (!torch.int, !torch.int) -> !torch.list<int>
    %1879 = torch.aten.view %arg6, %1878 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %1880 = torch.aten.mm %1879, %1877 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2188 = torch.constant.int 2
    %int16_2189 = torch.constant.int 16
    %int1280_2190 = torch.constant.int 1280
    %1881 = torch.prim.ListConstruct %int2_2188, %int16_2189, %int1280_2190 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1882 = torch.aten.view %1880, %1881 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_2191 = torch.constant.int 2
    %int-1_2192 = torch.constant.int -1
    %int20_2193 = torch.constant.int 20
    %int64_2194 = torch.constant.int 64
    %1883 = torch.prim.ListConstruct %int2_2191, %int-1_2192, %int20_2193, %int64_2194 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1884 = torch.aten.view %1868, %1883 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2195 = torch.constant.int 1
    %int2_2196 = torch.constant.int 2
    %1885 = torch.aten.transpose.int %1884, %int1_2195, %int2_2196 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2197 = torch.constant.int 2
    %int-1_2198 = torch.constant.int -1
    %int20_2199 = torch.constant.int 20
    %int64_2200 = torch.constant.int 64
    %1886 = torch.prim.ListConstruct %int2_2197, %int-1_2198, %int20_2199, %int64_2200 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1887 = torch.aten.view %1875, %1886 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2201 = torch.constant.int 1
    %int2_2202 = torch.constant.int 2
    %1888 = torch.aten.transpose.int %1887, %int1_2201, %int2_2202 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_2203 = torch.constant.int 2
    %int-1_2204 = torch.constant.int -1
    %int20_2205 = torch.constant.int 20
    %int64_2206 = torch.constant.int 64
    %1889 = torch.prim.ListConstruct %int2_2203, %int-1_2204, %int20_2205, %int64_2206 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1890 = torch.aten.view %1882, %1889 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2207 = torch.constant.int 1
    %int2_2208 = torch.constant.int 2
    %1891 = torch.aten.transpose.int %1890, %int1_2207, %int2_2208 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_2209 = torch.constant.float 0.000000e+00
    %false_2210 = torch.constant.bool false
    %none_2211 = torch.constant.none
    %none_2212 = torch.constant.none
    %1892:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1885, %1888, %1891, %float0.000000e00_2209, %false_2210, %none_2211, %none_2212) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2213 = torch.constant.int 1
    %int2_2214 = torch.constant.int 2
    %1893 = torch.aten.transpose.int %1892#0, %int1_2213, %int2_2214 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2215 = torch.constant.int 2
    %int-1_2216 = torch.constant.int -1
    %int1280_2217 = torch.constant.int 1280
    %1894 = torch.prim.ListConstruct %int2_2215, %int-1_2216, %int1280_2217 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1895 = torch.aten.view %1893, %1894 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2218 = torch.constant.int 5
    %1896 = torch.prims.convert_element_type %1895, %int5_2218 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2219 = torch.constant.int 1920
    %int1280_2220 = torch.constant.int 1280
    %1897 = torch.prim.ListConstruct %int1920_2219, %int1280_2220 : (!torch.int, !torch.int) -> !torch.list<int>
    %1898 = torch.aten.view %1896, %1897 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %1899 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2221 = torch.constant.int 0
    %int1_2222 = torch.constant.int 1
    %1900 = torch.aten.transpose.int %1899, %int0_2221, %int1_2222 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %1901 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2223 = torch.constant.int 6
    %1902 = torch.prims.convert_element_type %1901, %int6_2223 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2224 = torch.constant.int 6
    %1903 = torch.prims.convert_element_type %1898, %int6_2224 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2225 = torch.constant.int 6
    %1904 = torch.prims.convert_element_type %1900, %int6_2225 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %1905 = torch.aten.mm %1903, %1904 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2226 = torch.constant.int 1
    %1906 = torch.aten.mul.Scalar %1905, %int1_2226 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2227 = torch.constant.int 1
    %1907 = torch.aten.mul.Scalar %1902, %int1_2227 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2228 = torch.constant.int 1
    %1908 = torch.aten.add.Tensor %1906, %1907, %int1_2228 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2229 = torch.constant.int 5
    %1909 = torch.prims.convert_element_type %1908, %int5_2229 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2230 = torch.constant.int 2
    %int960_2231 = torch.constant.int 960
    %int1280_2232 = torch.constant.int 1280
    %1910 = torch.prim.ListConstruct %int2_2230, %int960_2231, %int1280_2232 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1911 = torch.aten.view %1909, %1910 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2233 = torch.constant.none
    %1912 = torch.aten.clone %1911, %none_2233 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2234 = torch.constant.float 1.000000e+00
    %1913 = torch.aten.div.Scalar %1912, %float1.000000e00_2234 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2235 = torch.constant.int 1
    %1914 = torch.aten.add.Tensor %1913, %1850, %int1_2235 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2236 = torch.constant.int 6
    %1915 = torch.prims.convert_element_type %1914, %int6_2236 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2237 = torch.constant.int 2
    %1916 = torch.prim.ListConstruct %int2_2237 : (!torch.int) -> !torch.list<int>
    %int0_2238 = torch.constant.int 0
    %true_2239 = torch.constant.bool true
    %result0_2240, %result1_2241 = torch.aten.var_mean.correction %1915, %1916, %int0_2238, %true_2239 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2242 = torch.constant.float 1.000000e-05
    %int1_2243 = torch.constant.int 1
    %1917 = torch.aten.add.Scalar %result0_2240, %float1.000000e-05_2242, %int1_2243 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1918 = torch.aten.rsqrt %1917 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2244 = torch.constant.int 1
    %1919 = torch.aten.sub.Tensor %1914, %result1_2241, %int1_2244 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1920 = torch.aten.mul.Tensor %1919, %1918 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %1921 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1922 = torch.aten.mul.Tensor %1920, %1921 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %1923 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2245 = torch.constant.int 1
    %1924 = torch.aten.add.Tensor %1922, %1923, %int1_2245 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2246 = torch.constant.int 5
    %1925 = torch.prims.convert_element_type %1924, %int5_2246 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2247 = torch.constant.int 1920
    %int1280_2248 = torch.constant.int 1280
    %1926 = torch.prim.ListConstruct %int1920_2247, %int1280_2248 : (!torch.int, !torch.int) -> !torch.list<int>
    %1927 = torch.aten.view %1925, %1926 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %1928 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_2249 = torch.constant.int 0
    %int1_2250 = torch.constant.int 1
    %1929 = torch.aten.transpose.int %1928, %int0_2249, %int1_2250 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %1930 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_2251 = torch.constant.int 6
    %1931 = torch.prims.convert_element_type %1930, %int6_2251 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_2252 = torch.constant.int 6
    %1932 = torch.prims.convert_element_type %1927, %int6_2252 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2253 = torch.constant.int 6
    %1933 = torch.prims.convert_element_type %1929, %int6_2253 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %1934 = torch.aten.mm %1932, %1933 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_2254 = torch.constant.int 1
    %1935 = torch.aten.mul.Scalar %1934, %int1_2254 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_2255 = torch.constant.int 1
    %1936 = torch.aten.mul.Scalar %1931, %int1_2255 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_2256 = torch.constant.int 1
    %1937 = torch.aten.add.Tensor %1935, %1936, %int1_2256 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_2257 = torch.constant.int 5
    %1938 = torch.prims.convert_element_type %1937, %int5_2257 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_2258 = torch.constant.int 2
    %int960_2259 = torch.constant.int 960
    %int10240_2260 = torch.constant.int 10240
    %1939 = torch.prim.ListConstruct %int2_2258, %int960_2259, %int10240_2260 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1940 = torch.aten.view %1938, %1939 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_2261 = torch.constant.int -1
    %int0_2262 = torch.constant.int 0
    %int5120_2263 = torch.constant.int 5120
    %int1_2264 = torch.constant.int 1
    %1941 = torch.aten.slice.Tensor %1940, %int-1_2261, %int0_2262, %int5120_2263, %int1_2264 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_2265 = torch.constant.int -1
    %int5120_2266 = torch.constant.int 5120
    %int10240_2267 = torch.constant.int 10240
    %int1_2268 = torch.constant.int 1
    %1942 = torch.aten.slice.Tensor %1940, %int-1_2265, %int5120_2266, %int10240_2267, %int1_2268 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_2269 = torch.constant.str "none"
    %1943 = torch.aten.gelu %1942, %str_2269 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %1944 = torch.aten.mul.Tensor %1941, %1943 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_2270 = torch.constant.none
    %1945 = torch.aten.clone %1944, %none_2270 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_2271 = torch.constant.int 1920
    %int5120_2272 = torch.constant.int 5120
    %1946 = torch.prim.ListConstruct %int1920_2271, %int5120_2272 : (!torch.int, !torch.int) -> !torch.list<int>
    %1947 = torch.aten.view %1945, %1946 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %1948 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2273 = torch.constant.int 0
    %int1_2274 = torch.constant.int 1
    %1949 = torch.aten.transpose.int %1948, %int0_2273, %int1_2274 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %1950 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2275 = torch.constant.int 6
    %1951 = torch.prims.convert_element_type %1950, %int6_2275 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2276 = torch.constant.int 6
    %1952 = torch.prims.convert_element_type %1947, %int6_2276 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_2277 = torch.constant.int 6
    %1953 = torch.prims.convert_element_type %1949, %int6_2277 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %1954 = torch.aten.mm %1952, %1953 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2278 = torch.constant.int 1
    %1955 = torch.aten.mul.Scalar %1954, %int1_2278 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2279 = torch.constant.int 1
    %1956 = torch.aten.mul.Scalar %1951, %int1_2279 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2280 = torch.constant.int 1
    %1957 = torch.aten.add.Tensor %1955, %1956, %int1_2280 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2281 = torch.constant.int 5
    %1958 = torch.prims.convert_element_type %1957, %int5_2281 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2282 = torch.constant.int 2
    %int960_2283 = torch.constant.int 960
    %int1280_2284 = torch.constant.int 1280
    %1959 = torch.prim.ListConstruct %int2_2282, %int960_2283, %int1280_2284 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1960 = torch.aten.view %1958, %1959 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_2285 = torch.constant.int 1
    %1961 = torch.aten.add.Tensor %1960, %1914, %int1_2285 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2286 = torch.constant.int 6
    %1962 = torch.prims.convert_element_type %1961, %int6_2286 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2287 = torch.constant.int 2
    %1963 = torch.prim.ListConstruct %int2_2287 : (!torch.int) -> !torch.list<int>
    %int0_2288 = torch.constant.int 0
    %true_2289 = torch.constant.bool true
    %result0_2290, %result1_2291 = torch.aten.var_mean.correction %1962, %1963, %int0_2288, %true_2289 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2292 = torch.constant.float 1.000000e-05
    %int1_2293 = torch.constant.int 1
    %1964 = torch.aten.add.Scalar %result0_2290, %float1.000000e-05_2292, %int1_2293 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %1965 = torch.aten.rsqrt %1964 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2294 = torch.constant.int 1
    %1966 = torch.aten.sub.Tensor %1961, %result1_2291, %int1_2294 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %1967 = torch.aten.mul.Tensor %1966, %1965 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %1968 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %1969 = torch.aten.mul.Tensor %1967, %1968 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %1970 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2295 = torch.constant.int 1
    %1971 = torch.aten.add.Tensor %1969, %1970, %int1_2295 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2296 = torch.constant.int 5
    %1972 = torch.prims.convert_element_type %1971, %int5_2296 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %1973 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2297 = torch.constant.int 0
    %int1_2298 = torch.constant.int 1
    %1974 = torch.aten.transpose.int %1973, %int0_2297, %int1_2298 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2299 = torch.constant.int 1920
    %int1280_2300 = torch.constant.int 1280
    %1975 = torch.prim.ListConstruct %int1920_2299, %int1280_2300 : (!torch.int, !torch.int) -> !torch.list<int>
    %1976 = torch.aten.view %1972, %1975 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1977 = torch.aten.mm %1976, %1974 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2301 = torch.constant.int 2
    %int960_2302 = torch.constant.int 960
    %int1280_2303 = torch.constant.int 1280
    %1978 = torch.prim.ListConstruct %int2_2301, %int960_2302, %int1280_2303 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1979 = torch.aten.view %1977, %1978 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %1980 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2304 = torch.constant.int 0
    %int1_2305 = torch.constant.int 1
    %1981 = torch.aten.transpose.int %1980, %int0_2304, %int1_2305 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2306 = torch.constant.int 1920
    %int1280_2307 = torch.constant.int 1280
    %1982 = torch.prim.ListConstruct %int1920_2306, %int1280_2307 : (!torch.int, !torch.int) -> !torch.list<int>
    %1983 = torch.aten.view %1972, %1982 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1984 = torch.aten.mm %1983, %1981 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2308 = torch.constant.int 2
    %int960_2309 = torch.constant.int 960
    %int1280_2310 = torch.constant.int 1280
    %1985 = torch.prim.ListConstruct %int2_2308, %int960_2309, %int1280_2310 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1986 = torch.aten.view %1984, %1985 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %1987 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2311 = torch.constant.int 0
    %int1_2312 = torch.constant.int 1
    %1988 = torch.aten.transpose.int %1987, %int0_2311, %int1_2312 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2313 = torch.constant.int 1920
    %int1280_2314 = torch.constant.int 1280
    %1989 = torch.prim.ListConstruct %int1920_2313, %int1280_2314 : (!torch.int, !torch.int) -> !torch.list<int>
    %1990 = torch.aten.view %1972, %1989 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %1991 = torch.aten.mm %1990, %1988 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2315 = torch.constant.int 2
    %int960_2316 = torch.constant.int 960
    %int1280_2317 = torch.constant.int 1280
    %1992 = torch.prim.ListConstruct %int2_2315, %int960_2316, %int1280_2317 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1993 = torch.aten.view %1991, %1992 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_2318 = torch.constant.int 2
    %int-1_2319 = torch.constant.int -1
    %int20_2320 = torch.constant.int 20
    %int64_2321 = torch.constant.int 64
    %1994 = torch.prim.ListConstruct %int2_2318, %int-1_2319, %int20_2320, %int64_2321 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1995 = torch.aten.view %1979, %1994 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2322 = torch.constant.int 1
    %int2_2323 = torch.constant.int 2
    %1996 = torch.aten.transpose.int %1995, %int1_2322, %int2_2323 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2324 = torch.constant.int 2
    %int-1_2325 = torch.constant.int -1
    %int20_2326 = torch.constant.int 20
    %int64_2327 = torch.constant.int 64
    %1997 = torch.prim.ListConstruct %int2_2324, %int-1_2325, %int20_2326, %int64_2327 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %1998 = torch.aten.view %1986, %1997 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2328 = torch.constant.int 1
    %int2_2329 = torch.constant.int 2
    %1999 = torch.aten.transpose.int %1998, %int1_2328, %int2_2329 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2330 = torch.constant.int 2
    %int-1_2331 = torch.constant.int -1
    %int20_2332 = torch.constant.int 20
    %int64_2333 = torch.constant.int 64
    %2000 = torch.prim.ListConstruct %int2_2330, %int-1_2331, %int20_2332, %int64_2333 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2001 = torch.aten.view %1993, %2000 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2334 = torch.constant.int 1
    %int2_2335 = torch.constant.int 2
    %2002 = torch.aten.transpose.int %2001, %int1_2334, %int2_2335 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_2336 = torch.constant.float 0.000000e+00
    %false_2337 = torch.constant.bool false
    %none_2338 = torch.constant.none
    %none_2339 = torch.constant.none
    %2003:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%1996, %1999, %2002, %float0.000000e00_2336, %false_2337, %none_2338, %none_2339) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2340 = torch.constant.int 1
    %int2_2341 = torch.constant.int 2
    %2004 = torch.aten.transpose.int %2003#0, %int1_2340, %int2_2341 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2342 = torch.constant.int 2
    %int-1_2343 = torch.constant.int -1
    %int1280_2344 = torch.constant.int 1280
    %2005 = torch.prim.ListConstruct %int2_2342, %int-1_2343, %int1280_2344 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2006 = torch.aten.view %2004, %2005 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2345 = torch.constant.int 5
    %2007 = torch.prims.convert_element_type %2006, %int5_2345 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2346 = torch.constant.int 1920
    %int1280_2347 = torch.constant.int 1280
    %2008 = torch.prim.ListConstruct %int1920_2346, %int1280_2347 : (!torch.int, !torch.int) -> !torch.list<int>
    %2009 = torch.aten.view %2007, %2008 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2010 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2348 = torch.constant.int 0
    %int1_2349 = torch.constant.int 1
    %2011 = torch.aten.transpose.int %2010, %int0_2348, %int1_2349 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %2012 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2350 = torch.constant.int 6
    %2013 = torch.prims.convert_element_type %2012, %int6_2350 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2351 = torch.constant.int 6
    %2014 = torch.prims.convert_element_type %2009, %int6_2351 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2352 = torch.constant.int 6
    %2015 = torch.prims.convert_element_type %2011, %int6_2352 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2016 = torch.aten.mm %2014, %2015 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2353 = torch.constant.int 1
    %2017 = torch.aten.mul.Scalar %2016, %int1_2353 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2354 = torch.constant.int 1
    %2018 = torch.aten.mul.Scalar %2013, %int1_2354 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2355 = torch.constant.int 1
    %2019 = torch.aten.add.Tensor %2017, %2018, %int1_2355 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2356 = torch.constant.int 5
    %2020 = torch.prims.convert_element_type %2019, %int5_2356 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2357 = torch.constant.int 2
    %int960_2358 = torch.constant.int 960
    %int1280_2359 = torch.constant.int 1280
    %2021 = torch.prim.ListConstruct %int2_2357, %int960_2358, %int1280_2359 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2022 = torch.aten.view %2020, %2021 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2360 = torch.constant.none
    %2023 = torch.aten.clone %2022, %none_2360 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2361 = torch.constant.float 1.000000e+00
    %2024 = torch.aten.div.Scalar %2023, %float1.000000e00_2361 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2362 = torch.constant.int 1
    %2025 = torch.aten.add.Tensor %2024, %1961, %int1_2362 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2363 = torch.constant.int 6
    %2026 = torch.prims.convert_element_type %2025, %int6_2363 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2364 = torch.constant.int 2
    %2027 = torch.prim.ListConstruct %int2_2364 : (!torch.int) -> !torch.list<int>
    %int0_2365 = torch.constant.int 0
    %true_2366 = torch.constant.bool true
    %result0_2367, %result1_2368 = torch.aten.var_mean.correction %2026, %2027, %int0_2365, %true_2366 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2369 = torch.constant.float 1.000000e-05
    %int1_2370 = torch.constant.int 1
    %2028 = torch.aten.add.Scalar %result0_2367, %float1.000000e-05_2369, %int1_2370 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2029 = torch.aten.rsqrt %2028 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2371 = torch.constant.int 1
    %2030 = torch.aten.sub.Tensor %2025, %result1_2368, %int1_2371 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2031 = torch.aten.mul.Tensor %2030, %2029 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %2032 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2033 = torch.aten.mul.Tensor %2031, %2032 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %2034 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2372 = torch.constant.int 1
    %2035 = torch.aten.add.Tensor %2033, %2034, %int1_2372 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2373 = torch.constant.int 5
    %2036 = torch.prims.convert_element_type %2035, %int5_2373 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %2037 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2374 = torch.constant.int 0
    %int1_2375 = torch.constant.int 1
    %2038 = torch.aten.transpose.int %2037, %int0_2374, %int1_2375 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2376 = torch.constant.int 1920
    %int1280_2377 = torch.constant.int 1280
    %2039 = torch.prim.ListConstruct %int1920_2376, %int1280_2377 : (!torch.int, !torch.int) -> !torch.list<int>
    %2040 = torch.aten.view %2036, %2039 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2041 = torch.aten.mm %2040, %2038 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2378 = torch.constant.int 2
    %int960_2379 = torch.constant.int 960
    %int1280_2380 = torch.constant.int 1280
    %2042 = torch.prim.ListConstruct %int2_2378, %int960_2379, %int1280_2380 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2043 = torch.aten.view %2041, %2042 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %2044 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2381 = torch.constant.int 0
    %int1_2382 = torch.constant.int 1
    %2045 = torch.aten.transpose.int %2044, %int0_2381, %int1_2382 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2383 = torch.constant.int 32
    %int2048_2384 = torch.constant.int 2048
    %2046 = torch.prim.ListConstruct %int32_2383, %int2048_2384 : (!torch.int, !torch.int) -> !torch.list<int>
    %2047 = torch.aten.view %arg6, %2046 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2048 = torch.aten.mm %2047, %2045 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2385 = torch.constant.int 2
    %int16_2386 = torch.constant.int 16
    %int1280_2387 = torch.constant.int 1280
    %2049 = torch.prim.ListConstruct %int2_2385, %int16_2386, %int1280_2387 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2050 = torch.aten.view %2048, %2049 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %2051 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2388 = torch.constant.int 0
    %int1_2389 = torch.constant.int 1
    %2052 = torch.aten.transpose.int %2051, %int0_2388, %int1_2389 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2390 = torch.constant.int 32
    %int2048_2391 = torch.constant.int 2048
    %2053 = torch.prim.ListConstruct %int32_2390, %int2048_2391 : (!torch.int, !torch.int) -> !torch.list<int>
    %2054 = torch.aten.view %arg6, %2053 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2055 = torch.aten.mm %2054, %2052 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2392 = torch.constant.int 2
    %int16_2393 = torch.constant.int 16
    %int1280_2394 = torch.constant.int 1280
    %2056 = torch.prim.ListConstruct %int2_2392, %int16_2393, %int1280_2394 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2057 = torch.aten.view %2055, %2056 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_2395 = torch.constant.int 2
    %int-1_2396 = torch.constant.int -1
    %int20_2397 = torch.constant.int 20
    %int64_2398 = torch.constant.int 64
    %2058 = torch.prim.ListConstruct %int2_2395, %int-1_2396, %int20_2397, %int64_2398 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2059 = torch.aten.view %2043, %2058 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2399 = torch.constant.int 1
    %int2_2400 = torch.constant.int 2
    %2060 = torch.aten.transpose.int %2059, %int1_2399, %int2_2400 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2401 = torch.constant.int 2
    %int-1_2402 = torch.constant.int -1
    %int20_2403 = torch.constant.int 20
    %int64_2404 = torch.constant.int 64
    %2061 = torch.prim.ListConstruct %int2_2401, %int-1_2402, %int20_2403, %int64_2404 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2062 = torch.aten.view %2050, %2061 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2405 = torch.constant.int 1
    %int2_2406 = torch.constant.int 2
    %2063 = torch.aten.transpose.int %2062, %int1_2405, %int2_2406 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_2407 = torch.constant.int 2
    %int-1_2408 = torch.constant.int -1
    %int20_2409 = torch.constant.int 20
    %int64_2410 = torch.constant.int 64
    %2064 = torch.prim.ListConstruct %int2_2407, %int-1_2408, %int20_2409, %int64_2410 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2065 = torch.aten.view %2057, %2064 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2411 = torch.constant.int 1
    %int2_2412 = torch.constant.int 2
    %2066 = torch.aten.transpose.int %2065, %int1_2411, %int2_2412 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_2413 = torch.constant.float 0.000000e+00
    %false_2414 = torch.constant.bool false
    %none_2415 = torch.constant.none
    %none_2416 = torch.constant.none
    %2067:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2060, %2063, %2066, %float0.000000e00_2413, %false_2414, %none_2415, %none_2416) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2417 = torch.constant.int 1
    %int2_2418 = torch.constant.int 2
    %2068 = torch.aten.transpose.int %2067#0, %int1_2417, %int2_2418 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2419 = torch.constant.int 2
    %int-1_2420 = torch.constant.int -1
    %int1280_2421 = torch.constant.int 1280
    %2069 = torch.prim.ListConstruct %int2_2419, %int-1_2420, %int1280_2421 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2070 = torch.aten.view %2068, %2069 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2422 = torch.constant.int 5
    %2071 = torch.prims.convert_element_type %2070, %int5_2422 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2423 = torch.constant.int 1920
    %int1280_2424 = torch.constant.int 1280
    %2072 = torch.prim.ListConstruct %int1920_2423, %int1280_2424 : (!torch.int, !torch.int) -> !torch.list<int>
    %2073 = torch.aten.view %2071, %2072 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2074 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2425 = torch.constant.int 0
    %int1_2426 = torch.constant.int 1
    %2075 = torch.aten.transpose.int %2074, %int0_2425, %int1_2426 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %2076 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2427 = torch.constant.int 6
    %2077 = torch.prims.convert_element_type %2076, %int6_2427 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2428 = torch.constant.int 6
    %2078 = torch.prims.convert_element_type %2073, %int6_2428 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2429 = torch.constant.int 6
    %2079 = torch.prims.convert_element_type %2075, %int6_2429 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2080 = torch.aten.mm %2078, %2079 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2430 = torch.constant.int 1
    %2081 = torch.aten.mul.Scalar %2080, %int1_2430 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2431 = torch.constant.int 1
    %2082 = torch.aten.mul.Scalar %2077, %int1_2431 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2432 = torch.constant.int 1
    %2083 = torch.aten.add.Tensor %2081, %2082, %int1_2432 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2433 = torch.constant.int 5
    %2084 = torch.prims.convert_element_type %2083, %int5_2433 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2434 = torch.constant.int 2
    %int960_2435 = torch.constant.int 960
    %int1280_2436 = torch.constant.int 1280
    %2085 = torch.prim.ListConstruct %int2_2434, %int960_2435, %int1280_2436 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2086 = torch.aten.view %2084, %2085 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2437 = torch.constant.none
    %2087 = torch.aten.clone %2086, %none_2437 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2438 = torch.constant.float 1.000000e+00
    %2088 = torch.aten.div.Scalar %2087, %float1.000000e00_2438 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2439 = torch.constant.int 1
    %2089 = torch.aten.add.Tensor %2088, %2025, %int1_2439 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2440 = torch.constant.int 6
    %2090 = torch.prims.convert_element_type %2089, %int6_2440 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2441 = torch.constant.int 2
    %2091 = torch.prim.ListConstruct %int2_2441 : (!torch.int) -> !torch.list<int>
    %int0_2442 = torch.constant.int 0
    %true_2443 = torch.constant.bool true
    %result0_2444, %result1_2445 = torch.aten.var_mean.correction %2090, %2091, %int0_2442, %true_2443 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2446 = torch.constant.float 1.000000e-05
    %int1_2447 = torch.constant.int 1
    %2092 = torch.aten.add.Scalar %result0_2444, %float1.000000e-05_2446, %int1_2447 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2093 = torch.aten.rsqrt %2092 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2448 = torch.constant.int 1
    %2094 = torch.aten.sub.Tensor %2089, %result1_2445, %int1_2448 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2095 = torch.aten.mul.Tensor %2094, %2093 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %2096 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2097 = torch.aten.mul.Tensor %2095, %2096 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %2098 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2449 = torch.constant.int 1
    %2099 = torch.aten.add.Tensor %2097, %2098, %int1_2449 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2450 = torch.constant.int 5
    %2100 = torch.prims.convert_element_type %2099, %int5_2450 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2451 = torch.constant.int 1920
    %int1280_2452 = torch.constant.int 1280
    %2101 = torch.prim.ListConstruct %int1920_2451, %int1280_2452 : (!torch.int, !torch.int) -> !torch.list<int>
    %2102 = torch.aten.view %2100, %2101 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2103 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_2453 = torch.constant.int 0
    %int1_2454 = torch.constant.int 1
    %2104 = torch.aten.transpose.int %2103, %int0_2453, %int1_2454 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %2105 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_2455 = torch.constant.int 6
    %2106 = torch.prims.convert_element_type %2105, %int6_2455 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_2456 = torch.constant.int 6
    %2107 = torch.prims.convert_element_type %2102, %int6_2456 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2457 = torch.constant.int 6
    %2108 = torch.prims.convert_element_type %2104, %int6_2457 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2109 = torch.aten.mm %2107, %2108 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_2458 = torch.constant.int 1
    %2110 = torch.aten.mul.Scalar %2109, %int1_2458 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_2459 = torch.constant.int 1
    %2111 = torch.aten.mul.Scalar %2106, %int1_2459 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_2460 = torch.constant.int 1
    %2112 = torch.aten.add.Tensor %2110, %2111, %int1_2460 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_2461 = torch.constant.int 5
    %2113 = torch.prims.convert_element_type %2112, %int5_2461 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_2462 = torch.constant.int 2
    %int960_2463 = torch.constant.int 960
    %int10240_2464 = torch.constant.int 10240
    %2114 = torch.prim.ListConstruct %int2_2462, %int960_2463, %int10240_2464 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2115 = torch.aten.view %2113, %2114 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_2465 = torch.constant.int -1
    %int0_2466 = torch.constant.int 0
    %int5120_2467 = torch.constant.int 5120
    %int1_2468 = torch.constant.int 1
    %2116 = torch.aten.slice.Tensor %2115, %int-1_2465, %int0_2466, %int5120_2467, %int1_2468 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_2469 = torch.constant.int -1
    %int5120_2470 = torch.constant.int 5120
    %int10240_2471 = torch.constant.int 10240
    %int1_2472 = torch.constant.int 1
    %2117 = torch.aten.slice.Tensor %2115, %int-1_2469, %int5120_2470, %int10240_2471, %int1_2472 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_2473 = torch.constant.str "none"
    %2118 = torch.aten.gelu %2117, %str_2473 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %2119 = torch.aten.mul.Tensor %2116, %2118 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_2474 = torch.constant.none
    %2120 = torch.aten.clone %2119, %none_2474 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_2475 = torch.constant.int 1920
    %int5120_2476 = torch.constant.int 5120
    %2121 = torch.prim.ListConstruct %int1920_2475, %int5120_2476 : (!torch.int, !torch.int) -> !torch.list<int>
    %2122 = torch.aten.view %2120, %2121 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %2123 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2477 = torch.constant.int 0
    %int1_2478 = torch.constant.int 1
    %2124 = torch.aten.transpose.int %2123, %int0_2477, %int1_2478 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %2125 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2479 = torch.constant.int 6
    %2126 = torch.prims.convert_element_type %2125, %int6_2479 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2480 = torch.constant.int 6
    %2127 = torch.prims.convert_element_type %2122, %int6_2480 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_2481 = torch.constant.int 6
    %2128 = torch.prims.convert_element_type %2124, %int6_2481 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2129 = torch.aten.mm %2127, %2128 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2482 = torch.constant.int 1
    %2130 = torch.aten.mul.Scalar %2129, %int1_2482 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2483 = torch.constant.int 1
    %2131 = torch.aten.mul.Scalar %2126, %int1_2483 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2484 = torch.constant.int 1
    %2132 = torch.aten.add.Tensor %2130, %2131, %int1_2484 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2485 = torch.constant.int 5
    %2133 = torch.prims.convert_element_type %2132, %int5_2485 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2486 = torch.constant.int 2
    %int960_2487 = torch.constant.int 960
    %int1280_2488 = torch.constant.int 1280
    %2134 = torch.prim.ListConstruct %int2_2486, %int960_2487, %int1280_2488 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2135 = torch.aten.view %2133, %2134 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_2489 = torch.constant.int 1
    %2136 = torch.aten.add.Tensor %2135, %2089, %int1_2489 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2490 = torch.constant.int 6
    %2137 = torch.prims.convert_element_type %2136, %int6_2490 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2491 = torch.constant.int 2
    %2138 = torch.prim.ListConstruct %int2_2491 : (!torch.int) -> !torch.list<int>
    %int0_2492 = torch.constant.int 0
    %true_2493 = torch.constant.bool true
    %result0_2494, %result1_2495 = torch.aten.var_mean.correction %2137, %2138, %int0_2492, %true_2493 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2496 = torch.constant.float 1.000000e-05
    %int1_2497 = torch.constant.int 1
    %2139 = torch.aten.add.Scalar %result0_2494, %float1.000000e-05_2496, %int1_2497 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2140 = torch.aten.rsqrt %2139 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2498 = torch.constant.int 1
    %2141 = torch.aten.sub.Tensor %2136, %result1_2495, %int1_2498 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2142 = torch.aten.mul.Tensor %2141, %2140 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %2143 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2144 = torch.aten.mul.Tensor %2142, %2143 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %2145 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2499 = torch.constant.int 1
    %2146 = torch.aten.add.Tensor %2144, %2145, %int1_2499 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2500 = torch.constant.int 5
    %2147 = torch.prims.convert_element_type %2146, %int5_2500 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %2148 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2501 = torch.constant.int 0
    %int1_2502 = torch.constant.int 1
    %2149 = torch.aten.transpose.int %2148, %int0_2501, %int1_2502 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2503 = torch.constant.int 1920
    %int1280_2504 = torch.constant.int 1280
    %2150 = torch.prim.ListConstruct %int1920_2503, %int1280_2504 : (!torch.int, !torch.int) -> !torch.list<int>
    %2151 = torch.aten.view %2147, %2150 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2152 = torch.aten.mm %2151, %2149 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2505 = torch.constant.int 2
    %int960_2506 = torch.constant.int 960
    %int1280_2507 = torch.constant.int 1280
    %2153 = torch.prim.ListConstruct %int2_2505, %int960_2506, %int1280_2507 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2154 = torch.aten.view %2152, %2153 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %2155 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2508 = torch.constant.int 0
    %int1_2509 = torch.constant.int 1
    %2156 = torch.aten.transpose.int %2155, %int0_2508, %int1_2509 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2510 = torch.constant.int 1920
    %int1280_2511 = torch.constant.int 1280
    %2157 = torch.prim.ListConstruct %int1920_2510, %int1280_2511 : (!torch.int, !torch.int) -> !torch.list<int>
    %2158 = torch.aten.view %2147, %2157 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2159 = torch.aten.mm %2158, %2156 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2512 = torch.constant.int 2
    %int960_2513 = torch.constant.int 960
    %int1280_2514 = torch.constant.int 1280
    %2160 = torch.prim.ListConstruct %int2_2512, %int960_2513, %int1280_2514 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2161 = torch.aten.view %2159, %2160 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %2162 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2515 = torch.constant.int 0
    %int1_2516 = torch.constant.int 1
    %2163 = torch.aten.transpose.int %2162, %int0_2515, %int1_2516 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2517 = torch.constant.int 1920
    %int1280_2518 = torch.constant.int 1280
    %2164 = torch.prim.ListConstruct %int1920_2517, %int1280_2518 : (!torch.int, !torch.int) -> !torch.list<int>
    %2165 = torch.aten.view %2147, %2164 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2166 = torch.aten.mm %2165, %2163 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2519 = torch.constant.int 2
    %int960_2520 = torch.constant.int 960
    %int1280_2521 = torch.constant.int 1280
    %2167 = torch.prim.ListConstruct %int2_2519, %int960_2520, %int1280_2521 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2168 = torch.aten.view %2166, %2167 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_2522 = torch.constant.int 2
    %int-1_2523 = torch.constant.int -1
    %int20_2524 = torch.constant.int 20
    %int64_2525 = torch.constant.int 64
    %2169 = torch.prim.ListConstruct %int2_2522, %int-1_2523, %int20_2524, %int64_2525 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2170 = torch.aten.view %2154, %2169 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2526 = torch.constant.int 1
    %int2_2527 = torch.constant.int 2
    %2171 = torch.aten.transpose.int %2170, %int1_2526, %int2_2527 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2528 = torch.constant.int 2
    %int-1_2529 = torch.constant.int -1
    %int20_2530 = torch.constant.int 20
    %int64_2531 = torch.constant.int 64
    %2172 = torch.prim.ListConstruct %int2_2528, %int-1_2529, %int20_2530, %int64_2531 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2173 = torch.aten.view %2161, %2172 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2532 = torch.constant.int 1
    %int2_2533 = torch.constant.int 2
    %2174 = torch.aten.transpose.int %2173, %int1_2532, %int2_2533 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2534 = torch.constant.int 2
    %int-1_2535 = torch.constant.int -1
    %int20_2536 = torch.constant.int 20
    %int64_2537 = torch.constant.int 64
    %2175 = torch.prim.ListConstruct %int2_2534, %int-1_2535, %int20_2536, %int64_2537 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2176 = torch.aten.view %2168, %2175 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2538 = torch.constant.int 1
    %int2_2539 = torch.constant.int 2
    %2177 = torch.aten.transpose.int %2176, %int1_2538, %int2_2539 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_2540 = torch.constant.float 0.000000e+00
    %false_2541 = torch.constant.bool false
    %none_2542 = torch.constant.none
    %none_2543 = torch.constant.none
    %2178:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2171, %2174, %2177, %float0.000000e00_2540, %false_2541, %none_2542, %none_2543) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2544 = torch.constant.int 1
    %int2_2545 = torch.constant.int 2
    %2179 = torch.aten.transpose.int %2178#0, %int1_2544, %int2_2545 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2546 = torch.constant.int 2
    %int-1_2547 = torch.constant.int -1
    %int1280_2548 = torch.constant.int 1280
    %2180 = torch.prim.ListConstruct %int2_2546, %int-1_2547, %int1280_2548 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2181 = torch.aten.view %2179, %2180 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2549 = torch.constant.int 5
    %2182 = torch.prims.convert_element_type %2181, %int5_2549 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2550 = torch.constant.int 1920
    %int1280_2551 = torch.constant.int 1280
    %2183 = torch.prim.ListConstruct %int1920_2550, %int1280_2551 : (!torch.int, !torch.int) -> !torch.list<int>
    %2184 = torch.aten.view %2182, %2183 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2185 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2552 = torch.constant.int 0
    %int1_2553 = torch.constant.int 1
    %2186 = torch.aten.transpose.int %2185, %int0_2552, %int1_2553 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %2187 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2554 = torch.constant.int 6
    %2188 = torch.prims.convert_element_type %2187, %int6_2554 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2555 = torch.constant.int 6
    %2189 = torch.prims.convert_element_type %2184, %int6_2555 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2556 = torch.constant.int 6
    %2190 = torch.prims.convert_element_type %2186, %int6_2556 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2191 = torch.aten.mm %2189, %2190 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2557 = torch.constant.int 1
    %2192 = torch.aten.mul.Scalar %2191, %int1_2557 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2558 = torch.constant.int 1
    %2193 = torch.aten.mul.Scalar %2188, %int1_2558 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2559 = torch.constant.int 1
    %2194 = torch.aten.add.Tensor %2192, %2193, %int1_2559 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2560 = torch.constant.int 5
    %2195 = torch.prims.convert_element_type %2194, %int5_2560 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2561 = torch.constant.int 2
    %int960_2562 = torch.constant.int 960
    %int1280_2563 = torch.constant.int 1280
    %2196 = torch.prim.ListConstruct %int2_2561, %int960_2562, %int1280_2563 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2197 = torch.aten.view %2195, %2196 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2564 = torch.constant.none
    %2198 = torch.aten.clone %2197, %none_2564 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2565 = torch.constant.float 1.000000e+00
    %2199 = torch.aten.div.Scalar %2198, %float1.000000e00_2565 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2566 = torch.constant.int 1
    %2200 = torch.aten.add.Tensor %2199, %2136, %int1_2566 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2567 = torch.constant.int 6
    %2201 = torch.prims.convert_element_type %2200, %int6_2567 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2568 = torch.constant.int 2
    %2202 = torch.prim.ListConstruct %int2_2568 : (!torch.int) -> !torch.list<int>
    %int0_2569 = torch.constant.int 0
    %true_2570 = torch.constant.bool true
    %result0_2571, %result1_2572 = torch.aten.var_mean.correction %2201, %2202, %int0_2569, %true_2570 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2573 = torch.constant.float 1.000000e-05
    %int1_2574 = torch.constant.int 1
    %2203 = torch.aten.add.Scalar %result0_2571, %float1.000000e-05_2573, %int1_2574 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2204 = torch.aten.rsqrt %2203 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2575 = torch.constant.int 1
    %2205 = torch.aten.sub.Tensor %2200, %result1_2572, %int1_2575 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2206 = torch.aten.mul.Tensor %2205, %2204 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %2207 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2208 = torch.aten.mul.Tensor %2206, %2207 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %2209 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2576 = torch.constant.int 1
    %2210 = torch.aten.add.Tensor %2208, %2209, %int1_2576 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2577 = torch.constant.int 5
    %2211 = torch.prims.convert_element_type %2210, %int5_2577 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %2212 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2578 = torch.constant.int 0
    %int1_2579 = torch.constant.int 1
    %2213 = torch.aten.transpose.int %2212, %int0_2578, %int1_2579 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2580 = torch.constant.int 1920
    %int1280_2581 = torch.constant.int 1280
    %2214 = torch.prim.ListConstruct %int1920_2580, %int1280_2581 : (!torch.int, !torch.int) -> !torch.list<int>
    %2215 = torch.aten.view %2211, %2214 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2216 = torch.aten.mm %2215, %2213 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2582 = torch.constant.int 2
    %int960_2583 = torch.constant.int 960
    %int1280_2584 = torch.constant.int 1280
    %2217 = torch.prim.ListConstruct %int2_2582, %int960_2583, %int1280_2584 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2218 = torch.aten.view %2216, %2217 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %2219 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2585 = torch.constant.int 0
    %int1_2586 = torch.constant.int 1
    %2220 = torch.aten.transpose.int %2219, %int0_2585, %int1_2586 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2587 = torch.constant.int 32
    %int2048_2588 = torch.constant.int 2048
    %2221 = torch.prim.ListConstruct %int32_2587, %int2048_2588 : (!torch.int, !torch.int) -> !torch.list<int>
    %2222 = torch.aten.view %arg6, %2221 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2223 = torch.aten.mm %2222, %2220 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2589 = torch.constant.int 2
    %int16_2590 = torch.constant.int 16
    %int1280_2591 = torch.constant.int 1280
    %2224 = torch.prim.ListConstruct %int2_2589, %int16_2590, %int1280_2591 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2225 = torch.aten.view %2223, %2224 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %2226 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2592 = torch.constant.int 0
    %int1_2593 = torch.constant.int 1
    %2227 = torch.aten.transpose.int %2226, %int0_2592, %int1_2593 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2594 = torch.constant.int 32
    %int2048_2595 = torch.constant.int 2048
    %2228 = torch.prim.ListConstruct %int32_2594, %int2048_2595 : (!torch.int, !torch.int) -> !torch.list<int>
    %2229 = torch.aten.view %arg6, %2228 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2230 = torch.aten.mm %2229, %2227 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2596 = torch.constant.int 2
    %int16_2597 = torch.constant.int 16
    %int1280_2598 = torch.constant.int 1280
    %2231 = torch.prim.ListConstruct %int2_2596, %int16_2597, %int1280_2598 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2232 = torch.aten.view %2230, %2231 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_2599 = torch.constant.int 2
    %int-1_2600 = torch.constant.int -1
    %int20_2601 = torch.constant.int 20
    %int64_2602 = torch.constant.int 64
    %2233 = torch.prim.ListConstruct %int2_2599, %int-1_2600, %int20_2601, %int64_2602 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2234 = torch.aten.view %2218, %2233 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2603 = torch.constant.int 1
    %int2_2604 = torch.constant.int 2
    %2235 = torch.aten.transpose.int %2234, %int1_2603, %int2_2604 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2605 = torch.constant.int 2
    %int-1_2606 = torch.constant.int -1
    %int20_2607 = torch.constant.int 20
    %int64_2608 = torch.constant.int 64
    %2236 = torch.prim.ListConstruct %int2_2605, %int-1_2606, %int20_2607, %int64_2608 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2237 = torch.aten.view %2225, %2236 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2609 = torch.constant.int 1
    %int2_2610 = torch.constant.int 2
    %2238 = torch.aten.transpose.int %2237, %int1_2609, %int2_2610 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_2611 = torch.constant.int 2
    %int-1_2612 = torch.constant.int -1
    %int20_2613 = torch.constant.int 20
    %int64_2614 = torch.constant.int 64
    %2239 = torch.prim.ListConstruct %int2_2611, %int-1_2612, %int20_2613, %int64_2614 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2240 = torch.aten.view %2232, %2239 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2615 = torch.constant.int 1
    %int2_2616 = torch.constant.int 2
    %2241 = torch.aten.transpose.int %2240, %int1_2615, %int2_2616 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_2617 = torch.constant.float 0.000000e+00
    %false_2618 = torch.constant.bool false
    %none_2619 = torch.constant.none
    %none_2620 = torch.constant.none
    %2242:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2235, %2238, %2241, %float0.000000e00_2617, %false_2618, %none_2619, %none_2620) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2621 = torch.constant.int 1
    %int2_2622 = torch.constant.int 2
    %2243 = torch.aten.transpose.int %2242#0, %int1_2621, %int2_2622 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2623 = torch.constant.int 2
    %int-1_2624 = torch.constant.int -1
    %int1280_2625 = torch.constant.int 1280
    %2244 = torch.prim.ListConstruct %int2_2623, %int-1_2624, %int1280_2625 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2245 = torch.aten.view %2243, %2244 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2626 = torch.constant.int 5
    %2246 = torch.prims.convert_element_type %2245, %int5_2626 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2627 = torch.constant.int 1920
    %int1280_2628 = torch.constant.int 1280
    %2247 = torch.prim.ListConstruct %int1920_2627, %int1280_2628 : (!torch.int, !torch.int) -> !torch.list<int>
    %2248 = torch.aten.view %2246, %2247 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2249 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2629 = torch.constant.int 0
    %int1_2630 = torch.constant.int 1
    %2250 = torch.aten.transpose.int %2249, %int0_2629, %int1_2630 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %2251 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2631 = torch.constant.int 6
    %2252 = torch.prims.convert_element_type %2251, %int6_2631 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2632 = torch.constant.int 6
    %2253 = torch.prims.convert_element_type %2248, %int6_2632 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2633 = torch.constant.int 6
    %2254 = torch.prims.convert_element_type %2250, %int6_2633 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2255 = torch.aten.mm %2253, %2254 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2634 = torch.constant.int 1
    %2256 = torch.aten.mul.Scalar %2255, %int1_2634 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2635 = torch.constant.int 1
    %2257 = torch.aten.mul.Scalar %2252, %int1_2635 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2636 = torch.constant.int 1
    %2258 = torch.aten.add.Tensor %2256, %2257, %int1_2636 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2637 = torch.constant.int 5
    %2259 = torch.prims.convert_element_type %2258, %int5_2637 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2638 = torch.constant.int 2
    %int960_2639 = torch.constant.int 960
    %int1280_2640 = torch.constant.int 1280
    %2260 = torch.prim.ListConstruct %int2_2638, %int960_2639, %int1280_2640 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2261 = torch.aten.view %2259, %2260 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2641 = torch.constant.none
    %2262 = torch.aten.clone %2261, %none_2641 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2642 = torch.constant.float 1.000000e+00
    %2263 = torch.aten.div.Scalar %2262, %float1.000000e00_2642 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2643 = torch.constant.int 1
    %2264 = torch.aten.add.Tensor %2263, %2200, %int1_2643 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2644 = torch.constant.int 6
    %2265 = torch.prims.convert_element_type %2264, %int6_2644 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2645 = torch.constant.int 2
    %2266 = torch.prim.ListConstruct %int2_2645 : (!torch.int) -> !torch.list<int>
    %int0_2646 = torch.constant.int 0
    %true_2647 = torch.constant.bool true
    %result0_2648, %result1_2649 = torch.aten.var_mean.correction %2265, %2266, %int0_2646, %true_2647 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2650 = torch.constant.float 1.000000e-05
    %int1_2651 = torch.constant.int 1
    %2267 = torch.aten.add.Scalar %result0_2648, %float1.000000e-05_2650, %int1_2651 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2268 = torch.aten.rsqrt %2267 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2652 = torch.constant.int 1
    %2269 = torch.aten.sub.Tensor %2264, %result1_2649, %int1_2652 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2270 = torch.aten.mul.Tensor %2269, %2268 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %2271 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2272 = torch.aten.mul.Tensor %2270, %2271 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %2273 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2653 = torch.constant.int 1
    %2274 = torch.aten.add.Tensor %2272, %2273, %int1_2653 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2654 = torch.constant.int 5
    %2275 = torch.prims.convert_element_type %2274, %int5_2654 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2655 = torch.constant.int 1920
    %int1280_2656 = torch.constant.int 1280
    %2276 = torch.prim.ListConstruct %int1920_2655, %int1280_2656 : (!torch.int, !torch.int) -> !torch.list<int>
    %2277 = torch.aten.view %2275, %2276 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2278 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_2657 = torch.constant.int 0
    %int1_2658 = torch.constant.int 1
    %2279 = torch.aten.transpose.int %2278, %int0_2657, %int1_2658 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %2280 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_2659 = torch.constant.int 6
    %2281 = torch.prims.convert_element_type %2280, %int6_2659 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_2660 = torch.constant.int 6
    %2282 = torch.prims.convert_element_type %2277, %int6_2660 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2661 = torch.constant.int 6
    %2283 = torch.prims.convert_element_type %2279, %int6_2661 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2284 = torch.aten.mm %2282, %2283 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_2662 = torch.constant.int 1
    %2285 = torch.aten.mul.Scalar %2284, %int1_2662 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_2663 = torch.constant.int 1
    %2286 = torch.aten.mul.Scalar %2281, %int1_2663 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_2664 = torch.constant.int 1
    %2287 = torch.aten.add.Tensor %2285, %2286, %int1_2664 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_2665 = torch.constant.int 5
    %2288 = torch.prims.convert_element_type %2287, %int5_2665 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_2666 = torch.constant.int 2
    %int960_2667 = torch.constant.int 960
    %int10240_2668 = torch.constant.int 10240
    %2289 = torch.prim.ListConstruct %int2_2666, %int960_2667, %int10240_2668 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2290 = torch.aten.view %2288, %2289 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_2669 = torch.constant.int -1
    %int0_2670 = torch.constant.int 0
    %int5120_2671 = torch.constant.int 5120
    %int1_2672 = torch.constant.int 1
    %2291 = torch.aten.slice.Tensor %2290, %int-1_2669, %int0_2670, %int5120_2671, %int1_2672 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_2673 = torch.constant.int -1
    %int5120_2674 = torch.constant.int 5120
    %int10240_2675 = torch.constant.int 10240
    %int1_2676 = torch.constant.int 1
    %2292 = torch.aten.slice.Tensor %2290, %int-1_2673, %int5120_2674, %int10240_2675, %int1_2676 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_2677 = torch.constant.str "none"
    %2293 = torch.aten.gelu %2292, %str_2677 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %2294 = torch.aten.mul.Tensor %2291, %2293 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_2678 = torch.constant.none
    %2295 = torch.aten.clone %2294, %none_2678 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_2679 = torch.constant.int 1920
    %int5120_2680 = torch.constant.int 5120
    %2296 = torch.prim.ListConstruct %int1920_2679, %int5120_2680 : (!torch.int, !torch.int) -> !torch.list<int>
    %2297 = torch.aten.view %2295, %2296 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %2298 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2681 = torch.constant.int 0
    %int1_2682 = torch.constant.int 1
    %2299 = torch.aten.transpose.int %2298, %int0_2681, %int1_2682 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %2300 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2683 = torch.constant.int 6
    %2301 = torch.prims.convert_element_type %2300, %int6_2683 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2684 = torch.constant.int 6
    %2302 = torch.prims.convert_element_type %2297, %int6_2684 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_2685 = torch.constant.int 6
    %2303 = torch.prims.convert_element_type %2299, %int6_2685 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2304 = torch.aten.mm %2302, %2303 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2686 = torch.constant.int 1
    %2305 = torch.aten.mul.Scalar %2304, %int1_2686 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2687 = torch.constant.int 1
    %2306 = torch.aten.mul.Scalar %2301, %int1_2687 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2688 = torch.constant.int 1
    %2307 = torch.aten.add.Tensor %2305, %2306, %int1_2688 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2689 = torch.constant.int 5
    %2308 = torch.prims.convert_element_type %2307, %int5_2689 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2690 = torch.constant.int 2
    %int960_2691 = torch.constant.int 960
    %int1280_2692 = torch.constant.int 1280
    %2309 = torch.prim.ListConstruct %int2_2690, %int960_2691, %int1280_2692 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2310 = torch.aten.view %2308, %2309 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_2693 = torch.constant.int 1
    %2311 = torch.aten.add.Tensor %2310, %2264, %int1_2693 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2694 = torch.constant.int 6
    %2312 = torch.prims.convert_element_type %2311, %int6_2694 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2695 = torch.constant.int 2
    %2313 = torch.prim.ListConstruct %int2_2695 : (!torch.int) -> !torch.list<int>
    %int0_2696 = torch.constant.int 0
    %true_2697 = torch.constant.bool true
    %result0_2698, %result1_2699 = torch.aten.var_mean.correction %2312, %2313, %int0_2696, %true_2697 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2700 = torch.constant.float 1.000000e-05
    %int1_2701 = torch.constant.int 1
    %2314 = torch.aten.add.Scalar %result0_2698, %float1.000000e-05_2700, %int1_2701 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2315 = torch.aten.rsqrt %2314 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2702 = torch.constant.int 1
    %2316 = torch.aten.sub.Tensor %2311, %result1_2699, %int1_2702 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2317 = torch.aten.mul.Tensor %2316, %2315 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %2318 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2319 = torch.aten.mul.Tensor %2317, %2318 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %2320 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2703 = torch.constant.int 1
    %2321 = torch.aten.add.Tensor %2319, %2320, %int1_2703 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2704 = torch.constant.int 5
    %2322 = torch.prims.convert_element_type %2321, %int5_2704 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %2323 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2705 = torch.constant.int 0
    %int1_2706 = torch.constant.int 1
    %2324 = torch.aten.transpose.int %2323, %int0_2705, %int1_2706 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2707 = torch.constant.int 1920
    %int1280_2708 = torch.constant.int 1280
    %2325 = torch.prim.ListConstruct %int1920_2707, %int1280_2708 : (!torch.int, !torch.int) -> !torch.list<int>
    %2326 = torch.aten.view %2322, %2325 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2327 = torch.aten.mm %2326, %2324 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2709 = torch.constant.int 2
    %int960_2710 = torch.constant.int 960
    %int1280_2711 = torch.constant.int 1280
    %2328 = torch.prim.ListConstruct %int2_2709, %int960_2710, %int1280_2711 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2329 = torch.aten.view %2327, %2328 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %2330 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2712 = torch.constant.int 0
    %int1_2713 = torch.constant.int 1
    %2331 = torch.aten.transpose.int %2330, %int0_2712, %int1_2713 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2714 = torch.constant.int 1920
    %int1280_2715 = torch.constant.int 1280
    %2332 = torch.prim.ListConstruct %int1920_2714, %int1280_2715 : (!torch.int, !torch.int) -> !torch.list<int>
    %2333 = torch.aten.view %2322, %2332 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2334 = torch.aten.mm %2333, %2331 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2716 = torch.constant.int 2
    %int960_2717 = torch.constant.int 960
    %int1280_2718 = torch.constant.int 1280
    %2335 = torch.prim.ListConstruct %int2_2716, %int960_2717, %int1280_2718 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2336 = torch.aten.view %2334, %2335 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %2337 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2719 = torch.constant.int 0
    %int1_2720 = torch.constant.int 1
    %2338 = torch.aten.transpose.int %2337, %int0_2719, %int1_2720 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2721 = torch.constant.int 1920
    %int1280_2722 = torch.constant.int 1280
    %2339 = torch.prim.ListConstruct %int1920_2721, %int1280_2722 : (!torch.int, !torch.int) -> !torch.list<int>
    %2340 = torch.aten.view %2322, %2339 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2341 = torch.aten.mm %2340, %2338 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2723 = torch.constant.int 2
    %int960_2724 = torch.constant.int 960
    %int1280_2725 = torch.constant.int 1280
    %2342 = torch.prim.ListConstruct %int2_2723, %int960_2724, %int1280_2725 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2343 = torch.aten.view %2341, %2342 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_2726 = torch.constant.int 2
    %int-1_2727 = torch.constant.int -1
    %int20_2728 = torch.constant.int 20
    %int64_2729 = torch.constant.int 64
    %2344 = torch.prim.ListConstruct %int2_2726, %int-1_2727, %int20_2728, %int64_2729 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2345 = torch.aten.view %2329, %2344 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2730 = torch.constant.int 1
    %int2_2731 = torch.constant.int 2
    %2346 = torch.aten.transpose.int %2345, %int1_2730, %int2_2731 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2732 = torch.constant.int 2
    %int-1_2733 = torch.constant.int -1
    %int20_2734 = torch.constant.int 20
    %int64_2735 = torch.constant.int 64
    %2347 = torch.prim.ListConstruct %int2_2732, %int-1_2733, %int20_2734, %int64_2735 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2348 = torch.aten.view %2336, %2347 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2736 = torch.constant.int 1
    %int2_2737 = torch.constant.int 2
    %2349 = torch.aten.transpose.int %2348, %int1_2736, %int2_2737 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2738 = torch.constant.int 2
    %int-1_2739 = torch.constant.int -1
    %int20_2740 = torch.constant.int 20
    %int64_2741 = torch.constant.int 64
    %2350 = torch.prim.ListConstruct %int2_2738, %int-1_2739, %int20_2740, %int64_2741 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2351 = torch.aten.view %2343, %2350 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2742 = torch.constant.int 1
    %int2_2743 = torch.constant.int 2
    %2352 = torch.aten.transpose.int %2351, %int1_2742, %int2_2743 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_2744 = torch.constant.float 0.000000e+00
    %false_2745 = torch.constant.bool false
    %none_2746 = torch.constant.none
    %none_2747 = torch.constant.none
    %2353:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2346, %2349, %2352, %float0.000000e00_2744, %false_2745, %none_2746, %none_2747) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2748 = torch.constant.int 1
    %int2_2749 = torch.constant.int 2
    %2354 = torch.aten.transpose.int %2353#0, %int1_2748, %int2_2749 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2750 = torch.constant.int 2
    %int-1_2751 = torch.constant.int -1
    %int1280_2752 = torch.constant.int 1280
    %2355 = torch.prim.ListConstruct %int2_2750, %int-1_2751, %int1280_2752 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2356 = torch.aten.view %2354, %2355 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2753 = torch.constant.int 5
    %2357 = torch.prims.convert_element_type %2356, %int5_2753 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2754 = torch.constant.int 1920
    %int1280_2755 = torch.constant.int 1280
    %2358 = torch.prim.ListConstruct %int1920_2754, %int1280_2755 : (!torch.int, !torch.int) -> !torch.list<int>
    %2359 = torch.aten.view %2357, %2358 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2360 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2756 = torch.constant.int 0
    %int1_2757 = torch.constant.int 1
    %2361 = torch.aten.transpose.int %2360, %int0_2756, %int1_2757 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %2362 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2758 = torch.constant.int 6
    %2363 = torch.prims.convert_element_type %2362, %int6_2758 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2759 = torch.constant.int 6
    %2364 = torch.prims.convert_element_type %2359, %int6_2759 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2760 = torch.constant.int 6
    %2365 = torch.prims.convert_element_type %2361, %int6_2760 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2366 = torch.aten.mm %2364, %2365 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2761 = torch.constant.int 1
    %2367 = torch.aten.mul.Scalar %2366, %int1_2761 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2762 = torch.constant.int 1
    %2368 = torch.aten.mul.Scalar %2363, %int1_2762 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2763 = torch.constant.int 1
    %2369 = torch.aten.add.Tensor %2367, %2368, %int1_2763 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2764 = torch.constant.int 5
    %2370 = torch.prims.convert_element_type %2369, %int5_2764 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2765 = torch.constant.int 2
    %int960_2766 = torch.constant.int 960
    %int1280_2767 = torch.constant.int 1280
    %2371 = torch.prim.ListConstruct %int2_2765, %int960_2766, %int1280_2767 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2372 = torch.aten.view %2370, %2371 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2768 = torch.constant.none
    %2373 = torch.aten.clone %2372, %none_2768 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2769 = torch.constant.float 1.000000e+00
    %2374 = torch.aten.div.Scalar %2373, %float1.000000e00_2769 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2770 = torch.constant.int 1
    %2375 = torch.aten.add.Tensor %2374, %2311, %int1_2770 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2771 = torch.constant.int 6
    %2376 = torch.prims.convert_element_type %2375, %int6_2771 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2772 = torch.constant.int 2
    %2377 = torch.prim.ListConstruct %int2_2772 : (!torch.int) -> !torch.list<int>
    %int0_2773 = torch.constant.int 0
    %true_2774 = torch.constant.bool true
    %result0_2775, %result1_2776 = torch.aten.var_mean.correction %2376, %2377, %int0_2773, %true_2774 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2777 = torch.constant.float 1.000000e-05
    %int1_2778 = torch.constant.int 1
    %2378 = torch.aten.add.Scalar %result0_2775, %float1.000000e-05_2777, %int1_2778 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2379 = torch.aten.rsqrt %2378 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2779 = torch.constant.int 1
    %2380 = torch.aten.sub.Tensor %2375, %result1_2776, %int1_2779 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2381 = torch.aten.mul.Tensor %2380, %2379 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %2382 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2383 = torch.aten.mul.Tensor %2381, %2382 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %2384 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2780 = torch.constant.int 1
    %2385 = torch.aten.add.Tensor %2383, %2384, %int1_2780 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2781 = torch.constant.int 5
    %2386 = torch.prims.convert_element_type %2385, %int5_2781 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %2387 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2782 = torch.constant.int 0
    %int1_2783 = torch.constant.int 1
    %2388 = torch.aten.transpose.int %2387, %int0_2782, %int1_2783 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2784 = torch.constant.int 1920
    %int1280_2785 = torch.constant.int 1280
    %2389 = torch.prim.ListConstruct %int1920_2784, %int1280_2785 : (!torch.int, !torch.int) -> !torch.list<int>
    %2390 = torch.aten.view %2386, %2389 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2391 = torch.aten.mm %2390, %2388 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2786 = torch.constant.int 2
    %int960_2787 = torch.constant.int 960
    %int1280_2788 = torch.constant.int 1280
    %2392 = torch.prim.ListConstruct %int2_2786, %int960_2787, %int1280_2788 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2393 = torch.aten.view %2391, %2392 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %2394 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2789 = torch.constant.int 0
    %int1_2790 = torch.constant.int 1
    %2395 = torch.aten.transpose.int %2394, %int0_2789, %int1_2790 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2791 = torch.constant.int 32
    %int2048_2792 = torch.constant.int 2048
    %2396 = torch.prim.ListConstruct %int32_2791, %int2048_2792 : (!torch.int, !torch.int) -> !torch.list<int>
    %2397 = torch.aten.view %arg6, %2396 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2398 = torch.aten.mm %2397, %2395 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2793 = torch.constant.int 2
    %int16_2794 = torch.constant.int 16
    %int1280_2795 = torch.constant.int 1280
    %2399 = torch.prim.ListConstruct %int2_2793, %int16_2794, %int1280_2795 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2400 = torch.aten.view %2398, %2399 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %2401 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2796 = torch.constant.int 0
    %int1_2797 = torch.constant.int 1
    %2402 = torch.aten.transpose.int %2401, %int0_2796, %int1_2797 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2798 = torch.constant.int 32
    %int2048_2799 = torch.constant.int 2048
    %2403 = torch.prim.ListConstruct %int32_2798, %int2048_2799 : (!torch.int, !torch.int) -> !torch.list<int>
    %2404 = torch.aten.view %arg6, %2403 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2405 = torch.aten.mm %2404, %2402 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2800 = torch.constant.int 2
    %int16_2801 = torch.constant.int 16
    %int1280_2802 = torch.constant.int 1280
    %2406 = torch.prim.ListConstruct %int2_2800, %int16_2801, %int1280_2802 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2407 = torch.aten.view %2405, %2406 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_2803 = torch.constant.int 2
    %int-1_2804 = torch.constant.int -1
    %int20_2805 = torch.constant.int 20
    %int64_2806 = torch.constant.int 64
    %2408 = torch.prim.ListConstruct %int2_2803, %int-1_2804, %int20_2805, %int64_2806 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2409 = torch.aten.view %2393, %2408 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2807 = torch.constant.int 1
    %int2_2808 = torch.constant.int 2
    %2410 = torch.aten.transpose.int %2409, %int1_2807, %int2_2808 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2809 = torch.constant.int 2
    %int-1_2810 = torch.constant.int -1
    %int20_2811 = torch.constant.int 20
    %int64_2812 = torch.constant.int 64
    %2411 = torch.prim.ListConstruct %int2_2809, %int-1_2810, %int20_2811, %int64_2812 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2412 = torch.aten.view %2400, %2411 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2813 = torch.constant.int 1
    %int2_2814 = torch.constant.int 2
    %2413 = torch.aten.transpose.int %2412, %int1_2813, %int2_2814 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_2815 = torch.constant.int 2
    %int-1_2816 = torch.constant.int -1
    %int20_2817 = torch.constant.int 20
    %int64_2818 = torch.constant.int 64
    %2414 = torch.prim.ListConstruct %int2_2815, %int-1_2816, %int20_2817, %int64_2818 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2415 = torch.aten.view %2407, %2414 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_2819 = torch.constant.int 1
    %int2_2820 = torch.constant.int 2
    %2416 = torch.aten.transpose.int %2415, %int1_2819, %int2_2820 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_2821 = torch.constant.float 0.000000e+00
    %false_2822 = torch.constant.bool false
    %none_2823 = torch.constant.none
    %none_2824 = torch.constant.none
    %2417:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2410, %2413, %2416, %float0.000000e00_2821, %false_2822, %none_2823, %none_2824) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2825 = torch.constant.int 1
    %int2_2826 = torch.constant.int 2
    %2418 = torch.aten.transpose.int %2417#0, %int1_2825, %int2_2826 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2827 = torch.constant.int 2
    %int-1_2828 = torch.constant.int -1
    %int1280_2829 = torch.constant.int 1280
    %2419 = torch.prim.ListConstruct %int2_2827, %int-1_2828, %int1280_2829 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2420 = torch.aten.view %2418, %2419 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2830 = torch.constant.int 5
    %2421 = torch.prims.convert_element_type %2420, %int5_2830 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2831 = torch.constant.int 1920
    %int1280_2832 = torch.constant.int 1280
    %2422 = torch.prim.ListConstruct %int1920_2831, %int1280_2832 : (!torch.int, !torch.int) -> !torch.list<int>
    %2423 = torch.aten.view %2421, %2422 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2424 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2833 = torch.constant.int 0
    %int1_2834 = torch.constant.int 1
    %2425 = torch.aten.transpose.int %2424, %int0_2833, %int1_2834 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %2426 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2835 = torch.constant.int 6
    %2427 = torch.prims.convert_element_type %2426, %int6_2835 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2836 = torch.constant.int 6
    %2428 = torch.prims.convert_element_type %2423, %int6_2836 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2837 = torch.constant.int 6
    %2429 = torch.prims.convert_element_type %2425, %int6_2837 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2430 = torch.aten.mm %2428, %2429 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2838 = torch.constant.int 1
    %2431 = torch.aten.mul.Scalar %2430, %int1_2838 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2839 = torch.constant.int 1
    %2432 = torch.aten.mul.Scalar %2427, %int1_2839 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2840 = torch.constant.int 1
    %2433 = torch.aten.add.Tensor %2431, %2432, %int1_2840 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2841 = torch.constant.int 5
    %2434 = torch.prims.convert_element_type %2433, %int5_2841 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2842 = torch.constant.int 2
    %int960_2843 = torch.constant.int 960
    %int1280_2844 = torch.constant.int 1280
    %2435 = torch.prim.ListConstruct %int2_2842, %int960_2843, %int1280_2844 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2436 = torch.aten.view %2434, %2435 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2845 = torch.constant.none
    %2437 = torch.aten.clone %2436, %none_2845 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2846 = torch.constant.float 1.000000e+00
    %2438 = torch.aten.div.Scalar %2437, %float1.000000e00_2846 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2847 = torch.constant.int 1
    %2439 = torch.aten.add.Tensor %2438, %2375, %int1_2847 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2848 = torch.constant.int 6
    %2440 = torch.prims.convert_element_type %2439, %int6_2848 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2849 = torch.constant.int 2
    %2441 = torch.prim.ListConstruct %int2_2849 : (!torch.int) -> !torch.list<int>
    %int0_2850 = torch.constant.int 0
    %true_2851 = torch.constant.bool true
    %result0_2852, %result1_2853 = torch.aten.var_mean.correction %2440, %2441, %int0_2850, %true_2851 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2854 = torch.constant.float 1.000000e-05
    %int1_2855 = torch.constant.int 1
    %2442 = torch.aten.add.Scalar %result0_2852, %float1.000000e-05_2854, %int1_2855 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2443 = torch.aten.rsqrt %2442 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2856 = torch.constant.int 1
    %2444 = torch.aten.sub.Tensor %2439, %result1_2853, %int1_2856 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2445 = torch.aten.mul.Tensor %2444, %2443 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %2446 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2447 = torch.aten.mul.Tensor %2445, %2446 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %2448 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2857 = torch.constant.int 1
    %2449 = torch.aten.add.Tensor %2447, %2448, %int1_2857 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2858 = torch.constant.int 5
    %2450 = torch.prims.convert_element_type %2449, %int5_2858 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2859 = torch.constant.int 1920
    %int1280_2860 = torch.constant.int 1280
    %2451 = torch.prim.ListConstruct %int1920_2859, %int1280_2860 : (!torch.int, !torch.int) -> !torch.list<int>
    %2452 = torch.aten.view %2450, %2451 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2453 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_2861 = torch.constant.int 0
    %int1_2862 = torch.constant.int 1
    %2454 = torch.aten.transpose.int %2453, %int0_2861, %int1_2862 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %2455 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_2863 = torch.constant.int 6
    %2456 = torch.prims.convert_element_type %2455, %int6_2863 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_2864 = torch.constant.int 6
    %2457 = torch.prims.convert_element_type %2452, %int6_2864 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2865 = torch.constant.int 6
    %2458 = torch.prims.convert_element_type %2454, %int6_2865 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2459 = torch.aten.mm %2457, %2458 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_2866 = torch.constant.int 1
    %2460 = torch.aten.mul.Scalar %2459, %int1_2866 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_2867 = torch.constant.int 1
    %2461 = torch.aten.mul.Scalar %2456, %int1_2867 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_2868 = torch.constant.int 1
    %2462 = torch.aten.add.Tensor %2460, %2461, %int1_2868 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_2869 = torch.constant.int 5
    %2463 = torch.prims.convert_element_type %2462, %int5_2869 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_2870 = torch.constant.int 2
    %int960_2871 = torch.constant.int 960
    %int10240_2872 = torch.constant.int 10240
    %2464 = torch.prim.ListConstruct %int2_2870, %int960_2871, %int10240_2872 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2465 = torch.aten.view %2463, %2464 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_2873 = torch.constant.int -1
    %int0_2874 = torch.constant.int 0
    %int5120_2875 = torch.constant.int 5120
    %int1_2876 = torch.constant.int 1
    %2466 = torch.aten.slice.Tensor %2465, %int-1_2873, %int0_2874, %int5120_2875, %int1_2876 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_2877 = torch.constant.int -1
    %int5120_2878 = torch.constant.int 5120
    %int10240_2879 = torch.constant.int 10240
    %int1_2880 = torch.constant.int 1
    %2467 = torch.aten.slice.Tensor %2465, %int-1_2877, %int5120_2878, %int10240_2879, %int1_2880 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_2881 = torch.constant.str "none"
    %2468 = torch.aten.gelu %2467, %str_2881 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %2469 = torch.aten.mul.Tensor %2466, %2468 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_2882 = torch.constant.none
    %2470 = torch.aten.clone %2469, %none_2882 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_2883 = torch.constant.int 1920
    %int5120_2884 = torch.constant.int 5120
    %2471 = torch.prim.ListConstruct %int1920_2883, %int5120_2884 : (!torch.int, !torch.int) -> !torch.list<int>
    %2472 = torch.aten.view %2470, %2471 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %2473 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_2885 = torch.constant.int 0
    %int1_2886 = torch.constant.int 1
    %2474 = torch.aten.transpose.int %2473, %int0_2885, %int1_2886 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %2475 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2887 = torch.constant.int 6
    %2476 = torch.prims.convert_element_type %2475, %int6_2887 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2888 = torch.constant.int 6
    %2477 = torch.prims.convert_element_type %2472, %int6_2888 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_2889 = torch.constant.int 6
    %2478 = torch.prims.convert_element_type %2474, %int6_2889 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2479 = torch.aten.mm %2477, %2478 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2890 = torch.constant.int 1
    %2480 = torch.aten.mul.Scalar %2479, %int1_2890 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2891 = torch.constant.int 1
    %2481 = torch.aten.mul.Scalar %2476, %int1_2891 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2892 = torch.constant.int 1
    %2482 = torch.aten.add.Tensor %2480, %2481, %int1_2892 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2893 = torch.constant.int 5
    %2483 = torch.prims.convert_element_type %2482, %int5_2893 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2894 = torch.constant.int 2
    %int960_2895 = torch.constant.int 960
    %int1280_2896 = torch.constant.int 1280
    %2484 = torch.prim.ListConstruct %int2_2894, %int960_2895, %int1280_2896 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2485 = torch.aten.view %2483, %2484 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_2897 = torch.constant.int 1
    %2486 = torch.aten.add.Tensor %2485, %2439, %int1_2897 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2898 = torch.constant.int 6
    %2487 = torch.prims.convert_element_type %2486, %int6_2898 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2899 = torch.constant.int 2
    %2488 = torch.prim.ListConstruct %int2_2899 : (!torch.int) -> !torch.list<int>
    %int0_2900 = torch.constant.int 0
    %true_2901 = torch.constant.bool true
    %result0_2902, %result1_2903 = torch.aten.var_mean.correction %2487, %2488, %int0_2900, %true_2901 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2904 = torch.constant.float 1.000000e-05
    %int1_2905 = torch.constant.int 1
    %2489 = torch.aten.add.Scalar %result0_2902, %float1.000000e-05_2904, %int1_2905 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2490 = torch.aten.rsqrt %2489 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2906 = torch.constant.int 1
    %2491 = torch.aten.sub.Tensor %2486, %result1_2903, %int1_2906 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2492 = torch.aten.mul.Tensor %2491, %2490 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %2493 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2494 = torch.aten.mul.Tensor %2492, %2493 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %2495 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2907 = torch.constant.int 1
    %2496 = torch.aten.add.Tensor %2494, %2495, %int1_2907 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2908 = torch.constant.int 5
    %2497 = torch.prims.convert_element_type %2496, %int5_2908 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %2498 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2909 = torch.constant.int 0
    %int1_2910 = torch.constant.int 1
    %2499 = torch.aten.transpose.int %2498, %int0_2909, %int1_2910 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2911 = torch.constant.int 1920
    %int1280_2912 = torch.constant.int 1280
    %2500 = torch.prim.ListConstruct %int1920_2911, %int1280_2912 : (!torch.int, !torch.int) -> !torch.list<int>
    %2501 = torch.aten.view %2497, %2500 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2502 = torch.aten.mm %2501, %2499 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2913 = torch.constant.int 2
    %int960_2914 = torch.constant.int 960
    %int1280_2915 = torch.constant.int 1280
    %2503 = torch.prim.ListConstruct %int2_2913, %int960_2914, %int1280_2915 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2504 = torch.aten.view %2502, %2503 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %2505 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2916 = torch.constant.int 0
    %int1_2917 = torch.constant.int 1
    %2506 = torch.aten.transpose.int %2505, %int0_2916, %int1_2917 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2918 = torch.constant.int 1920
    %int1280_2919 = torch.constant.int 1280
    %2507 = torch.prim.ListConstruct %int1920_2918, %int1280_2919 : (!torch.int, !torch.int) -> !torch.list<int>
    %2508 = torch.aten.view %2497, %2507 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2509 = torch.aten.mm %2508, %2506 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2920 = torch.constant.int 2
    %int960_2921 = torch.constant.int 960
    %int1280_2922 = torch.constant.int 1280
    %2510 = torch.prim.ListConstruct %int2_2920, %int960_2921, %int1280_2922 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2511 = torch.aten.view %2509, %2510 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %2512 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2923 = torch.constant.int 0
    %int1_2924 = torch.constant.int 1
    %2513 = torch.aten.transpose.int %2512, %int0_2923, %int1_2924 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2925 = torch.constant.int 1920
    %int1280_2926 = torch.constant.int 1280
    %2514 = torch.prim.ListConstruct %int1920_2925, %int1280_2926 : (!torch.int, !torch.int) -> !torch.list<int>
    %2515 = torch.aten.view %2497, %2514 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2516 = torch.aten.mm %2515, %2513 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2927 = torch.constant.int 2
    %int960_2928 = torch.constant.int 960
    %int1280_2929 = torch.constant.int 1280
    %2517 = torch.prim.ListConstruct %int2_2927, %int960_2928, %int1280_2929 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2518 = torch.aten.view %2516, %2517 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_2930 = torch.constant.int 2
    %int-1_2931 = torch.constant.int -1
    %int20_2932 = torch.constant.int 20
    %int64_2933 = torch.constant.int 64
    %2519 = torch.prim.ListConstruct %int2_2930, %int-1_2931, %int20_2932, %int64_2933 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2520 = torch.aten.view %2504, %2519 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2934 = torch.constant.int 1
    %int2_2935 = torch.constant.int 2
    %2521 = torch.aten.transpose.int %2520, %int1_2934, %int2_2935 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2936 = torch.constant.int 2
    %int-1_2937 = torch.constant.int -1
    %int20_2938 = torch.constant.int 20
    %int64_2939 = torch.constant.int 64
    %2522 = torch.prim.ListConstruct %int2_2936, %int-1_2937, %int20_2938, %int64_2939 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2523 = torch.aten.view %2511, %2522 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2940 = torch.constant.int 1
    %int2_2941 = torch.constant.int 2
    %2524 = torch.aten.transpose.int %2523, %int1_2940, %int2_2941 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_2942 = torch.constant.int 2
    %int-1_2943 = torch.constant.int -1
    %int20_2944 = torch.constant.int 20
    %int64_2945 = torch.constant.int 64
    %2525 = torch.prim.ListConstruct %int2_2942, %int-1_2943, %int20_2944, %int64_2945 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2526 = torch.aten.view %2518, %2525 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_2946 = torch.constant.int 1
    %int2_2947 = torch.constant.int 2
    %2527 = torch.aten.transpose.int %2526, %int1_2946, %int2_2947 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_2948 = torch.constant.float 0.000000e+00
    %false_2949 = torch.constant.bool false
    %none_2950 = torch.constant.none
    %none_2951 = torch.constant.none
    %2528:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2521, %2524, %2527, %float0.000000e00_2948, %false_2949, %none_2950, %none_2951) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_2952 = torch.constant.int 1
    %int2_2953 = torch.constant.int 2
    %2529 = torch.aten.transpose.int %2528#0, %int1_2952, %int2_2953 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_2954 = torch.constant.int 2
    %int-1_2955 = torch.constant.int -1
    %int1280_2956 = torch.constant.int 1280
    %2530 = torch.prim.ListConstruct %int2_2954, %int-1_2955, %int1280_2956 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2531 = torch.aten.view %2529, %2530 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_2957 = torch.constant.int 5
    %2532 = torch.prims.convert_element_type %2531, %int5_2957 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_2958 = torch.constant.int 1920
    %int1280_2959 = torch.constant.int 1280
    %2533 = torch.prim.ListConstruct %int1920_2958, %int1280_2959 : (!torch.int, !torch.int) -> !torch.list<int>
    %2534 = torch.aten.view %2532, %2533 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2535 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2960 = torch.constant.int 0
    %int1_2961 = torch.constant.int 1
    %2536 = torch.aten.transpose.int %2535, %int0_2960, %int1_2961 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %2537 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_2962 = torch.constant.int 6
    %2538 = torch.prims.convert_element_type %2537, %int6_2962 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_2963 = torch.constant.int 6
    %2539 = torch.prims.convert_element_type %2534, %int6_2963 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_2964 = torch.constant.int 6
    %2540 = torch.prims.convert_element_type %2536, %int6_2964 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2541 = torch.aten.mm %2539, %2540 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_2965 = torch.constant.int 1
    %2542 = torch.aten.mul.Scalar %2541, %int1_2965 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_2966 = torch.constant.int 1
    %2543 = torch.aten.mul.Scalar %2538, %int1_2966 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_2967 = torch.constant.int 1
    %2544 = torch.aten.add.Tensor %2542, %2543, %int1_2967 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_2968 = torch.constant.int 5
    %2545 = torch.prims.convert_element_type %2544, %int5_2968 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_2969 = torch.constant.int 2
    %int960_2970 = torch.constant.int 960
    %int1280_2971 = torch.constant.int 1280
    %2546 = torch.prim.ListConstruct %int2_2969, %int960_2970, %int1280_2971 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2547 = torch.aten.view %2545, %2546 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_2972 = torch.constant.none
    %2548 = torch.aten.clone %2547, %none_2972 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_2973 = torch.constant.float 1.000000e+00
    %2549 = torch.aten.div.Scalar %2548, %float1.000000e00_2973 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_2974 = torch.constant.int 1
    %2550 = torch.aten.add.Tensor %2549, %2486, %int1_2974 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_2975 = torch.constant.int 6
    %2551 = torch.prims.convert_element_type %2550, %int6_2975 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_2976 = torch.constant.int 2
    %2552 = torch.prim.ListConstruct %int2_2976 : (!torch.int) -> !torch.list<int>
    %int0_2977 = torch.constant.int 0
    %true_2978 = torch.constant.bool true
    %result0_2979, %result1_2980 = torch.aten.var_mean.correction %2551, %2552, %int0_2977, %true_2978 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_2981 = torch.constant.float 1.000000e-05
    %int1_2982 = torch.constant.int 1
    %2553 = torch.aten.add.Scalar %result0_2979, %float1.000000e-05_2981, %int1_2982 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2554 = torch.aten.rsqrt %2553 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_2983 = torch.constant.int 1
    %2555 = torch.aten.sub.Tensor %2550, %result1_2980, %int1_2983 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2556 = torch.aten.mul.Tensor %2555, %2554 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %2557 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2558 = torch.aten.mul.Tensor %2556, %2557 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %2559 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_2984 = torch.constant.int 1
    %2560 = torch.aten.add.Tensor %2558, %2559, %int1_2984 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_2985 = torch.constant.int 5
    %2561 = torch.prims.convert_element_type %2560, %int5_2985 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %2562 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_2986 = torch.constant.int 0
    %int1_2987 = torch.constant.int 1
    %2563 = torch.aten.transpose.int %2562, %int0_2986, %int1_2987 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_2988 = torch.constant.int 1920
    %int1280_2989 = torch.constant.int 1280
    %2564 = torch.prim.ListConstruct %int1920_2988, %int1280_2989 : (!torch.int, !torch.int) -> !torch.list<int>
    %2565 = torch.aten.view %2561, %2564 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2566 = torch.aten.mm %2565, %2563 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_2990 = torch.constant.int 2
    %int960_2991 = torch.constant.int 960
    %int1280_2992 = torch.constant.int 1280
    %2567 = torch.prim.ListConstruct %int2_2990, %int960_2991, %int1280_2992 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2568 = torch.aten.view %2566, %2567 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %2569 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_2993 = torch.constant.int 0
    %int1_2994 = torch.constant.int 1
    %2570 = torch.aten.transpose.int %2569, %int0_2993, %int1_2994 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_2995 = torch.constant.int 32
    %int2048_2996 = torch.constant.int 2048
    %2571 = torch.prim.ListConstruct %int32_2995, %int2048_2996 : (!torch.int, !torch.int) -> !torch.list<int>
    %2572 = torch.aten.view %arg6, %2571 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2573 = torch.aten.mm %2572, %2570 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_2997 = torch.constant.int 2
    %int16_2998 = torch.constant.int 16
    %int1280_2999 = torch.constant.int 1280
    %2574 = torch.prim.ListConstruct %int2_2997, %int16_2998, %int1280_2999 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2575 = torch.aten.view %2573, %2574 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %2576 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3000 = torch.constant.int 0
    %int1_3001 = torch.constant.int 1
    %2577 = torch.aten.transpose.int %2576, %int0_3000, %int1_3001 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3002 = torch.constant.int 32
    %int2048_3003 = torch.constant.int 2048
    %2578 = torch.prim.ListConstruct %int32_3002, %int2048_3003 : (!torch.int, !torch.int) -> !torch.list<int>
    %2579 = torch.aten.view %arg6, %2578 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2580 = torch.aten.mm %2579, %2577 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3004 = torch.constant.int 2
    %int16_3005 = torch.constant.int 16
    %int1280_3006 = torch.constant.int 1280
    %2581 = torch.prim.ListConstruct %int2_3004, %int16_3005, %int1280_3006 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2582 = torch.aten.view %2580, %2581 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_3007 = torch.constant.int 2
    %int-1_3008 = torch.constant.int -1
    %int20_3009 = torch.constant.int 20
    %int64_3010 = torch.constant.int 64
    %2583 = torch.prim.ListConstruct %int2_3007, %int-1_3008, %int20_3009, %int64_3010 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2584 = torch.aten.view %2568, %2583 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3011 = torch.constant.int 1
    %int2_3012 = torch.constant.int 2
    %2585 = torch.aten.transpose.int %2584, %int1_3011, %int2_3012 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3013 = torch.constant.int 2
    %int-1_3014 = torch.constant.int -1
    %int20_3015 = torch.constant.int 20
    %int64_3016 = torch.constant.int 64
    %2586 = torch.prim.ListConstruct %int2_3013, %int-1_3014, %int20_3015, %int64_3016 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2587 = torch.aten.view %2575, %2586 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3017 = torch.constant.int 1
    %int2_3018 = torch.constant.int 2
    %2588 = torch.aten.transpose.int %2587, %int1_3017, %int2_3018 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_3019 = torch.constant.int 2
    %int-1_3020 = torch.constant.int -1
    %int20_3021 = torch.constant.int 20
    %int64_3022 = torch.constant.int 64
    %2589 = torch.prim.ListConstruct %int2_3019, %int-1_3020, %int20_3021, %int64_3022 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2590 = torch.aten.view %2582, %2589 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3023 = torch.constant.int 1
    %int2_3024 = torch.constant.int 2
    %2591 = torch.aten.transpose.int %2590, %int1_3023, %int2_3024 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_3025 = torch.constant.float 0.000000e+00
    %false_3026 = torch.constant.bool false
    %none_3027 = torch.constant.none
    %none_3028 = torch.constant.none
    %2592:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2585, %2588, %2591, %float0.000000e00_3025, %false_3026, %none_3027, %none_3028) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3029 = torch.constant.int 1
    %int2_3030 = torch.constant.int 2
    %2593 = torch.aten.transpose.int %2592#0, %int1_3029, %int2_3030 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3031 = torch.constant.int 2
    %int-1_3032 = torch.constant.int -1
    %int1280_3033 = torch.constant.int 1280
    %2594 = torch.prim.ListConstruct %int2_3031, %int-1_3032, %int1280_3033 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2595 = torch.aten.view %2593, %2594 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3034 = torch.constant.int 5
    %2596 = torch.prims.convert_element_type %2595, %int5_3034 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3035 = torch.constant.int 1920
    %int1280_3036 = torch.constant.int 1280
    %2597 = torch.prim.ListConstruct %int1920_3035, %int1280_3036 : (!torch.int, !torch.int) -> !torch.list<int>
    %2598 = torch.aten.view %2596, %2597 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2599 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3037 = torch.constant.int 0
    %int1_3038 = torch.constant.int 1
    %2600 = torch.aten.transpose.int %2599, %int0_3037, %int1_3038 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %2601 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3039 = torch.constant.int 6
    %2602 = torch.prims.convert_element_type %2601, %int6_3039 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3040 = torch.constant.int 6
    %2603 = torch.prims.convert_element_type %2598, %int6_3040 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3041 = torch.constant.int 6
    %2604 = torch.prims.convert_element_type %2600, %int6_3041 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2605 = torch.aten.mm %2603, %2604 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3042 = torch.constant.int 1
    %2606 = torch.aten.mul.Scalar %2605, %int1_3042 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3043 = torch.constant.int 1
    %2607 = torch.aten.mul.Scalar %2602, %int1_3043 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3044 = torch.constant.int 1
    %2608 = torch.aten.add.Tensor %2606, %2607, %int1_3044 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3045 = torch.constant.int 5
    %2609 = torch.prims.convert_element_type %2608, %int5_3045 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3046 = torch.constant.int 2
    %int960_3047 = torch.constant.int 960
    %int1280_3048 = torch.constant.int 1280
    %2610 = torch.prim.ListConstruct %int2_3046, %int960_3047, %int1280_3048 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2611 = torch.aten.view %2609, %2610 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3049 = torch.constant.none
    %2612 = torch.aten.clone %2611, %none_3049 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3050 = torch.constant.float 1.000000e+00
    %2613 = torch.aten.div.Scalar %2612, %float1.000000e00_3050 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3051 = torch.constant.int 1
    %2614 = torch.aten.add.Tensor %2613, %2550, %int1_3051 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3052 = torch.constant.int 6
    %2615 = torch.prims.convert_element_type %2614, %int6_3052 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3053 = torch.constant.int 2
    %2616 = torch.prim.ListConstruct %int2_3053 : (!torch.int) -> !torch.list<int>
    %int0_3054 = torch.constant.int 0
    %true_3055 = torch.constant.bool true
    %result0_3056, %result1_3057 = torch.aten.var_mean.correction %2615, %2616, %int0_3054, %true_3055 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3058 = torch.constant.float 1.000000e-05
    %int1_3059 = torch.constant.int 1
    %2617 = torch.aten.add.Scalar %result0_3056, %float1.000000e-05_3058, %int1_3059 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2618 = torch.aten.rsqrt %2617 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3060 = torch.constant.int 1
    %2619 = torch.aten.sub.Tensor %2614, %result1_3057, %int1_3060 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2620 = torch.aten.mul.Tensor %2619, %2618 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %2621 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2622 = torch.aten.mul.Tensor %2620, %2621 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %2623 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3061 = torch.constant.int 1
    %2624 = torch.aten.add.Tensor %2622, %2623, %int1_3061 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3062 = torch.constant.int 5
    %2625 = torch.prims.convert_element_type %2624, %int5_3062 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3063 = torch.constant.int 1920
    %int1280_3064 = torch.constant.int 1280
    %2626 = torch.prim.ListConstruct %int1920_3063, %int1280_3064 : (!torch.int, !torch.int) -> !torch.list<int>
    %2627 = torch.aten.view %2625, %2626 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2628 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_3065 = torch.constant.int 0
    %int1_3066 = torch.constant.int 1
    %2629 = torch.aten.transpose.int %2628, %int0_3065, %int1_3066 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %2630 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_3067 = torch.constant.int 6
    %2631 = torch.prims.convert_element_type %2630, %int6_3067 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_3068 = torch.constant.int 6
    %2632 = torch.prims.convert_element_type %2627, %int6_3068 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3069 = torch.constant.int 6
    %2633 = torch.prims.convert_element_type %2629, %int6_3069 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2634 = torch.aten.mm %2632, %2633 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_3070 = torch.constant.int 1
    %2635 = torch.aten.mul.Scalar %2634, %int1_3070 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_3071 = torch.constant.int 1
    %2636 = torch.aten.mul.Scalar %2631, %int1_3071 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_3072 = torch.constant.int 1
    %2637 = torch.aten.add.Tensor %2635, %2636, %int1_3072 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_3073 = torch.constant.int 5
    %2638 = torch.prims.convert_element_type %2637, %int5_3073 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_3074 = torch.constant.int 2
    %int960_3075 = torch.constant.int 960
    %int10240_3076 = torch.constant.int 10240
    %2639 = torch.prim.ListConstruct %int2_3074, %int960_3075, %int10240_3076 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2640 = torch.aten.view %2638, %2639 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_3077 = torch.constant.int -1
    %int0_3078 = torch.constant.int 0
    %int5120_3079 = torch.constant.int 5120
    %int1_3080 = torch.constant.int 1
    %2641 = torch.aten.slice.Tensor %2640, %int-1_3077, %int0_3078, %int5120_3079, %int1_3080 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_3081 = torch.constant.int -1
    %int5120_3082 = torch.constant.int 5120
    %int10240_3083 = torch.constant.int 10240
    %int1_3084 = torch.constant.int 1
    %2642 = torch.aten.slice.Tensor %2640, %int-1_3081, %int5120_3082, %int10240_3083, %int1_3084 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_3085 = torch.constant.str "none"
    %2643 = torch.aten.gelu %2642, %str_3085 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %2644 = torch.aten.mul.Tensor %2641, %2643 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_3086 = torch.constant.none
    %2645 = torch.aten.clone %2644, %none_3086 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_3087 = torch.constant.int 1920
    %int5120_3088 = torch.constant.int 5120
    %2646 = torch.prim.ListConstruct %int1920_3087, %int5120_3088 : (!torch.int, !torch.int) -> !torch.list<int>
    %2647 = torch.aten.view %2645, %2646 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %2648 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3089 = torch.constant.int 0
    %int1_3090 = torch.constant.int 1
    %2649 = torch.aten.transpose.int %2648, %int0_3089, %int1_3090 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %2650 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3091 = torch.constant.int 6
    %2651 = torch.prims.convert_element_type %2650, %int6_3091 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3092 = torch.constant.int 6
    %2652 = torch.prims.convert_element_type %2647, %int6_3092 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_3093 = torch.constant.int 6
    %2653 = torch.prims.convert_element_type %2649, %int6_3093 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2654 = torch.aten.mm %2652, %2653 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3094 = torch.constant.int 1
    %2655 = torch.aten.mul.Scalar %2654, %int1_3094 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3095 = torch.constant.int 1
    %2656 = torch.aten.mul.Scalar %2651, %int1_3095 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3096 = torch.constant.int 1
    %2657 = torch.aten.add.Tensor %2655, %2656, %int1_3096 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3097 = torch.constant.int 5
    %2658 = torch.prims.convert_element_type %2657, %int5_3097 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3098 = torch.constant.int 2
    %int960_3099 = torch.constant.int 960
    %int1280_3100 = torch.constant.int 1280
    %2659 = torch.prim.ListConstruct %int2_3098, %int960_3099, %int1280_3100 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2660 = torch.aten.view %2658, %2659 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_3101 = torch.constant.int 1
    %2661 = torch.aten.add.Tensor %2660, %2614, %int1_3101 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3102 = torch.constant.int 6
    %2662 = torch.prims.convert_element_type %2661, %int6_3102 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3103 = torch.constant.int 2
    %2663 = torch.prim.ListConstruct %int2_3103 : (!torch.int) -> !torch.list<int>
    %int0_3104 = torch.constant.int 0
    %true_3105 = torch.constant.bool true
    %result0_3106, %result1_3107 = torch.aten.var_mean.correction %2662, %2663, %int0_3104, %true_3105 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3108 = torch.constant.float 1.000000e-05
    %int1_3109 = torch.constant.int 1
    %2664 = torch.aten.add.Scalar %result0_3106, %float1.000000e-05_3108, %int1_3109 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2665 = torch.aten.rsqrt %2664 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3110 = torch.constant.int 1
    %2666 = torch.aten.sub.Tensor %2661, %result1_3107, %int1_3110 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2667 = torch.aten.mul.Tensor %2666, %2665 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %2668 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2669 = torch.aten.mul.Tensor %2667, %2668 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %2670 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3111 = torch.constant.int 1
    %2671 = torch.aten.add.Tensor %2669, %2670, %int1_3111 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3112 = torch.constant.int 5
    %2672 = torch.prims.convert_element_type %2671, %int5_3112 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %2673 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3113 = torch.constant.int 0
    %int1_3114 = torch.constant.int 1
    %2674 = torch.aten.transpose.int %2673, %int0_3113, %int1_3114 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3115 = torch.constant.int 1920
    %int1280_3116 = torch.constant.int 1280
    %2675 = torch.prim.ListConstruct %int1920_3115, %int1280_3116 : (!torch.int, !torch.int) -> !torch.list<int>
    %2676 = torch.aten.view %2672, %2675 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2677 = torch.aten.mm %2676, %2674 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3117 = torch.constant.int 2
    %int960_3118 = torch.constant.int 960
    %int1280_3119 = torch.constant.int 1280
    %2678 = torch.prim.ListConstruct %int2_3117, %int960_3118, %int1280_3119 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2679 = torch.aten.view %2677, %2678 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %2680 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3120 = torch.constant.int 0
    %int1_3121 = torch.constant.int 1
    %2681 = torch.aten.transpose.int %2680, %int0_3120, %int1_3121 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3122 = torch.constant.int 1920
    %int1280_3123 = torch.constant.int 1280
    %2682 = torch.prim.ListConstruct %int1920_3122, %int1280_3123 : (!torch.int, !torch.int) -> !torch.list<int>
    %2683 = torch.aten.view %2672, %2682 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2684 = torch.aten.mm %2683, %2681 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3124 = torch.constant.int 2
    %int960_3125 = torch.constant.int 960
    %int1280_3126 = torch.constant.int 1280
    %2685 = torch.prim.ListConstruct %int2_3124, %int960_3125, %int1280_3126 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2686 = torch.aten.view %2684, %2685 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %2687 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3127 = torch.constant.int 0
    %int1_3128 = torch.constant.int 1
    %2688 = torch.aten.transpose.int %2687, %int0_3127, %int1_3128 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3129 = torch.constant.int 1920
    %int1280_3130 = torch.constant.int 1280
    %2689 = torch.prim.ListConstruct %int1920_3129, %int1280_3130 : (!torch.int, !torch.int) -> !torch.list<int>
    %2690 = torch.aten.view %2672, %2689 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2691 = torch.aten.mm %2690, %2688 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3131 = torch.constant.int 2
    %int960_3132 = torch.constant.int 960
    %int1280_3133 = torch.constant.int 1280
    %2692 = torch.prim.ListConstruct %int2_3131, %int960_3132, %int1280_3133 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2693 = torch.aten.view %2691, %2692 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_3134 = torch.constant.int 2
    %int-1_3135 = torch.constant.int -1
    %int20_3136 = torch.constant.int 20
    %int64_3137 = torch.constant.int 64
    %2694 = torch.prim.ListConstruct %int2_3134, %int-1_3135, %int20_3136, %int64_3137 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2695 = torch.aten.view %2679, %2694 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3138 = torch.constant.int 1
    %int2_3139 = torch.constant.int 2
    %2696 = torch.aten.transpose.int %2695, %int1_3138, %int2_3139 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3140 = torch.constant.int 2
    %int-1_3141 = torch.constant.int -1
    %int20_3142 = torch.constant.int 20
    %int64_3143 = torch.constant.int 64
    %2697 = torch.prim.ListConstruct %int2_3140, %int-1_3141, %int20_3142, %int64_3143 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2698 = torch.aten.view %2686, %2697 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3144 = torch.constant.int 1
    %int2_3145 = torch.constant.int 2
    %2699 = torch.aten.transpose.int %2698, %int1_3144, %int2_3145 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3146 = torch.constant.int 2
    %int-1_3147 = torch.constant.int -1
    %int20_3148 = torch.constant.int 20
    %int64_3149 = torch.constant.int 64
    %2700 = torch.prim.ListConstruct %int2_3146, %int-1_3147, %int20_3148, %int64_3149 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2701 = torch.aten.view %2693, %2700 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3150 = torch.constant.int 1
    %int2_3151 = torch.constant.int 2
    %2702 = torch.aten.transpose.int %2701, %int1_3150, %int2_3151 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_3152 = torch.constant.float 0.000000e+00
    %false_3153 = torch.constant.bool false
    %none_3154 = torch.constant.none
    %none_3155 = torch.constant.none
    %2703:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2696, %2699, %2702, %float0.000000e00_3152, %false_3153, %none_3154, %none_3155) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3156 = torch.constant.int 1
    %int2_3157 = torch.constant.int 2
    %2704 = torch.aten.transpose.int %2703#0, %int1_3156, %int2_3157 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3158 = torch.constant.int 2
    %int-1_3159 = torch.constant.int -1
    %int1280_3160 = torch.constant.int 1280
    %2705 = torch.prim.ListConstruct %int2_3158, %int-1_3159, %int1280_3160 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2706 = torch.aten.view %2704, %2705 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3161 = torch.constant.int 5
    %2707 = torch.prims.convert_element_type %2706, %int5_3161 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3162 = torch.constant.int 1920
    %int1280_3163 = torch.constant.int 1280
    %2708 = torch.prim.ListConstruct %int1920_3162, %int1280_3163 : (!torch.int, !torch.int) -> !torch.list<int>
    %2709 = torch.aten.view %2707, %2708 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2710 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3164 = torch.constant.int 0
    %int1_3165 = torch.constant.int 1
    %2711 = torch.aten.transpose.int %2710, %int0_3164, %int1_3165 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %2712 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3166 = torch.constant.int 6
    %2713 = torch.prims.convert_element_type %2712, %int6_3166 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3167 = torch.constant.int 6
    %2714 = torch.prims.convert_element_type %2709, %int6_3167 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3168 = torch.constant.int 6
    %2715 = torch.prims.convert_element_type %2711, %int6_3168 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2716 = torch.aten.mm %2714, %2715 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3169 = torch.constant.int 1
    %2717 = torch.aten.mul.Scalar %2716, %int1_3169 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3170 = torch.constant.int 1
    %2718 = torch.aten.mul.Scalar %2713, %int1_3170 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3171 = torch.constant.int 1
    %2719 = torch.aten.add.Tensor %2717, %2718, %int1_3171 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3172 = torch.constant.int 5
    %2720 = torch.prims.convert_element_type %2719, %int5_3172 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3173 = torch.constant.int 2
    %int960_3174 = torch.constant.int 960
    %int1280_3175 = torch.constant.int 1280
    %2721 = torch.prim.ListConstruct %int2_3173, %int960_3174, %int1280_3175 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2722 = torch.aten.view %2720, %2721 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3176 = torch.constant.none
    %2723 = torch.aten.clone %2722, %none_3176 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3177 = torch.constant.float 1.000000e+00
    %2724 = torch.aten.div.Scalar %2723, %float1.000000e00_3177 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3178 = torch.constant.int 1
    %2725 = torch.aten.add.Tensor %2724, %2661, %int1_3178 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3179 = torch.constant.int 6
    %2726 = torch.prims.convert_element_type %2725, %int6_3179 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3180 = torch.constant.int 2
    %2727 = torch.prim.ListConstruct %int2_3180 : (!torch.int) -> !torch.list<int>
    %int0_3181 = torch.constant.int 0
    %true_3182 = torch.constant.bool true
    %result0_3183, %result1_3184 = torch.aten.var_mean.correction %2726, %2727, %int0_3181, %true_3182 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3185 = torch.constant.float 1.000000e-05
    %int1_3186 = torch.constant.int 1
    %2728 = torch.aten.add.Scalar %result0_3183, %float1.000000e-05_3185, %int1_3186 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2729 = torch.aten.rsqrt %2728 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3187 = torch.constant.int 1
    %2730 = torch.aten.sub.Tensor %2725, %result1_3184, %int1_3187 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2731 = torch.aten.mul.Tensor %2730, %2729 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %2732 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2733 = torch.aten.mul.Tensor %2731, %2732 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %2734 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3188 = torch.constant.int 1
    %2735 = torch.aten.add.Tensor %2733, %2734, %int1_3188 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3189 = torch.constant.int 5
    %2736 = torch.prims.convert_element_type %2735, %int5_3189 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %2737 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3190 = torch.constant.int 0
    %int1_3191 = torch.constant.int 1
    %2738 = torch.aten.transpose.int %2737, %int0_3190, %int1_3191 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3192 = torch.constant.int 1920
    %int1280_3193 = torch.constant.int 1280
    %2739 = torch.prim.ListConstruct %int1920_3192, %int1280_3193 : (!torch.int, !torch.int) -> !torch.list<int>
    %2740 = torch.aten.view %2736, %2739 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2741 = torch.aten.mm %2740, %2738 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3194 = torch.constant.int 2
    %int960_3195 = torch.constant.int 960
    %int1280_3196 = torch.constant.int 1280
    %2742 = torch.prim.ListConstruct %int2_3194, %int960_3195, %int1280_3196 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2743 = torch.aten.view %2741, %2742 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %2744 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3197 = torch.constant.int 0
    %int1_3198 = torch.constant.int 1
    %2745 = torch.aten.transpose.int %2744, %int0_3197, %int1_3198 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3199 = torch.constant.int 32
    %int2048_3200 = torch.constant.int 2048
    %2746 = torch.prim.ListConstruct %int32_3199, %int2048_3200 : (!torch.int, !torch.int) -> !torch.list<int>
    %2747 = torch.aten.view %arg6, %2746 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2748 = torch.aten.mm %2747, %2745 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3201 = torch.constant.int 2
    %int16_3202 = torch.constant.int 16
    %int1280_3203 = torch.constant.int 1280
    %2749 = torch.prim.ListConstruct %int2_3201, %int16_3202, %int1280_3203 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2750 = torch.aten.view %2748, %2749 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %2751 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3204 = torch.constant.int 0
    %int1_3205 = torch.constant.int 1
    %2752 = torch.aten.transpose.int %2751, %int0_3204, %int1_3205 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3206 = torch.constant.int 32
    %int2048_3207 = torch.constant.int 2048
    %2753 = torch.prim.ListConstruct %int32_3206, %int2048_3207 : (!torch.int, !torch.int) -> !torch.list<int>
    %2754 = torch.aten.view %arg6, %2753 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2755 = torch.aten.mm %2754, %2752 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3208 = torch.constant.int 2
    %int16_3209 = torch.constant.int 16
    %int1280_3210 = torch.constant.int 1280
    %2756 = torch.prim.ListConstruct %int2_3208, %int16_3209, %int1280_3210 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2757 = torch.aten.view %2755, %2756 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_3211 = torch.constant.int 2
    %int-1_3212 = torch.constant.int -1
    %int20_3213 = torch.constant.int 20
    %int64_3214 = torch.constant.int 64
    %2758 = torch.prim.ListConstruct %int2_3211, %int-1_3212, %int20_3213, %int64_3214 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2759 = torch.aten.view %2743, %2758 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3215 = torch.constant.int 1
    %int2_3216 = torch.constant.int 2
    %2760 = torch.aten.transpose.int %2759, %int1_3215, %int2_3216 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3217 = torch.constant.int 2
    %int-1_3218 = torch.constant.int -1
    %int20_3219 = torch.constant.int 20
    %int64_3220 = torch.constant.int 64
    %2761 = torch.prim.ListConstruct %int2_3217, %int-1_3218, %int20_3219, %int64_3220 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2762 = torch.aten.view %2750, %2761 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3221 = torch.constant.int 1
    %int2_3222 = torch.constant.int 2
    %2763 = torch.aten.transpose.int %2762, %int1_3221, %int2_3222 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_3223 = torch.constant.int 2
    %int-1_3224 = torch.constant.int -1
    %int20_3225 = torch.constant.int 20
    %int64_3226 = torch.constant.int 64
    %2764 = torch.prim.ListConstruct %int2_3223, %int-1_3224, %int20_3225, %int64_3226 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2765 = torch.aten.view %2757, %2764 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3227 = torch.constant.int 1
    %int2_3228 = torch.constant.int 2
    %2766 = torch.aten.transpose.int %2765, %int1_3227, %int2_3228 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_3229 = torch.constant.float 0.000000e+00
    %false_3230 = torch.constant.bool false
    %none_3231 = torch.constant.none
    %none_3232 = torch.constant.none
    %2767:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2760, %2763, %2766, %float0.000000e00_3229, %false_3230, %none_3231, %none_3232) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3233 = torch.constant.int 1
    %int2_3234 = torch.constant.int 2
    %2768 = torch.aten.transpose.int %2767#0, %int1_3233, %int2_3234 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3235 = torch.constant.int 2
    %int-1_3236 = torch.constant.int -1
    %int1280_3237 = torch.constant.int 1280
    %2769 = torch.prim.ListConstruct %int2_3235, %int-1_3236, %int1280_3237 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2770 = torch.aten.view %2768, %2769 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3238 = torch.constant.int 5
    %2771 = torch.prims.convert_element_type %2770, %int5_3238 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3239 = torch.constant.int 1920
    %int1280_3240 = torch.constant.int 1280
    %2772 = torch.prim.ListConstruct %int1920_3239, %int1280_3240 : (!torch.int, !torch.int) -> !torch.list<int>
    %2773 = torch.aten.view %2771, %2772 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2774 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3241 = torch.constant.int 0
    %int1_3242 = torch.constant.int 1
    %2775 = torch.aten.transpose.int %2774, %int0_3241, %int1_3242 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %2776 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3243 = torch.constant.int 6
    %2777 = torch.prims.convert_element_type %2776, %int6_3243 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3244 = torch.constant.int 6
    %2778 = torch.prims.convert_element_type %2773, %int6_3244 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3245 = torch.constant.int 6
    %2779 = torch.prims.convert_element_type %2775, %int6_3245 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2780 = torch.aten.mm %2778, %2779 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3246 = torch.constant.int 1
    %2781 = torch.aten.mul.Scalar %2780, %int1_3246 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3247 = torch.constant.int 1
    %2782 = torch.aten.mul.Scalar %2777, %int1_3247 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3248 = torch.constant.int 1
    %2783 = torch.aten.add.Tensor %2781, %2782, %int1_3248 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3249 = torch.constant.int 5
    %2784 = torch.prims.convert_element_type %2783, %int5_3249 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3250 = torch.constant.int 2
    %int960_3251 = torch.constant.int 960
    %int1280_3252 = torch.constant.int 1280
    %2785 = torch.prim.ListConstruct %int2_3250, %int960_3251, %int1280_3252 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2786 = torch.aten.view %2784, %2785 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3253 = torch.constant.none
    %2787 = torch.aten.clone %2786, %none_3253 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3254 = torch.constant.float 1.000000e+00
    %2788 = torch.aten.div.Scalar %2787, %float1.000000e00_3254 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3255 = torch.constant.int 1
    %2789 = torch.aten.add.Tensor %2788, %2725, %int1_3255 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3256 = torch.constant.int 6
    %2790 = torch.prims.convert_element_type %2789, %int6_3256 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3257 = torch.constant.int 2
    %2791 = torch.prim.ListConstruct %int2_3257 : (!torch.int) -> !torch.list<int>
    %int0_3258 = torch.constant.int 0
    %true_3259 = torch.constant.bool true
    %result0_3260, %result1_3261 = torch.aten.var_mean.correction %2790, %2791, %int0_3258, %true_3259 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3262 = torch.constant.float 1.000000e-05
    %int1_3263 = torch.constant.int 1
    %2792 = torch.aten.add.Scalar %result0_3260, %float1.000000e-05_3262, %int1_3263 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2793 = torch.aten.rsqrt %2792 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3264 = torch.constant.int 1
    %2794 = torch.aten.sub.Tensor %2789, %result1_3261, %int1_3264 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2795 = torch.aten.mul.Tensor %2794, %2793 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %2796 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2797 = torch.aten.mul.Tensor %2795, %2796 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %2798 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3265 = torch.constant.int 1
    %2799 = torch.aten.add.Tensor %2797, %2798, %int1_3265 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3266 = torch.constant.int 5
    %2800 = torch.prims.convert_element_type %2799, %int5_3266 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3267 = torch.constant.int 1920
    %int1280_3268 = torch.constant.int 1280
    %2801 = torch.prim.ListConstruct %int1920_3267, %int1280_3268 : (!torch.int, !torch.int) -> !torch.list<int>
    %2802 = torch.aten.view %2800, %2801 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2803 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_3269 = torch.constant.int 0
    %int1_3270 = torch.constant.int 1
    %2804 = torch.aten.transpose.int %2803, %int0_3269, %int1_3270 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %2805 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_3271 = torch.constant.int 6
    %2806 = torch.prims.convert_element_type %2805, %int6_3271 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_3272 = torch.constant.int 6
    %2807 = torch.prims.convert_element_type %2802, %int6_3272 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3273 = torch.constant.int 6
    %2808 = torch.prims.convert_element_type %2804, %int6_3273 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2809 = torch.aten.mm %2807, %2808 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_3274 = torch.constant.int 1
    %2810 = torch.aten.mul.Scalar %2809, %int1_3274 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_3275 = torch.constant.int 1
    %2811 = torch.aten.mul.Scalar %2806, %int1_3275 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_3276 = torch.constant.int 1
    %2812 = torch.aten.add.Tensor %2810, %2811, %int1_3276 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_3277 = torch.constant.int 5
    %2813 = torch.prims.convert_element_type %2812, %int5_3277 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_3278 = torch.constant.int 2
    %int960_3279 = torch.constant.int 960
    %int10240_3280 = torch.constant.int 10240
    %2814 = torch.prim.ListConstruct %int2_3278, %int960_3279, %int10240_3280 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2815 = torch.aten.view %2813, %2814 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_3281 = torch.constant.int -1
    %int0_3282 = torch.constant.int 0
    %int5120_3283 = torch.constant.int 5120
    %int1_3284 = torch.constant.int 1
    %2816 = torch.aten.slice.Tensor %2815, %int-1_3281, %int0_3282, %int5120_3283, %int1_3284 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_3285 = torch.constant.int -1
    %int5120_3286 = torch.constant.int 5120
    %int10240_3287 = torch.constant.int 10240
    %int1_3288 = torch.constant.int 1
    %2817 = torch.aten.slice.Tensor %2815, %int-1_3285, %int5120_3286, %int10240_3287, %int1_3288 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_3289 = torch.constant.str "none"
    %2818 = torch.aten.gelu %2817, %str_3289 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %2819 = torch.aten.mul.Tensor %2816, %2818 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_3290 = torch.constant.none
    %2820 = torch.aten.clone %2819, %none_3290 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_3291 = torch.constant.int 1920
    %int5120_3292 = torch.constant.int 5120
    %2821 = torch.prim.ListConstruct %int1920_3291, %int5120_3292 : (!torch.int, !torch.int) -> !torch.list<int>
    %2822 = torch.aten.view %2820, %2821 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %2823 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3293 = torch.constant.int 0
    %int1_3294 = torch.constant.int 1
    %2824 = torch.aten.transpose.int %2823, %int0_3293, %int1_3294 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %2825 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3295 = torch.constant.int 6
    %2826 = torch.prims.convert_element_type %2825, %int6_3295 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3296 = torch.constant.int 6
    %2827 = torch.prims.convert_element_type %2822, %int6_3296 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_3297 = torch.constant.int 6
    %2828 = torch.prims.convert_element_type %2824, %int6_3297 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %2829 = torch.aten.mm %2827, %2828 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3298 = torch.constant.int 1
    %2830 = torch.aten.mul.Scalar %2829, %int1_3298 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3299 = torch.constant.int 1
    %2831 = torch.aten.mul.Scalar %2826, %int1_3299 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3300 = torch.constant.int 1
    %2832 = torch.aten.add.Tensor %2830, %2831, %int1_3300 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3301 = torch.constant.int 5
    %2833 = torch.prims.convert_element_type %2832, %int5_3301 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3302 = torch.constant.int 2
    %int960_3303 = torch.constant.int 960
    %int1280_3304 = torch.constant.int 1280
    %2834 = torch.prim.ListConstruct %int2_3302, %int960_3303, %int1280_3304 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2835 = torch.aten.view %2833, %2834 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_3305 = torch.constant.int 1
    %2836 = torch.aten.add.Tensor %2835, %2789, %int1_3305 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3306 = torch.constant.int 6
    %2837 = torch.prims.convert_element_type %2836, %int6_3306 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3307 = torch.constant.int 2
    %2838 = torch.prim.ListConstruct %int2_3307 : (!torch.int) -> !torch.list<int>
    %int0_3308 = torch.constant.int 0
    %true_3309 = torch.constant.bool true
    %result0_3310, %result1_3311 = torch.aten.var_mean.correction %2837, %2838, %int0_3308, %true_3309 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3312 = torch.constant.float 1.000000e-05
    %int1_3313 = torch.constant.int 1
    %2839 = torch.aten.add.Scalar %result0_3310, %float1.000000e-05_3312, %int1_3313 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2840 = torch.aten.rsqrt %2839 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3314 = torch.constant.int 1
    %2841 = torch.aten.sub.Tensor %2836, %result1_3311, %int1_3314 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2842 = torch.aten.mul.Tensor %2841, %2840 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %2843 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2844 = torch.aten.mul.Tensor %2842, %2843 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %2845 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3315 = torch.constant.int 1
    %2846 = torch.aten.add.Tensor %2844, %2845, %int1_3315 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3316 = torch.constant.int 5
    %2847 = torch.prims.convert_element_type %2846, %int5_3316 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %2848 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3317 = torch.constant.int 0
    %int1_3318 = torch.constant.int 1
    %2849 = torch.aten.transpose.int %2848, %int0_3317, %int1_3318 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3319 = torch.constant.int 1920
    %int1280_3320 = torch.constant.int 1280
    %2850 = torch.prim.ListConstruct %int1920_3319, %int1280_3320 : (!torch.int, !torch.int) -> !torch.list<int>
    %2851 = torch.aten.view %2847, %2850 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2852 = torch.aten.mm %2851, %2849 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3321 = torch.constant.int 2
    %int960_3322 = torch.constant.int 960
    %int1280_3323 = torch.constant.int 1280
    %2853 = torch.prim.ListConstruct %int2_3321, %int960_3322, %int1280_3323 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2854 = torch.aten.view %2852, %2853 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %2855 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3324 = torch.constant.int 0
    %int1_3325 = torch.constant.int 1
    %2856 = torch.aten.transpose.int %2855, %int0_3324, %int1_3325 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3326 = torch.constant.int 1920
    %int1280_3327 = torch.constant.int 1280
    %2857 = torch.prim.ListConstruct %int1920_3326, %int1280_3327 : (!torch.int, !torch.int) -> !torch.list<int>
    %2858 = torch.aten.view %2847, %2857 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2859 = torch.aten.mm %2858, %2856 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3328 = torch.constant.int 2
    %int960_3329 = torch.constant.int 960
    %int1280_3330 = torch.constant.int 1280
    %2860 = torch.prim.ListConstruct %int2_3328, %int960_3329, %int1280_3330 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2861 = torch.aten.view %2859, %2860 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %2862 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3331 = torch.constant.int 0
    %int1_3332 = torch.constant.int 1
    %2863 = torch.aten.transpose.int %2862, %int0_3331, %int1_3332 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3333 = torch.constant.int 1920
    %int1280_3334 = torch.constant.int 1280
    %2864 = torch.prim.ListConstruct %int1920_3333, %int1280_3334 : (!torch.int, !torch.int) -> !torch.list<int>
    %2865 = torch.aten.view %2847, %2864 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2866 = torch.aten.mm %2865, %2863 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3335 = torch.constant.int 2
    %int960_3336 = torch.constant.int 960
    %int1280_3337 = torch.constant.int 1280
    %2867 = torch.prim.ListConstruct %int2_3335, %int960_3336, %int1280_3337 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2868 = torch.aten.view %2866, %2867 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_3338 = torch.constant.int 2
    %int-1_3339 = torch.constant.int -1
    %int20_3340 = torch.constant.int 20
    %int64_3341 = torch.constant.int 64
    %2869 = torch.prim.ListConstruct %int2_3338, %int-1_3339, %int20_3340, %int64_3341 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2870 = torch.aten.view %2854, %2869 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3342 = torch.constant.int 1
    %int2_3343 = torch.constant.int 2
    %2871 = torch.aten.transpose.int %2870, %int1_3342, %int2_3343 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3344 = torch.constant.int 2
    %int-1_3345 = torch.constant.int -1
    %int20_3346 = torch.constant.int 20
    %int64_3347 = torch.constant.int 64
    %2872 = torch.prim.ListConstruct %int2_3344, %int-1_3345, %int20_3346, %int64_3347 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2873 = torch.aten.view %2861, %2872 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3348 = torch.constant.int 1
    %int2_3349 = torch.constant.int 2
    %2874 = torch.aten.transpose.int %2873, %int1_3348, %int2_3349 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3350 = torch.constant.int 2
    %int-1_3351 = torch.constant.int -1
    %int20_3352 = torch.constant.int 20
    %int64_3353 = torch.constant.int 64
    %2875 = torch.prim.ListConstruct %int2_3350, %int-1_3351, %int20_3352, %int64_3353 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2876 = torch.aten.view %2868, %2875 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3354 = torch.constant.int 1
    %int2_3355 = torch.constant.int 2
    %2877 = torch.aten.transpose.int %2876, %int1_3354, %int2_3355 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_3356 = torch.constant.float 0.000000e+00
    %false_3357 = torch.constant.bool false
    %none_3358 = torch.constant.none
    %none_3359 = torch.constant.none
    %2878:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2871, %2874, %2877, %float0.000000e00_3356, %false_3357, %none_3358, %none_3359) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3360 = torch.constant.int 1
    %int2_3361 = torch.constant.int 2
    %2879 = torch.aten.transpose.int %2878#0, %int1_3360, %int2_3361 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3362 = torch.constant.int 2
    %int-1_3363 = torch.constant.int -1
    %int1280_3364 = torch.constant.int 1280
    %2880 = torch.prim.ListConstruct %int2_3362, %int-1_3363, %int1280_3364 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2881 = torch.aten.view %2879, %2880 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3365 = torch.constant.int 5
    %2882 = torch.prims.convert_element_type %2881, %int5_3365 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3366 = torch.constant.int 1920
    %int1280_3367 = torch.constant.int 1280
    %2883 = torch.prim.ListConstruct %int1920_3366, %int1280_3367 : (!torch.int, !torch.int) -> !torch.list<int>
    %2884 = torch.aten.view %2882, %2883 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %2885 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3368 = torch.constant.int 0
    %int1_3369 = torch.constant.int 1
    %2886 = torch.aten.transpose.int %2885, %int0_3368, %int1_3369 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %2887 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3370 = torch.constant.int 6
    %2888 = torch.prims.convert_element_type %2887, %int6_3370 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3371 = torch.constant.int 6
    %2889 = torch.prims.convert_element_type %2884, %int6_3371 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3372 = torch.constant.int 6
    %2890 = torch.prims.convert_element_type %2886, %int6_3372 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2891 = torch.aten.mm %2889, %2890 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3373 = torch.constant.int 1
    %2892 = torch.aten.mul.Scalar %2891, %int1_3373 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3374 = torch.constant.int 1
    %2893 = torch.aten.mul.Scalar %2888, %int1_3374 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3375 = torch.constant.int 1
    %2894 = torch.aten.add.Tensor %2892, %2893, %int1_3375 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3376 = torch.constant.int 5
    %2895 = torch.prims.convert_element_type %2894, %int5_3376 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3377 = torch.constant.int 2
    %int960_3378 = torch.constant.int 960
    %int1280_3379 = torch.constant.int 1280
    %2896 = torch.prim.ListConstruct %int2_3377, %int960_3378, %int1280_3379 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2897 = torch.aten.view %2895, %2896 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3380 = torch.constant.none
    %2898 = torch.aten.clone %2897, %none_3380 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3381 = torch.constant.float 1.000000e+00
    %2899 = torch.aten.div.Scalar %2898, %float1.000000e00_3381 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3382 = torch.constant.int 1
    %2900 = torch.aten.add.Tensor %2899, %2836, %int1_3382 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3383 = torch.constant.int 6
    %2901 = torch.prims.convert_element_type %2900, %int6_3383 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3384 = torch.constant.int 2
    %2902 = torch.prim.ListConstruct %int2_3384 : (!torch.int) -> !torch.list<int>
    %int0_3385 = torch.constant.int 0
    %true_3386 = torch.constant.bool true
    %result0_3387, %result1_3388 = torch.aten.var_mean.correction %2901, %2902, %int0_3385, %true_3386 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3389 = torch.constant.float 1.000000e-05
    %int1_3390 = torch.constant.int 1
    %2903 = torch.aten.add.Scalar %result0_3387, %float1.000000e-05_3389, %int1_3390 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2904 = torch.aten.rsqrt %2903 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3391 = torch.constant.int 1
    %2905 = torch.aten.sub.Tensor %2900, %result1_3388, %int1_3391 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2906 = torch.aten.mul.Tensor %2905, %2904 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %2907 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2908 = torch.aten.mul.Tensor %2906, %2907 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %2909 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3392 = torch.constant.int 1
    %2910 = torch.aten.add.Tensor %2908, %2909, %int1_3392 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3393 = torch.constant.int 5
    %2911 = torch.prims.convert_element_type %2910, %int5_3393 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %2912 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3394 = torch.constant.int 0
    %int1_3395 = torch.constant.int 1
    %2913 = torch.aten.transpose.int %2912, %int0_3394, %int1_3395 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3396 = torch.constant.int 1920
    %int1280_3397 = torch.constant.int 1280
    %2914 = torch.prim.ListConstruct %int1920_3396, %int1280_3397 : (!torch.int, !torch.int) -> !torch.list<int>
    %2915 = torch.aten.view %2911, %2914 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %2916 = torch.aten.mm %2915, %2913 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3398 = torch.constant.int 2
    %int960_3399 = torch.constant.int 960
    %int1280_3400 = torch.constant.int 1280
    %2917 = torch.prim.ListConstruct %int2_3398, %int960_3399, %int1280_3400 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2918 = torch.aten.view %2916, %2917 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %2919 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3401 = torch.constant.int 0
    %int1_3402 = torch.constant.int 1
    %2920 = torch.aten.transpose.int %2919, %int0_3401, %int1_3402 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3403 = torch.constant.int 32
    %int2048_3404 = torch.constant.int 2048
    %2921 = torch.prim.ListConstruct %int32_3403, %int2048_3404 : (!torch.int, !torch.int) -> !torch.list<int>
    %2922 = torch.aten.view %arg6, %2921 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2923 = torch.aten.mm %2922, %2920 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3405 = torch.constant.int 2
    %int16_3406 = torch.constant.int 16
    %int1280_3407 = torch.constant.int 1280
    %2924 = torch.prim.ListConstruct %int2_3405, %int16_3406, %int1280_3407 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2925 = torch.aten.view %2923, %2924 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %2926 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3408 = torch.constant.int 0
    %int1_3409 = torch.constant.int 1
    %2927 = torch.aten.transpose.int %2926, %int0_3408, %int1_3409 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3410 = torch.constant.int 32
    %int2048_3411 = torch.constant.int 2048
    %2928 = torch.prim.ListConstruct %int32_3410, %int2048_3411 : (!torch.int, !torch.int) -> !torch.list<int>
    %2929 = torch.aten.view %arg6, %2928 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %2930 = torch.aten.mm %2929, %2927 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3412 = torch.constant.int 2
    %int16_3413 = torch.constant.int 16
    %int1280_3414 = torch.constant.int 1280
    %2931 = torch.prim.ListConstruct %int2_3412, %int16_3413, %int1280_3414 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2932 = torch.aten.view %2930, %2931 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_3415 = torch.constant.int 2
    %int-1_3416 = torch.constant.int -1
    %int20_3417 = torch.constant.int 20
    %int64_3418 = torch.constant.int 64
    %2933 = torch.prim.ListConstruct %int2_3415, %int-1_3416, %int20_3417, %int64_3418 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2934 = torch.aten.view %2918, %2933 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3419 = torch.constant.int 1
    %int2_3420 = torch.constant.int 2
    %2935 = torch.aten.transpose.int %2934, %int1_3419, %int2_3420 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3421 = torch.constant.int 2
    %int-1_3422 = torch.constant.int -1
    %int20_3423 = torch.constant.int 20
    %int64_3424 = torch.constant.int 64
    %2936 = torch.prim.ListConstruct %int2_3421, %int-1_3422, %int20_3423, %int64_3424 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2937 = torch.aten.view %2925, %2936 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3425 = torch.constant.int 1
    %int2_3426 = torch.constant.int 2
    %2938 = torch.aten.transpose.int %2937, %int1_3425, %int2_3426 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_3427 = torch.constant.int 2
    %int-1_3428 = torch.constant.int -1
    %int20_3429 = torch.constant.int 20
    %int64_3430 = torch.constant.int 64
    %2939 = torch.prim.ListConstruct %int2_3427, %int-1_3428, %int20_3429, %int64_3430 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2940 = torch.aten.view %2932, %2939 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3431 = torch.constant.int 1
    %int2_3432 = torch.constant.int 2
    %2941 = torch.aten.transpose.int %2940, %int1_3431, %int2_3432 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_3433 = torch.constant.float 0.000000e+00
    %false_3434 = torch.constant.bool false
    %none_3435 = torch.constant.none
    %none_3436 = torch.constant.none
    %2942:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%2935, %2938, %2941, %float0.000000e00_3433, %false_3434, %none_3435, %none_3436) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3437 = torch.constant.int 1
    %int2_3438 = torch.constant.int 2
    %2943 = torch.aten.transpose.int %2942#0, %int1_3437, %int2_3438 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3439 = torch.constant.int 2
    %int-1_3440 = torch.constant.int -1
    %int1280_3441 = torch.constant.int 1280
    %2944 = torch.prim.ListConstruct %int2_3439, %int-1_3440, %int1280_3441 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2945 = torch.aten.view %2943, %2944 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3442 = torch.constant.int 5
    %2946 = torch.prims.convert_element_type %2945, %int5_3442 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3443 = torch.constant.int 1920
    %int1280_3444 = torch.constant.int 1280
    %2947 = torch.prim.ListConstruct %int1920_3443, %int1280_3444 : (!torch.int, !torch.int) -> !torch.list<int>
    %2948 = torch.aten.view %2946, %2947 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %2949 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3445 = torch.constant.int 0
    %int1_3446 = torch.constant.int 1
    %2950 = torch.aten.transpose.int %2949, %int0_3445, %int1_3446 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %2951 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3447 = torch.constant.int 6
    %2952 = torch.prims.convert_element_type %2951, %int6_3447 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3448 = torch.constant.int 6
    %2953 = torch.prims.convert_element_type %2948, %int6_3448 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3449 = torch.constant.int 6
    %2954 = torch.prims.convert_element_type %2950, %int6_3449 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %2955 = torch.aten.mm %2953, %2954 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3450 = torch.constant.int 1
    %2956 = torch.aten.mul.Scalar %2955, %int1_3450 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3451 = torch.constant.int 1
    %2957 = torch.aten.mul.Scalar %2952, %int1_3451 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3452 = torch.constant.int 1
    %2958 = torch.aten.add.Tensor %2956, %2957, %int1_3452 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3453 = torch.constant.int 5
    %2959 = torch.prims.convert_element_type %2958, %int5_3453 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3454 = torch.constant.int 2
    %int960_3455 = torch.constant.int 960
    %int1280_3456 = torch.constant.int 1280
    %2960 = torch.prim.ListConstruct %int2_3454, %int960_3455, %int1280_3456 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2961 = torch.aten.view %2959, %2960 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3457 = torch.constant.none
    %2962 = torch.aten.clone %2961, %none_3457 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3458 = torch.constant.float 1.000000e+00
    %2963 = torch.aten.div.Scalar %2962, %float1.000000e00_3458 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3459 = torch.constant.int 1
    %2964 = torch.aten.add.Tensor %2963, %2900, %int1_3459 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3460 = torch.constant.int 6
    %2965 = torch.prims.convert_element_type %2964, %int6_3460 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3461 = torch.constant.int 2
    %2966 = torch.prim.ListConstruct %int2_3461 : (!torch.int) -> !torch.list<int>
    %int0_3462 = torch.constant.int 0
    %true_3463 = torch.constant.bool true
    %result0_3464, %result1_3465 = torch.aten.var_mean.correction %2965, %2966, %int0_3462, %true_3463 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3466 = torch.constant.float 1.000000e-05
    %int1_3467 = torch.constant.int 1
    %2967 = torch.aten.add.Scalar %result0_3464, %float1.000000e-05_3466, %int1_3467 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %2968 = torch.aten.rsqrt %2967 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3468 = torch.constant.int 1
    %2969 = torch.aten.sub.Tensor %2964, %result1_3465, %int1_3468 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %2970 = torch.aten.mul.Tensor %2969, %2968 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %2971 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %2972 = torch.aten.mul.Tensor %2970, %2971 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %2973 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3469 = torch.constant.int 1
    %2974 = torch.aten.add.Tensor %2972, %2973, %int1_3469 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3470 = torch.constant.int 5
    %2975 = torch.prims.convert_element_type %2974, %int5_3470 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3471 = torch.constant.int 1920
    %int1280_3472 = torch.constant.int 1280
    %2976 = torch.prim.ListConstruct %int1920_3471, %int1280_3472 : (!torch.int, !torch.int) -> !torch.list<int>
    %2977 = torch.aten.view %2975, %2976 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %2978 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_3473 = torch.constant.int 0
    %int1_3474 = torch.constant.int 1
    %2979 = torch.aten.transpose.int %2978, %int0_3473, %int1_3474 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %2980 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_3475 = torch.constant.int 6
    %2981 = torch.prims.convert_element_type %2980, %int6_3475 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_3476 = torch.constant.int 6
    %2982 = torch.prims.convert_element_type %2977, %int6_3476 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3477 = torch.constant.int 6
    %2983 = torch.prims.convert_element_type %2979, %int6_3477 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %2984 = torch.aten.mm %2982, %2983 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_3478 = torch.constant.int 1
    %2985 = torch.aten.mul.Scalar %2984, %int1_3478 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_3479 = torch.constant.int 1
    %2986 = torch.aten.mul.Scalar %2981, %int1_3479 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_3480 = torch.constant.int 1
    %2987 = torch.aten.add.Tensor %2985, %2986, %int1_3480 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_3481 = torch.constant.int 5
    %2988 = torch.prims.convert_element_type %2987, %int5_3481 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_3482 = torch.constant.int 2
    %int960_3483 = torch.constant.int 960
    %int10240_3484 = torch.constant.int 10240
    %2989 = torch.prim.ListConstruct %int2_3482, %int960_3483, %int10240_3484 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %2990 = torch.aten.view %2988, %2989 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_3485 = torch.constant.int -1
    %int0_3486 = torch.constant.int 0
    %int5120_3487 = torch.constant.int 5120
    %int1_3488 = torch.constant.int 1
    %2991 = torch.aten.slice.Tensor %2990, %int-1_3485, %int0_3486, %int5120_3487, %int1_3488 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_3489 = torch.constant.int -1
    %int5120_3490 = torch.constant.int 5120
    %int10240_3491 = torch.constant.int 10240
    %int1_3492 = torch.constant.int 1
    %2992 = torch.aten.slice.Tensor %2990, %int-1_3489, %int5120_3490, %int10240_3491, %int1_3492 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_3493 = torch.constant.str "none"
    %2993 = torch.aten.gelu %2992, %str_3493 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %2994 = torch.aten.mul.Tensor %2991, %2993 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_3494 = torch.constant.none
    %2995 = torch.aten.clone %2994, %none_3494 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_3495 = torch.constant.int 1920
    %int5120_3496 = torch.constant.int 5120
    %2996 = torch.prim.ListConstruct %int1920_3495, %int5120_3496 : (!torch.int, !torch.int) -> !torch.list<int>
    %2997 = torch.aten.view %2995, %2996 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %2998 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3497 = torch.constant.int 0
    %int1_3498 = torch.constant.int 1
    %2999 = torch.aten.transpose.int %2998, %int0_3497, %int1_3498 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %3000 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3499 = torch.constant.int 6
    %3001 = torch.prims.convert_element_type %3000, %int6_3499 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3500 = torch.constant.int 6
    %3002 = torch.prims.convert_element_type %2997, %int6_3500 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_3501 = torch.constant.int 6
    %3003 = torch.prims.convert_element_type %2999, %int6_3501 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3004 = torch.aten.mm %3002, %3003 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3502 = torch.constant.int 1
    %3005 = torch.aten.mul.Scalar %3004, %int1_3502 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3503 = torch.constant.int 1
    %3006 = torch.aten.mul.Scalar %3001, %int1_3503 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3504 = torch.constant.int 1
    %3007 = torch.aten.add.Tensor %3005, %3006, %int1_3504 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3505 = torch.constant.int 5
    %3008 = torch.prims.convert_element_type %3007, %int5_3505 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3506 = torch.constant.int 2
    %int960_3507 = torch.constant.int 960
    %int1280_3508 = torch.constant.int 1280
    %3009 = torch.prim.ListConstruct %int2_3506, %int960_3507, %int1280_3508 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3010 = torch.aten.view %3008, %3009 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_3509 = torch.constant.int 1
    %3011 = torch.aten.add.Tensor %3010, %2964, %int1_3509 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3510 = torch.constant.int 6
    %3012 = torch.prims.convert_element_type %3011, %int6_3510 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3511 = torch.constant.int 2
    %3013 = torch.prim.ListConstruct %int2_3511 : (!torch.int) -> !torch.list<int>
    %int0_3512 = torch.constant.int 0
    %true_3513 = torch.constant.bool true
    %result0_3514, %result1_3515 = torch.aten.var_mean.correction %3012, %3013, %int0_3512, %true_3513 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3516 = torch.constant.float 1.000000e-05
    %int1_3517 = torch.constant.int 1
    %3014 = torch.aten.add.Scalar %result0_3514, %float1.000000e-05_3516, %int1_3517 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3015 = torch.aten.rsqrt %3014 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3518 = torch.constant.int 1
    %3016 = torch.aten.sub.Tensor %3011, %result1_3515, %int1_3518 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3017 = torch.aten.mul.Tensor %3016, %3015 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %3018 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3019 = torch.aten.mul.Tensor %3017, %3018 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %3020 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3519 = torch.constant.int 1
    %3021 = torch.aten.add.Tensor %3019, %3020, %int1_3519 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3520 = torch.constant.int 5
    %3022 = torch.prims.convert_element_type %3021, %int5_3520 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %3023 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3521 = torch.constant.int 0
    %int1_3522 = torch.constant.int 1
    %3024 = torch.aten.transpose.int %3023, %int0_3521, %int1_3522 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3523 = torch.constant.int 1920
    %int1280_3524 = torch.constant.int 1280
    %3025 = torch.prim.ListConstruct %int1920_3523, %int1280_3524 : (!torch.int, !torch.int) -> !torch.list<int>
    %3026 = torch.aten.view %3022, %3025 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3027 = torch.aten.mm %3026, %3024 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3525 = torch.constant.int 2
    %int960_3526 = torch.constant.int 960
    %int1280_3527 = torch.constant.int 1280
    %3028 = torch.prim.ListConstruct %int2_3525, %int960_3526, %int1280_3527 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3029 = torch.aten.view %3027, %3028 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %3030 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3528 = torch.constant.int 0
    %int1_3529 = torch.constant.int 1
    %3031 = torch.aten.transpose.int %3030, %int0_3528, %int1_3529 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3530 = torch.constant.int 1920
    %int1280_3531 = torch.constant.int 1280
    %3032 = torch.prim.ListConstruct %int1920_3530, %int1280_3531 : (!torch.int, !torch.int) -> !torch.list<int>
    %3033 = torch.aten.view %3022, %3032 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3034 = torch.aten.mm %3033, %3031 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3532 = torch.constant.int 2
    %int960_3533 = torch.constant.int 960
    %int1280_3534 = torch.constant.int 1280
    %3035 = torch.prim.ListConstruct %int2_3532, %int960_3533, %int1280_3534 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3036 = torch.aten.view %3034, %3035 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %3037 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3535 = torch.constant.int 0
    %int1_3536 = torch.constant.int 1
    %3038 = torch.aten.transpose.int %3037, %int0_3535, %int1_3536 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3537 = torch.constant.int 1920
    %int1280_3538 = torch.constant.int 1280
    %3039 = torch.prim.ListConstruct %int1920_3537, %int1280_3538 : (!torch.int, !torch.int) -> !torch.list<int>
    %3040 = torch.aten.view %3022, %3039 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3041 = torch.aten.mm %3040, %3038 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3539 = torch.constant.int 2
    %int960_3540 = torch.constant.int 960
    %int1280_3541 = torch.constant.int 1280
    %3042 = torch.prim.ListConstruct %int2_3539, %int960_3540, %int1280_3541 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3043 = torch.aten.view %3041, %3042 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_3542 = torch.constant.int 2
    %int-1_3543 = torch.constant.int -1
    %int20_3544 = torch.constant.int 20
    %int64_3545 = torch.constant.int 64
    %3044 = torch.prim.ListConstruct %int2_3542, %int-1_3543, %int20_3544, %int64_3545 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3045 = torch.aten.view %3029, %3044 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3546 = torch.constant.int 1
    %int2_3547 = torch.constant.int 2
    %3046 = torch.aten.transpose.int %3045, %int1_3546, %int2_3547 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3548 = torch.constant.int 2
    %int-1_3549 = torch.constant.int -1
    %int20_3550 = torch.constant.int 20
    %int64_3551 = torch.constant.int 64
    %3047 = torch.prim.ListConstruct %int2_3548, %int-1_3549, %int20_3550, %int64_3551 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3048 = torch.aten.view %3036, %3047 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3552 = torch.constant.int 1
    %int2_3553 = torch.constant.int 2
    %3049 = torch.aten.transpose.int %3048, %int1_3552, %int2_3553 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3554 = torch.constant.int 2
    %int-1_3555 = torch.constant.int -1
    %int20_3556 = torch.constant.int 20
    %int64_3557 = torch.constant.int 64
    %3050 = torch.prim.ListConstruct %int2_3554, %int-1_3555, %int20_3556, %int64_3557 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3051 = torch.aten.view %3043, %3050 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3558 = torch.constant.int 1
    %int2_3559 = torch.constant.int 2
    %3052 = torch.aten.transpose.int %3051, %int1_3558, %int2_3559 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_3560 = torch.constant.float 0.000000e+00
    %false_3561 = torch.constant.bool false
    %none_3562 = torch.constant.none
    %none_3563 = torch.constant.none
    %3053:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3046, %3049, %3052, %float0.000000e00_3560, %false_3561, %none_3562, %none_3563) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3564 = torch.constant.int 1
    %int2_3565 = torch.constant.int 2
    %3054 = torch.aten.transpose.int %3053#0, %int1_3564, %int2_3565 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3566 = torch.constant.int 2
    %int-1_3567 = torch.constant.int -1
    %int1280_3568 = torch.constant.int 1280
    %3055 = torch.prim.ListConstruct %int2_3566, %int-1_3567, %int1280_3568 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3056 = torch.aten.view %3054, %3055 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3569 = torch.constant.int 5
    %3057 = torch.prims.convert_element_type %3056, %int5_3569 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3570 = torch.constant.int 1920
    %int1280_3571 = torch.constant.int 1280
    %3058 = torch.prim.ListConstruct %int1920_3570, %int1280_3571 : (!torch.int, !torch.int) -> !torch.list<int>
    %3059 = torch.aten.view %3057, %3058 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3060 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3572 = torch.constant.int 0
    %int1_3573 = torch.constant.int 1
    %3061 = torch.aten.transpose.int %3060, %int0_3572, %int1_3573 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %3062 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3574 = torch.constant.int 6
    %3063 = torch.prims.convert_element_type %3062, %int6_3574 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3575 = torch.constant.int 6
    %3064 = torch.prims.convert_element_type %3059, %int6_3575 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3576 = torch.constant.int 6
    %3065 = torch.prims.convert_element_type %3061, %int6_3576 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3066 = torch.aten.mm %3064, %3065 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3577 = torch.constant.int 1
    %3067 = torch.aten.mul.Scalar %3066, %int1_3577 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3578 = torch.constant.int 1
    %3068 = torch.aten.mul.Scalar %3063, %int1_3578 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3579 = torch.constant.int 1
    %3069 = torch.aten.add.Tensor %3067, %3068, %int1_3579 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3580 = torch.constant.int 5
    %3070 = torch.prims.convert_element_type %3069, %int5_3580 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3581 = torch.constant.int 2
    %int960_3582 = torch.constant.int 960
    %int1280_3583 = torch.constant.int 1280
    %3071 = torch.prim.ListConstruct %int2_3581, %int960_3582, %int1280_3583 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3072 = torch.aten.view %3070, %3071 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3584 = torch.constant.none
    %3073 = torch.aten.clone %3072, %none_3584 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3585 = torch.constant.float 1.000000e+00
    %3074 = torch.aten.div.Scalar %3073, %float1.000000e00_3585 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3586 = torch.constant.int 1
    %3075 = torch.aten.add.Tensor %3074, %3011, %int1_3586 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3587 = torch.constant.int 6
    %3076 = torch.prims.convert_element_type %3075, %int6_3587 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3588 = torch.constant.int 2
    %3077 = torch.prim.ListConstruct %int2_3588 : (!torch.int) -> !torch.list<int>
    %int0_3589 = torch.constant.int 0
    %true_3590 = torch.constant.bool true
    %result0_3591, %result1_3592 = torch.aten.var_mean.correction %3076, %3077, %int0_3589, %true_3590 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3593 = torch.constant.float 1.000000e-05
    %int1_3594 = torch.constant.int 1
    %3078 = torch.aten.add.Scalar %result0_3591, %float1.000000e-05_3593, %int1_3594 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3079 = torch.aten.rsqrt %3078 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3595 = torch.constant.int 1
    %3080 = torch.aten.sub.Tensor %3075, %result1_3592, %int1_3595 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3081 = torch.aten.mul.Tensor %3080, %3079 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %3082 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3083 = torch.aten.mul.Tensor %3081, %3082 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %3084 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3596 = torch.constant.int 1
    %3085 = torch.aten.add.Tensor %3083, %3084, %int1_3596 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3597 = torch.constant.int 5
    %3086 = torch.prims.convert_element_type %3085, %int5_3597 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %3087 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3598 = torch.constant.int 0
    %int1_3599 = torch.constant.int 1
    %3088 = torch.aten.transpose.int %3087, %int0_3598, %int1_3599 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3600 = torch.constant.int 1920
    %int1280_3601 = torch.constant.int 1280
    %3089 = torch.prim.ListConstruct %int1920_3600, %int1280_3601 : (!torch.int, !torch.int) -> !torch.list<int>
    %3090 = torch.aten.view %3086, %3089 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3091 = torch.aten.mm %3090, %3088 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3602 = torch.constant.int 2
    %int960_3603 = torch.constant.int 960
    %int1280_3604 = torch.constant.int 1280
    %3092 = torch.prim.ListConstruct %int2_3602, %int960_3603, %int1280_3604 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3093 = torch.aten.view %3091, %3092 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %3094 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3605 = torch.constant.int 0
    %int1_3606 = torch.constant.int 1
    %3095 = torch.aten.transpose.int %3094, %int0_3605, %int1_3606 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3607 = torch.constant.int 32
    %int2048_3608 = torch.constant.int 2048
    %3096 = torch.prim.ListConstruct %int32_3607, %int2048_3608 : (!torch.int, !torch.int) -> !torch.list<int>
    %3097 = torch.aten.view %arg6, %3096 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3098 = torch.aten.mm %3097, %3095 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3609 = torch.constant.int 2
    %int16_3610 = torch.constant.int 16
    %int1280_3611 = torch.constant.int 1280
    %3099 = torch.prim.ListConstruct %int2_3609, %int16_3610, %int1280_3611 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3100 = torch.aten.view %3098, %3099 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %3101 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3612 = torch.constant.int 0
    %int1_3613 = torch.constant.int 1
    %3102 = torch.aten.transpose.int %3101, %int0_3612, %int1_3613 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3614 = torch.constant.int 32
    %int2048_3615 = torch.constant.int 2048
    %3103 = torch.prim.ListConstruct %int32_3614, %int2048_3615 : (!torch.int, !torch.int) -> !torch.list<int>
    %3104 = torch.aten.view %arg6, %3103 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3105 = torch.aten.mm %3104, %3102 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3616 = torch.constant.int 2
    %int16_3617 = torch.constant.int 16
    %int1280_3618 = torch.constant.int 1280
    %3106 = torch.prim.ListConstruct %int2_3616, %int16_3617, %int1280_3618 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3107 = torch.aten.view %3105, %3106 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_3619 = torch.constant.int 2
    %int-1_3620 = torch.constant.int -1
    %int20_3621 = torch.constant.int 20
    %int64_3622 = torch.constant.int 64
    %3108 = torch.prim.ListConstruct %int2_3619, %int-1_3620, %int20_3621, %int64_3622 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3109 = torch.aten.view %3093, %3108 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3623 = torch.constant.int 1
    %int2_3624 = torch.constant.int 2
    %3110 = torch.aten.transpose.int %3109, %int1_3623, %int2_3624 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3625 = torch.constant.int 2
    %int-1_3626 = torch.constant.int -1
    %int20_3627 = torch.constant.int 20
    %int64_3628 = torch.constant.int 64
    %3111 = torch.prim.ListConstruct %int2_3625, %int-1_3626, %int20_3627, %int64_3628 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3112 = torch.aten.view %3100, %3111 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3629 = torch.constant.int 1
    %int2_3630 = torch.constant.int 2
    %3113 = torch.aten.transpose.int %3112, %int1_3629, %int2_3630 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_3631 = torch.constant.int 2
    %int-1_3632 = torch.constant.int -1
    %int20_3633 = torch.constant.int 20
    %int64_3634 = torch.constant.int 64
    %3114 = torch.prim.ListConstruct %int2_3631, %int-1_3632, %int20_3633, %int64_3634 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3115 = torch.aten.view %3107, %3114 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3635 = torch.constant.int 1
    %int2_3636 = torch.constant.int 2
    %3116 = torch.aten.transpose.int %3115, %int1_3635, %int2_3636 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_3637 = torch.constant.float 0.000000e+00
    %false_3638 = torch.constant.bool false
    %none_3639 = torch.constant.none
    %none_3640 = torch.constant.none
    %3117:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3110, %3113, %3116, %float0.000000e00_3637, %false_3638, %none_3639, %none_3640) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3641 = torch.constant.int 1
    %int2_3642 = torch.constant.int 2
    %3118 = torch.aten.transpose.int %3117#0, %int1_3641, %int2_3642 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3643 = torch.constant.int 2
    %int-1_3644 = torch.constant.int -1
    %int1280_3645 = torch.constant.int 1280
    %3119 = torch.prim.ListConstruct %int2_3643, %int-1_3644, %int1280_3645 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3120 = torch.aten.view %3118, %3119 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3646 = torch.constant.int 5
    %3121 = torch.prims.convert_element_type %3120, %int5_3646 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3647 = torch.constant.int 1920
    %int1280_3648 = torch.constant.int 1280
    %3122 = torch.prim.ListConstruct %int1920_3647, %int1280_3648 : (!torch.int, !torch.int) -> !torch.list<int>
    %3123 = torch.aten.view %3121, %3122 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3124 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3649 = torch.constant.int 0
    %int1_3650 = torch.constant.int 1
    %3125 = torch.aten.transpose.int %3124, %int0_3649, %int1_3650 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %3126 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3651 = torch.constant.int 6
    %3127 = torch.prims.convert_element_type %3126, %int6_3651 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3652 = torch.constant.int 6
    %3128 = torch.prims.convert_element_type %3123, %int6_3652 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3653 = torch.constant.int 6
    %3129 = torch.prims.convert_element_type %3125, %int6_3653 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3130 = torch.aten.mm %3128, %3129 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3654 = torch.constant.int 1
    %3131 = torch.aten.mul.Scalar %3130, %int1_3654 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3655 = torch.constant.int 1
    %3132 = torch.aten.mul.Scalar %3127, %int1_3655 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3656 = torch.constant.int 1
    %3133 = torch.aten.add.Tensor %3131, %3132, %int1_3656 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3657 = torch.constant.int 5
    %3134 = torch.prims.convert_element_type %3133, %int5_3657 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3658 = torch.constant.int 2
    %int960_3659 = torch.constant.int 960
    %int1280_3660 = torch.constant.int 1280
    %3135 = torch.prim.ListConstruct %int2_3658, %int960_3659, %int1280_3660 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3136 = torch.aten.view %3134, %3135 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3661 = torch.constant.none
    %3137 = torch.aten.clone %3136, %none_3661 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3662 = torch.constant.float 1.000000e+00
    %3138 = torch.aten.div.Scalar %3137, %float1.000000e00_3662 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3663 = torch.constant.int 1
    %3139 = torch.aten.add.Tensor %3138, %3075, %int1_3663 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3664 = torch.constant.int 6
    %3140 = torch.prims.convert_element_type %3139, %int6_3664 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3665 = torch.constant.int 2
    %3141 = torch.prim.ListConstruct %int2_3665 : (!torch.int) -> !torch.list<int>
    %int0_3666 = torch.constant.int 0
    %true_3667 = torch.constant.bool true
    %result0_3668, %result1_3669 = torch.aten.var_mean.correction %3140, %3141, %int0_3666, %true_3667 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3670 = torch.constant.float 1.000000e-05
    %int1_3671 = torch.constant.int 1
    %3142 = torch.aten.add.Scalar %result0_3668, %float1.000000e-05_3670, %int1_3671 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3143 = torch.aten.rsqrt %3142 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3672 = torch.constant.int 1
    %3144 = torch.aten.sub.Tensor %3139, %result1_3669, %int1_3672 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3145 = torch.aten.mul.Tensor %3144, %3143 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %3146 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3147 = torch.aten.mul.Tensor %3145, %3146 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %3148 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3673 = torch.constant.int 1
    %3149 = torch.aten.add.Tensor %3147, %3148, %int1_3673 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3674 = torch.constant.int 5
    %3150 = torch.prims.convert_element_type %3149, %int5_3674 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3675 = torch.constant.int 1920
    %int1280_3676 = torch.constant.int 1280
    %3151 = torch.prim.ListConstruct %int1920_3675, %int1280_3676 : (!torch.int, !torch.int) -> !torch.list<int>
    %3152 = torch.aten.view %3150, %3151 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3153 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_3677 = torch.constant.int 0
    %int1_3678 = torch.constant.int 1
    %3154 = torch.aten.transpose.int %3153, %int0_3677, %int1_3678 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %3155 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_3679 = torch.constant.int 6
    %3156 = torch.prims.convert_element_type %3155, %int6_3679 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_3680 = torch.constant.int 6
    %3157 = torch.prims.convert_element_type %3152, %int6_3680 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3681 = torch.constant.int 6
    %3158 = torch.prims.convert_element_type %3154, %int6_3681 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3159 = torch.aten.mm %3157, %3158 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_3682 = torch.constant.int 1
    %3160 = torch.aten.mul.Scalar %3159, %int1_3682 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_3683 = torch.constant.int 1
    %3161 = torch.aten.mul.Scalar %3156, %int1_3683 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_3684 = torch.constant.int 1
    %3162 = torch.aten.add.Tensor %3160, %3161, %int1_3684 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_3685 = torch.constant.int 5
    %3163 = torch.prims.convert_element_type %3162, %int5_3685 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_3686 = torch.constant.int 2
    %int960_3687 = torch.constant.int 960
    %int10240_3688 = torch.constant.int 10240
    %3164 = torch.prim.ListConstruct %int2_3686, %int960_3687, %int10240_3688 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3165 = torch.aten.view %3163, %3164 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_3689 = torch.constant.int -1
    %int0_3690 = torch.constant.int 0
    %int5120_3691 = torch.constant.int 5120
    %int1_3692 = torch.constant.int 1
    %3166 = torch.aten.slice.Tensor %3165, %int-1_3689, %int0_3690, %int5120_3691, %int1_3692 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_3693 = torch.constant.int -1
    %int5120_3694 = torch.constant.int 5120
    %int10240_3695 = torch.constant.int 10240
    %int1_3696 = torch.constant.int 1
    %3167 = torch.aten.slice.Tensor %3165, %int-1_3693, %int5120_3694, %int10240_3695, %int1_3696 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_3697 = torch.constant.str "none"
    %3168 = torch.aten.gelu %3167, %str_3697 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %3169 = torch.aten.mul.Tensor %3166, %3168 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_3698 = torch.constant.none
    %3170 = torch.aten.clone %3169, %none_3698 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_3699 = torch.constant.int 1920
    %int5120_3700 = torch.constant.int 5120
    %3171 = torch.prim.ListConstruct %int1920_3699, %int5120_3700 : (!torch.int, !torch.int) -> !torch.list<int>
    %3172 = torch.aten.view %3170, %3171 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %3173 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_3701 = torch.constant.int 0
    %int1_3702 = torch.constant.int 1
    %3174 = torch.aten.transpose.int %3173, %int0_3701, %int1_3702 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %3175 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3703 = torch.constant.int 6
    %3176 = torch.prims.convert_element_type %3175, %int6_3703 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3704 = torch.constant.int 6
    %3177 = torch.prims.convert_element_type %3172, %int6_3704 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_3705 = torch.constant.int 6
    %3178 = torch.prims.convert_element_type %3174, %int6_3705 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3179 = torch.aten.mm %3177, %3178 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3706 = torch.constant.int 1
    %3180 = torch.aten.mul.Scalar %3179, %int1_3706 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3707 = torch.constant.int 1
    %3181 = torch.aten.mul.Scalar %3176, %int1_3707 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3708 = torch.constant.int 1
    %3182 = torch.aten.add.Tensor %3180, %3181, %int1_3708 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3709 = torch.constant.int 5
    %3183 = torch.prims.convert_element_type %3182, %int5_3709 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3710 = torch.constant.int 2
    %int960_3711 = torch.constant.int 960
    %int1280_3712 = torch.constant.int 1280
    %3184 = torch.prim.ListConstruct %int2_3710, %int960_3711, %int1280_3712 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3185 = torch.aten.view %3183, %3184 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_3713 = torch.constant.int 1
    %3186 = torch.aten.add.Tensor %3185, %3139, %int1_3713 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3714 = torch.constant.int 1920
    %int1280_3715 = torch.constant.int 1280
    %3187 = torch.prim.ListConstruct %int1920_3714, %int1280_3715 : (!torch.int, !torch.int) -> !torch.list<int>
    %3188 = torch.aten.view %3186, %3187 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.proj_out.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %3189 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3716 = torch.constant.int 0
    %int1_3717 = torch.constant.int 1
    %3190 = torch.aten.transpose.int %3189, %int0_3716, %int1_3717 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.0.proj_out.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.0.proj_out.bias : tensor<1280xf16>
    %3191 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3718 = torch.constant.int 6
    %3192 = torch.prims.convert_element_type %3191, %int6_3718 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3719 = torch.constant.int 6
    %3193 = torch.prims.convert_element_type %3188, %int6_3719 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3720 = torch.constant.int 6
    %3194 = torch.prims.convert_element_type %3190, %int6_3720 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3195 = torch.aten.mm %3193, %3194 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3721 = torch.constant.int 1
    %3196 = torch.aten.mul.Scalar %3195, %int1_3721 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3722 = torch.constant.int 1
    %3197 = torch.aten.mul.Scalar %3192, %int1_3722 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3723 = torch.constant.int 1
    %3198 = torch.aten.add.Tensor %3196, %3197, %int1_3723 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3724 = torch.constant.int 5
    %3199 = torch.prims.convert_element_type %3198, %int5_3724 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3725 = torch.constant.int 2
    %int960_3726 = torch.constant.int 960
    %int1280_3727 = torch.constant.int 1280
    %3200 = torch.prim.ListConstruct %int2_3725, %int960_3726, %int1280_3727 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3201 = torch.aten.view %3199, %3200 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_3728 = torch.constant.int 2
    %int30_3729 = torch.constant.int 30
    %int32_3730 = torch.constant.int 32
    %int1280_3731 = torch.constant.int 1280
    %3202 = torch.prim.ListConstruct %int2_3728, %int30_3729, %int32_3730, %int1280_3731 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3203 = torch.aten.view %3201, %3202 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,30,32,1280],f16>
    %int0_3732 = torch.constant.int 0
    %int3_3733 = torch.constant.int 3
    %int1_3734 = torch.constant.int 1
    %int2_3735 = torch.constant.int 2
    %3204 = torch.prim.ListConstruct %int0_3732, %int3_3733, %int1_3734, %int2_3735 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3205 = torch.aten.permute %3203, %3204 : !torch.vtensor<[2,30,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f16>
    %int0_3736 = torch.constant.int 0
    %3206 = torch.aten.clone %3205, %int0_3736 : !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_3737 = torch.constant.int 1
    %3207 = torch.aten.add.Tensor %3206, %1401, %int1_3737 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_3738 = torch.constant.int 2
    %int32_3739 = torch.constant.int 32
    %int40_3740 = torch.constant.int 40
    %int960_3741 = torch.constant.int 960
    %3208 = torch.prim.ListConstruct %int2_3738, %int32_3739, %int40_3740, %int960_3741 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3209 = torch.aten.view %3207, %3208 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_3742 = torch.constant.int 6
    %3210 = torch.prims.convert_element_type %3209, %int6_3742 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_3743 = torch.constant.int 2
    %int3_3744 = torch.constant.int 3
    %3211 = torch.prim.ListConstruct %int2_3743, %int3_3744 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_3745 = torch.constant.int 0
    %true_3746 = torch.constant.bool true
    %result0_3747, %result1_3748 = torch.aten.var_mean.correction %3210, %3211, %int0_3745, %true_3746 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_3749 = torch.constant.float 1.000000e-05
    %int1_3750 = torch.constant.int 1
    %3212 = torch.aten.add.Scalar %result0_3747, %float1.000000e-05_3749, %int1_3750 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3213 = torch.aten.rsqrt %3212 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_3751 = torch.constant.int 1
    %3214 = torch.aten.sub.Tensor %3209, %result1_3748, %int1_3751 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %3215 = torch.aten.mul.Tensor %3214, %3213 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_3752 = torch.constant.int 2
    %int1280_3753 = torch.constant.int 1280
    %int30_3754 = torch.constant.int 30
    %int32_3755 = torch.constant.int 32
    %3216 = torch.prim.ListConstruct %int2_3752, %int1280_3753, %int30_3754, %int32_3755 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3217 = torch.aten.view %3215, %3216 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.down_blocks.2.resnets.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.norm1.bias : tensor<1280xf16>
    %3218 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_3756 = torch.constant.int 0
    %3219 = torch.aten.unsqueeze %3218, %int0_3756 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_3757 = torch.constant.int 2
    %3220 = torch.aten.unsqueeze %3219, %int2_3757 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_3758 = torch.constant.int 3
    %3221 = torch.aten.unsqueeze %3220, %int3_3758 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.norm1.weight : tensor<1280xf16>
    %3222 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_3759 = torch.constant.int 0
    %3223 = torch.aten.unsqueeze %3222, %int0_3759 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_3760 = torch.constant.int 2
    %3224 = torch.aten.unsqueeze %3223, %int2_3760 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_3761 = torch.constant.int 3
    %3225 = torch.aten.unsqueeze %3224, %int3_3761 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3226 = torch.aten.mul.Tensor %3217, %3225 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_3762 = torch.constant.int 1
    %3227 = torch.aten.add.Tensor %3226, %3221, %int1_3762 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_3763 = torch.constant.int 5
    %3228 = torch.prims.convert_element_type %3227, %int5_3763 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %3229 = torch.aten.silu %3228 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.conv1.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16>
    %3230 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.conv1.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.conv1.bias : tensor<1280xf16>
    %3231 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3764 = torch.constant.int 1
    %int1_3765 = torch.constant.int 1
    %3232 = torch.prim.ListConstruct %int1_3764, %int1_3765 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3766 = torch.constant.int 1
    %int1_3767 = torch.constant.int 1
    %3233 = torch.prim.ListConstruct %int1_3766, %int1_3767 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3768 = torch.constant.int 1
    %int1_3769 = torch.constant.int 1
    %3234 = torch.prim.ListConstruct %int1_3768, %int1_3769 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_3770 = torch.constant.bool false
    %int0_3771 = torch.constant.int 0
    %int0_3772 = torch.constant.int 0
    %3235 = torch.prim.ListConstruct %int0_3771, %int0_3772 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3773 = torch.constant.int 1
    %3236 = torch.aten.convolution %3229, %3230, %3231, %3232, %3233, %3234, %false_3770, %3235, %int1_3773 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %3237 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %3238 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3774 = torch.constant.int 0
    %int1_3775 = torch.constant.int 1
    %3239 = torch.aten.transpose.int %3238, %int0_3774, %int1_3775 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %3240 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3776 = torch.constant.int 6
    %3241 = torch.prims.convert_element_type %3240, %int6_3776 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3777 = torch.constant.int 6
    %3242 = torch.prims.convert_element_type %3237, %int6_3777 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_3778 = torch.constant.int 6
    %3243 = torch.prims.convert_element_type %3239, %int6_3778 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3244 = torch.aten.mm %3242, %3243 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_3779 = torch.constant.int 1
    %3245 = torch.aten.mul.Scalar %3244, %int1_3779 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_3780 = torch.constant.int 1
    %3246 = torch.aten.mul.Scalar %3241, %int1_3780 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3781 = torch.constant.int 1
    %3247 = torch.aten.add.Tensor %3245, %3246, %int1_3781 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_3782 = torch.constant.int 5
    %3248 = torch.prims.convert_element_type %3247, %int5_3782 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int0_3783 = torch.constant.int 0
    %int0_3784 = torch.constant.int 0
    %int9223372036854775807_3785 = torch.constant.int 9223372036854775807
    %int1_3786 = torch.constant.int 1
    %3249 = torch.aten.slice.Tensor %3248, %int0_3783, %int0_3784, %int9223372036854775807_3785, %int1_3786 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int1_3787 = torch.constant.int 1
    %int0_3788 = torch.constant.int 0
    %int9223372036854775807_3789 = torch.constant.int 9223372036854775807
    %int1_3790 = torch.constant.int 1
    %3250 = torch.aten.slice.Tensor %3249, %int1_3787, %int0_3788, %int9223372036854775807_3789, %int1_3790 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int2_3791 = torch.constant.int 2
    %3251 = torch.aten.unsqueeze %3250, %int2_3791 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %int3_3792 = torch.constant.int 3
    %3252 = torch.aten.unsqueeze %3251, %int3_3792 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %int1_3793 = torch.constant.int 1
    %3253 = torch.aten.add.Tensor %3236, %3252, %int1_3793 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_3794 = torch.constant.int 2
    %int32_3795 = torch.constant.int 32
    %int40_3796 = torch.constant.int 40
    %int960_3797 = torch.constant.int 960
    %3254 = torch.prim.ListConstruct %int2_3794, %int32_3795, %int40_3796, %int960_3797 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3255 = torch.aten.view %3253, %3254 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_3798 = torch.constant.int 6
    %3256 = torch.prims.convert_element_type %3255, %int6_3798 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_3799 = torch.constant.int 2
    %int3_3800 = torch.constant.int 3
    %3257 = torch.prim.ListConstruct %int2_3799, %int3_3800 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_3801 = torch.constant.int 0
    %true_3802 = torch.constant.bool true
    %result0_3803, %result1_3804 = torch.aten.var_mean.correction %3256, %3257, %int0_3801, %true_3802 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_3805 = torch.constant.float 1.000000e-05
    %int1_3806 = torch.constant.int 1
    %3258 = torch.aten.add.Scalar %result0_3803, %float1.000000e-05_3805, %int1_3806 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3259 = torch.aten.rsqrt %3258 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_3807 = torch.constant.int 1
    %3260 = torch.aten.sub.Tensor %3255, %result1_3804, %int1_3807 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %3261 = torch.aten.mul.Tensor %3260, %3259 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_3808 = torch.constant.int 2
    %int1280_3809 = torch.constant.int 1280
    %int30_3810 = torch.constant.int 30
    %int32_3811 = torch.constant.int 32
    %3262 = torch.prim.ListConstruct %int2_3808, %int1280_3809, %int30_3810, %int32_3811 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3263 = torch.aten.view %3261, %3262 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.down_blocks.2.resnets.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.norm2.bias : tensor<1280xf16>
    %3264 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_3812 = torch.constant.int 0
    %3265 = torch.aten.unsqueeze %3264, %int0_3812 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_3813 = torch.constant.int 2
    %3266 = torch.aten.unsqueeze %3265, %int2_3813 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_3814 = torch.constant.int 3
    %3267 = torch.aten.unsqueeze %3266, %int3_3814 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.norm2.weight : tensor<1280xf16>
    %3268 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_3815 = torch.constant.int 0
    %3269 = torch.aten.unsqueeze %3268, %int0_3815 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_3816 = torch.constant.int 2
    %3270 = torch.aten.unsqueeze %3269, %int2_3816 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_3817 = torch.constant.int 3
    %3271 = torch.aten.unsqueeze %3270, %int3_3817 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3272 = torch.aten.mul.Tensor %3263, %3271 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_3818 = torch.constant.int 1
    %3273 = torch.aten.add.Tensor %3272, %3267, %int1_3818 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_3819 = torch.constant.int 5
    %3274 = torch.prims.convert_element_type %3273, %int5_3819 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %3275 = torch.aten.silu %3274 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %none_3820 = torch.constant.none
    %3276 = torch.aten.clone %3275, %none_3820 : !torch.vtensor<[2,1280,30,32],f16>, !torch.none -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.conv2.weight = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %3277 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.down_blocks.2.resnets.1.conv2.bias = util.global.load @__auto.controlnet.down_blocks.2.resnets.1.conv2.bias : tensor<1280xf16>
    %3278 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3821 = torch.constant.int 1
    %int1_3822 = torch.constant.int 1
    %3279 = torch.prim.ListConstruct %int1_3821, %int1_3822 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3823 = torch.constant.int 1
    %int1_3824 = torch.constant.int 1
    %3280 = torch.prim.ListConstruct %int1_3823, %int1_3824 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3825 = torch.constant.int 1
    %int1_3826 = torch.constant.int 1
    %3281 = torch.prim.ListConstruct %int1_3825, %int1_3826 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_3827 = torch.constant.bool false
    %int0_3828 = torch.constant.int 0
    %int0_3829 = torch.constant.int 0
    %3282 = torch.prim.ListConstruct %int0_3828, %int0_3829 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_3830 = torch.constant.int 1
    %3283 = torch.aten.convolution %3276, %3277, %3278, %3279, %3280, %3281, %false_3827, %3282, %int1_3830 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_3831 = torch.constant.int 1
    %3284 = torch.aten.add.Tensor %3207, %3283, %int1_3831 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %float1.000000e00_3832 = torch.constant.float 1.000000e+00
    %3285 = torch.aten.div.Scalar %3284, %float1.000000e00_3832 : !torch.vtensor<[2,1280,30,32],f16>, !torch.float -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_3833 = torch.constant.int 2
    %int32_3834 = torch.constant.int 32
    %int40_3835 = torch.constant.int 40
    %int960_3836 = torch.constant.int 960
    %3286 = torch.prim.ListConstruct %int2_3833, %int32_3834, %int40_3835, %int960_3836 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3287 = torch.aten.view %3285, %3286 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_3837 = torch.constant.int 6
    %3288 = torch.prims.convert_element_type %3287, %int6_3837 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_3838 = torch.constant.int 2
    %int3_3839 = torch.constant.int 3
    %3289 = torch.prim.ListConstruct %int2_3838, %int3_3839 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_3840 = torch.constant.int 0
    %true_3841 = torch.constant.bool true
    %result0_3842, %result1_3843 = torch.aten.var_mean.correction %3288, %3289, %int0_3840, %true_3841 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float9.999990e-07_3844 = torch.constant.float 9.9999999999999995E-7
    %int1_3845 = torch.constant.int 1
    %3290 = torch.aten.add.Scalar %result0_3842, %float9.999990e-07_3844, %int1_3845 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %3291 = torch.aten.rsqrt %3290 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_3846 = torch.constant.int 1
    %3292 = torch.aten.sub.Tensor %3287, %result1_3843, %int1_3846 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %3293 = torch.aten.mul.Tensor %3292, %3291 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_3847 = torch.constant.int 2
    %int1280_3848 = torch.constant.int 1280
    %int30_3849 = torch.constant.int 30
    %int32_3850 = torch.constant.int 32
    %3294 = torch.prim.ListConstruct %int2_3847, %int1280_3848, %int30_3849, %int32_3850 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3295 = torch.aten.view %3293, %3294 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.norm.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.norm.bias : tensor<1280xf16>
    %3296 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_3851 = torch.constant.int 0
    %3297 = torch.aten.unsqueeze %3296, %int0_3851 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_3852 = torch.constant.int 2
    %3298 = torch.aten.unsqueeze %3297, %int2_3852 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_3853 = torch.constant.int 3
    %3299 = torch.aten.unsqueeze %3298, %int3_3853 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.norm.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.norm.weight : tensor<1280xf16>
    %3300 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_3854 = torch.constant.int 0
    %3301 = torch.aten.unsqueeze %3300, %int0_3854 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_3855 = torch.constant.int 2
    %3302 = torch.aten.unsqueeze %3301, %int2_3855 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_3856 = torch.constant.int 3
    %3303 = torch.aten.unsqueeze %3302, %int3_3856 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %3304 = torch.aten.mul.Tensor %3295, %3303 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_3857 = torch.constant.int 1
    %3305 = torch.aten.add.Tensor %3304, %3299, %int1_3857 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_3858 = torch.constant.int 5
    %3306 = torch.prims.convert_element_type %3305, %int5_3858 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int0_3859 = torch.constant.int 0
    %int2_3860 = torch.constant.int 2
    %int3_3861 = torch.constant.int 3
    %int1_3862 = torch.constant.int 1
    %3307 = torch.prim.ListConstruct %int0_3859, %int2_3860, %int3_3861, %int1_3862 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3308 = torch.aten.permute %3306, %3307 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,30,32,1280],f16>
    %int2_3863 = torch.constant.int 2
    %int960_3864 = torch.constant.int 960
    %int1280_3865 = torch.constant.int 1280
    %3309 = torch.prim.ListConstruct %int2_3863, %int960_3864, %int1280_3865 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3310 = torch.aten.view %3308, %3309 : !torch.vtensor<[2,30,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.proj_in.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.proj_in.weight : tensor<1280x1280xf16>
    %3311 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3866 = torch.constant.int 0
    %int1_3867 = torch.constant.int 1
    %3312 = torch.aten.transpose.int %3311, %int0_3866, %int1_3867 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int0_3868 = torch.constant.int 0
    %3313 = torch.aten.clone %3310, %int0_3868 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3869 = torch.constant.int 1920
    %int1280_3870 = torch.constant.int 1280
    %3314 = torch.prim.ListConstruct %int1920_3869, %int1280_3870 : (!torch.int, !torch.int) -> !torch.list<int>
    %3315 = torch.aten._unsafe_view %3313, %3314 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3316 = torch.aten.mm %3315, %3312 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3871 = torch.constant.int 2
    %int960_3872 = torch.constant.int 960
    %int1280_3873 = torch.constant.int 1280
    %3317 = torch.prim.ListConstruct %int2_3871, %int960_3872, %int1280_3873 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3318 = torch.aten.view %3316, %3317 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.proj_in.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.proj_in.bias : tensor<1280xf16>
    %3319 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3874 = torch.constant.int 1
    %3320 = torch.aten.add.Tensor %3318, %3319, %int1_3874 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3875 = torch.constant.int 6
    %3321 = torch.prims.convert_element_type %3320, %int6_3875 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3876 = torch.constant.int 2
    %3322 = torch.prim.ListConstruct %int2_3876 : (!torch.int) -> !torch.list<int>
    %int0_3877 = torch.constant.int 0
    %true_3878 = torch.constant.bool true
    %result0_3879, %result1_3880 = torch.aten.var_mean.correction %3321, %3322, %int0_3877, %true_3878 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3881 = torch.constant.float 1.000000e-05
    %int1_3882 = torch.constant.int 1
    %3323 = torch.aten.add.Scalar %result0_3879, %float1.000000e-05_3881, %int1_3882 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3324 = torch.aten.rsqrt %3323 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3883 = torch.constant.int 1
    %3325 = torch.aten.sub.Tensor %3320, %result1_3880, %int1_3883 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3326 = torch.aten.mul.Tensor %3325, %3324 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %3327 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3328 = torch.aten.mul.Tensor %3326, %3327 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %3329 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3884 = torch.constant.int 1
    %3330 = torch.aten.add.Tensor %3328, %3329, %int1_3884 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3885 = torch.constant.int 5
    %3331 = torch.prims.convert_element_type %3330, %int5_3885 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %3332 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3886 = torch.constant.int 0
    %int1_3887 = torch.constant.int 1
    %3333 = torch.aten.transpose.int %3332, %int0_3886, %int1_3887 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3888 = torch.constant.int 1920
    %int1280_3889 = torch.constant.int 1280
    %3334 = torch.prim.ListConstruct %int1920_3888, %int1280_3889 : (!torch.int, !torch.int) -> !torch.list<int>
    %3335 = torch.aten.view %3331, %3334 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3336 = torch.aten.mm %3335, %3333 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3890 = torch.constant.int 2
    %int960_3891 = torch.constant.int 960
    %int1280_3892 = torch.constant.int 1280
    %3337 = torch.prim.ListConstruct %int2_3890, %int960_3891, %int1280_3892 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3338 = torch.aten.view %3336, %3337 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %3339 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3893 = torch.constant.int 0
    %int1_3894 = torch.constant.int 1
    %3340 = torch.aten.transpose.int %3339, %int0_3893, %int1_3894 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3895 = torch.constant.int 1920
    %int1280_3896 = torch.constant.int 1280
    %3341 = torch.prim.ListConstruct %int1920_3895, %int1280_3896 : (!torch.int, !torch.int) -> !torch.list<int>
    %3342 = torch.aten.view %3331, %3341 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3343 = torch.aten.mm %3342, %3340 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3897 = torch.constant.int 2
    %int960_3898 = torch.constant.int 960
    %int1280_3899 = torch.constant.int 1280
    %3344 = torch.prim.ListConstruct %int2_3897, %int960_3898, %int1280_3899 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3345 = torch.aten.view %3343, %3344 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %3346 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3900 = torch.constant.int 0
    %int1_3901 = torch.constant.int 1
    %3347 = torch.aten.transpose.int %3346, %int0_3900, %int1_3901 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3902 = torch.constant.int 1920
    %int1280_3903 = torch.constant.int 1280
    %3348 = torch.prim.ListConstruct %int1920_3902, %int1280_3903 : (!torch.int, !torch.int) -> !torch.list<int>
    %3349 = torch.aten.view %3331, %3348 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3350 = torch.aten.mm %3349, %3347 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3904 = torch.constant.int 2
    %int960_3905 = torch.constant.int 960
    %int1280_3906 = torch.constant.int 1280
    %3351 = torch.prim.ListConstruct %int2_3904, %int960_3905, %int1280_3906 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3352 = torch.aten.view %3350, %3351 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_3907 = torch.constant.int 2
    %int-1_3908 = torch.constant.int -1
    %int20_3909 = torch.constant.int 20
    %int64_3910 = torch.constant.int 64
    %3353 = torch.prim.ListConstruct %int2_3907, %int-1_3908, %int20_3909, %int64_3910 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3354 = torch.aten.view %3338, %3353 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3911 = torch.constant.int 1
    %int2_3912 = torch.constant.int 2
    %3355 = torch.aten.transpose.int %3354, %int1_3911, %int2_3912 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3913 = torch.constant.int 2
    %int-1_3914 = torch.constant.int -1
    %int20_3915 = torch.constant.int 20
    %int64_3916 = torch.constant.int 64
    %3356 = torch.prim.ListConstruct %int2_3913, %int-1_3914, %int20_3915, %int64_3916 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3357 = torch.aten.view %3345, %3356 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3917 = torch.constant.int 1
    %int2_3918 = torch.constant.int 2
    %3358 = torch.aten.transpose.int %3357, %int1_3917, %int2_3918 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3919 = torch.constant.int 2
    %int-1_3920 = torch.constant.int -1
    %int20_3921 = torch.constant.int 20
    %int64_3922 = torch.constant.int 64
    %3359 = torch.prim.ListConstruct %int2_3919, %int-1_3920, %int20_3921, %int64_3922 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3360 = torch.aten.view %3352, %3359 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3923 = torch.constant.int 1
    %int2_3924 = torch.constant.int 2
    %3361 = torch.aten.transpose.int %3360, %int1_3923, %int2_3924 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_3925 = torch.constant.float 0.000000e+00
    %false_3926 = torch.constant.bool false
    %none_3927 = torch.constant.none
    %none_3928 = torch.constant.none
    %3362:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3355, %3358, %3361, %float0.000000e00_3925, %false_3926, %none_3927, %none_3928) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_3929 = torch.constant.int 1
    %int2_3930 = torch.constant.int 2
    %3363 = torch.aten.transpose.int %3362#0, %int1_3929, %int2_3930 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_3931 = torch.constant.int 2
    %int-1_3932 = torch.constant.int -1
    %int1280_3933 = torch.constant.int 1280
    %3364 = torch.prim.ListConstruct %int2_3931, %int-1_3932, %int1280_3933 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3365 = torch.aten.view %3363, %3364 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_3934 = torch.constant.int 5
    %3366 = torch.prims.convert_element_type %3365, %int5_3934 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_3935 = torch.constant.int 1920
    %int1280_3936 = torch.constant.int 1280
    %3367 = torch.prim.ListConstruct %int1920_3935, %int1280_3936 : (!torch.int, !torch.int) -> !torch.list<int>
    %3368 = torch.aten.view %3366, %3367 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3369 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3937 = torch.constant.int 0
    %int1_3938 = torch.constant.int 1
    %3370 = torch.aten.transpose.int %3369, %int0_3937, %int1_3938 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %3371 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_3939 = torch.constant.int 6
    %3372 = torch.prims.convert_element_type %3371, %int6_3939 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_3940 = torch.constant.int 6
    %3373 = torch.prims.convert_element_type %3368, %int6_3940 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_3941 = torch.constant.int 6
    %3374 = torch.prims.convert_element_type %3370, %int6_3941 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3375 = torch.aten.mm %3373, %3374 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_3942 = torch.constant.int 1
    %3376 = torch.aten.mul.Scalar %3375, %int1_3942 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_3943 = torch.constant.int 1
    %3377 = torch.aten.mul.Scalar %3372, %int1_3943 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_3944 = torch.constant.int 1
    %3378 = torch.aten.add.Tensor %3376, %3377, %int1_3944 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_3945 = torch.constant.int 5
    %3379 = torch.prims.convert_element_type %3378, %int5_3945 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_3946 = torch.constant.int 2
    %int960_3947 = torch.constant.int 960
    %int1280_3948 = torch.constant.int 1280
    %3380 = torch.prim.ListConstruct %int2_3946, %int960_3947, %int1280_3948 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3381 = torch.aten.view %3379, %3380 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_3949 = torch.constant.none
    %3382 = torch.aten.clone %3381, %none_3949 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_3950 = torch.constant.float 1.000000e+00
    %3383 = torch.aten.div.Scalar %3382, %float1.000000e00_3950 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_3951 = torch.constant.int 1
    %3384 = torch.aten.add.Tensor %3383, %3320, %int1_3951 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_3952 = torch.constant.int 6
    %3385 = torch.prims.convert_element_type %3384, %int6_3952 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_3953 = torch.constant.int 2
    %3386 = torch.prim.ListConstruct %int2_3953 : (!torch.int) -> !torch.list<int>
    %int0_3954 = torch.constant.int 0
    %true_3955 = torch.constant.bool true
    %result0_3956, %result1_3957 = torch.aten.var_mean.correction %3385, %3386, %int0_3954, %true_3955 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_3958 = torch.constant.float 1.000000e-05
    %int1_3959 = torch.constant.int 1
    %3387 = torch.aten.add.Scalar %result0_3956, %float1.000000e-05_3958, %int1_3959 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3388 = torch.aten.rsqrt %3387 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_3960 = torch.constant.int 1
    %3389 = torch.aten.sub.Tensor %3384, %result1_3957, %int1_3960 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3390 = torch.aten.mul.Tensor %3389, %3388 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %3391 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3392 = torch.aten.mul.Tensor %3390, %3391 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %3393 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_3961 = torch.constant.int 1
    %3394 = torch.aten.add.Tensor %3392, %3393, %int1_3961 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_3962 = torch.constant.int 5
    %3395 = torch.prims.convert_element_type %3394, %int5_3962 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %3396 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_3963 = torch.constant.int 0
    %int1_3964 = torch.constant.int 1
    %3397 = torch.aten.transpose.int %3396, %int0_3963, %int1_3964 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_3965 = torch.constant.int 1920
    %int1280_3966 = torch.constant.int 1280
    %3398 = torch.prim.ListConstruct %int1920_3965, %int1280_3966 : (!torch.int, !torch.int) -> !torch.list<int>
    %3399 = torch.aten.view %3395, %3398 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3400 = torch.aten.mm %3399, %3397 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_3967 = torch.constant.int 2
    %int960_3968 = torch.constant.int 960
    %int1280_3969 = torch.constant.int 1280
    %3401 = torch.prim.ListConstruct %int2_3967, %int960_3968, %int1280_3969 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3402 = torch.aten.view %3400, %3401 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %3403 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3970 = torch.constant.int 0
    %int1_3971 = torch.constant.int 1
    %3404 = torch.aten.transpose.int %3403, %int0_3970, %int1_3971 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3972 = torch.constant.int 32
    %int2048_3973 = torch.constant.int 2048
    %3405 = torch.prim.ListConstruct %int32_3972, %int2048_3973 : (!torch.int, !torch.int) -> !torch.list<int>
    %3406 = torch.aten.view %arg6, %3405 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3407 = torch.aten.mm %3406, %3404 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3974 = torch.constant.int 2
    %int16_3975 = torch.constant.int 16
    %int1280_3976 = torch.constant.int 1280
    %3408 = torch.prim.ListConstruct %int2_3974, %int16_3975, %int1280_3976 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3409 = torch.aten.view %3407, %3408 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %3410 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_3977 = torch.constant.int 0
    %int1_3978 = torch.constant.int 1
    %3411 = torch.aten.transpose.int %3410, %int0_3977, %int1_3978 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_3979 = torch.constant.int 32
    %int2048_3980 = torch.constant.int 2048
    %3412 = torch.prim.ListConstruct %int32_3979, %int2048_3980 : (!torch.int, !torch.int) -> !torch.list<int>
    %3413 = torch.aten.view %arg6, %3412 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3414 = torch.aten.mm %3413, %3411 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_3981 = torch.constant.int 2
    %int16_3982 = torch.constant.int 16
    %int1280_3983 = torch.constant.int 1280
    %3415 = torch.prim.ListConstruct %int2_3981, %int16_3982, %int1280_3983 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3416 = torch.aten.view %3414, %3415 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_3984 = torch.constant.int 2
    %int-1_3985 = torch.constant.int -1
    %int20_3986 = torch.constant.int 20
    %int64_3987 = torch.constant.int 64
    %3417 = torch.prim.ListConstruct %int2_3984, %int-1_3985, %int20_3986, %int64_3987 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3418 = torch.aten.view %3402, %3417 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_3988 = torch.constant.int 1
    %int2_3989 = torch.constant.int 2
    %3419 = torch.aten.transpose.int %3418, %int1_3988, %int2_3989 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_3990 = torch.constant.int 2
    %int-1_3991 = torch.constant.int -1
    %int20_3992 = torch.constant.int 20
    %int64_3993 = torch.constant.int 64
    %3420 = torch.prim.ListConstruct %int2_3990, %int-1_3991, %int20_3992, %int64_3993 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3421 = torch.aten.view %3409, %3420 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_3994 = torch.constant.int 1
    %int2_3995 = torch.constant.int 2
    %3422 = torch.aten.transpose.int %3421, %int1_3994, %int2_3995 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_3996 = torch.constant.int 2
    %int-1_3997 = torch.constant.int -1
    %int20_3998 = torch.constant.int 20
    %int64_3999 = torch.constant.int 64
    %3423 = torch.prim.ListConstruct %int2_3996, %int-1_3997, %int20_3998, %int64_3999 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3424 = torch.aten.view %3416, %3423 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4000 = torch.constant.int 1
    %int2_4001 = torch.constant.int 2
    %3425 = torch.aten.transpose.int %3424, %int1_4000, %int2_4001 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_4002 = torch.constant.float 0.000000e+00
    %false_4003 = torch.constant.bool false
    %none_4004 = torch.constant.none
    %none_4005 = torch.constant.none
    %3426:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3419, %3422, %3425, %float0.000000e00_4002, %false_4003, %none_4004, %none_4005) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4006 = torch.constant.int 1
    %int2_4007 = torch.constant.int 2
    %3427 = torch.aten.transpose.int %3426#0, %int1_4006, %int2_4007 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4008 = torch.constant.int 2
    %int-1_4009 = torch.constant.int -1
    %int1280_4010 = torch.constant.int 1280
    %3428 = torch.prim.ListConstruct %int2_4008, %int-1_4009, %int1280_4010 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3429 = torch.aten.view %3427, %3428 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4011 = torch.constant.int 5
    %3430 = torch.prims.convert_element_type %3429, %int5_4011 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4012 = torch.constant.int 1920
    %int1280_4013 = torch.constant.int 1280
    %3431 = torch.prim.ListConstruct %int1920_4012, %int1280_4013 : (!torch.int, !torch.int) -> !torch.list<int>
    %3432 = torch.aten.view %3430, %3431 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3433 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4014 = torch.constant.int 0
    %int1_4015 = torch.constant.int 1
    %3434 = torch.aten.transpose.int %3433, %int0_4014, %int1_4015 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %3435 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4016 = torch.constant.int 6
    %3436 = torch.prims.convert_element_type %3435, %int6_4016 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4017 = torch.constant.int 6
    %3437 = torch.prims.convert_element_type %3432, %int6_4017 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4018 = torch.constant.int 6
    %3438 = torch.prims.convert_element_type %3434, %int6_4018 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3439 = torch.aten.mm %3437, %3438 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4019 = torch.constant.int 1
    %3440 = torch.aten.mul.Scalar %3439, %int1_4019 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4020 = torch.constant.int 1
    %3441 = torch.aten.mul.Scalar %3436, %int1_4020 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4021 = torch.constant.int 1
    %3442 = torch.aten.add.Tensor %3440, %3441, %int1_4021 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4022 = torch.constant.int 5
    %3443 = torch.prims.convert_element_type %3442, %int5_4022 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4023 = torch.constant.int 2
    %int960_4024 = torch.constant.int 960
    %int1280_4025 = torch.constant.int 1280
    %3444 = torch.prim.ListConstruct %int2_4023, %int960_4024, %int1280_4025 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3445 = torch.aten.view %3443, %3444 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4026 = torch.constant.none
    %3446 = torch.aten.clone %3445, %none_4026 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4027 = torch.constant.float 1.000000e+00
    %3447 = torch.aten.div.Scalar %3446, %float1.000000e00_4027 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4028 = torch.constant.int 1
    %3448 = torch.aten.add.Tensor %3447, %3384, %int1_4028 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4029 = torch.constant.int 6
    %3449 = torch.prims.convert_element_type %3448, %int6_4029 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4030 = torch.constant.int 2
    %3450 = torch.prim.ListConstruct %int2_4030 : (!torch.int) -> !torch.list<int>
    %int0_4031 = torch.constant.int 0
    %true_4032 = torch.constant.bool true
    %result0_4033, %result1_4034 = torch.aten.var_mean.correction %3449, %3450, %int0_4031, %true_4032 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4035 = torch.constant.float 1.000000e-05
    %int1_4036 = torch.constant.int 1
    %3451 = torch.aten.add.Scalar %result0_4033, %float1.000000e-05_4035, %int1_4036 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3452 = torch.aten.rsqrt %3451 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4037 = torch.constant.int 1
    %3453 = torch.aten.sub.Tensor %3448, %result1_4034, %int1_4037 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3454 = torch.aten.mul.Tensor %3453, %3452 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %3455 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3456 = torch.aten.mul.Tensor %3454, %3455 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %3457 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4038 = torch.constant.int 1
    %3458 = torch.aten.add.Tensor %3456, %3457, %int1_4038 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4039 = torch.constant.int 5
    %3459 = torch.prims.convert_element_type %3458, %int5_4039 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4040 = torch.constant.int 1920
    %int1280_4041 = torch.constant.int 1280
    %3460 = torch.prim.ListConstruct %int1920_4040, %int1280_4041 : (!torch.int, !torch.int) -> !torch.list<int>
    %3461 = torch.aten.view %3459, %3460 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3462 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_4042 = torch.constant.int 0
    %int1_4043 = torch.constant.int 1
    %3463 = torch.aten.transpose.int %3462, %int0_4042, %int1_4043 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %3464 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_4044 = torch.constant.int 6
    %3465 = torch.prims.convert_element_type %3464, %int6_4044 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_4045 = torch.constant.int 6
    %3466 = torch.prims.convert_element_type %3461, %int6_4045 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4046 = torch.constant.int 6
    %3467 = torch.prims.convert_element_type %3463, %int6_4046 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3468 = torch.aten.mm %3466, %3467 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_4047 = torch.constant.int 1
    %3469 = torch.aten.mul.Scalar %3468, %int1_4047 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_4048 = torch.constant.int 1
    %3470 = torch.aten.mul.Scalar %3465, %int1_4048 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_4049 = torch.constant.int 1
    %3471 = torch.aten.add.Tensor %3469, %3470, %int1_4049 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_4050 = torch.constant.int 5
    %3472 = torch.prims.convert_element_type %3471, %int5_4050 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_4051 = torch.constant.int 2
    %int960_4052 = torch.constant.int 960
    %int10240_4053 = torch.constant.int 10240
    %3473 = torch.prim.ListConstruct %int2_4051, %int960_4052, %int10240_4053 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3474 = torch.aten.view %3472, %3473 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_4054 = torch.constant.int -1
    %int0_4055 = torch.constant.int 0
    %int5120_4056 = torch.constant.int 5120
    %int1_4057 = torch.constant.int 1
    %3475 = torch.aten.slice.Tensor %3474, %int-1_4054, %int0_4055, %int5120_4056, %int1_4057 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_4058 = torch.constant.int -1
    %int5120_4059 = torch.constant.int 5120
    %int10240_4060 = torch.constant.int 10240
    %int1_4061 = torch.constant.int 1
    %3476 = torch.aten.slice.Tensor %3474, %int-1_4058, %int5120_4059, %int10240_4060, %int1_4061 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_4062 = torch.constant.str "none"
    %3477 = torch.aten.gelu %3476, %str_4062 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %3478 = torch.aten.mul.Tensor %3475, %3477 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_4063 = torch.constant.none
    %3479 = torch.aten.clone %3478, %none_4063 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_4064 = torch.constant.int 1920
    %int5120_4065 = torch.constant.int 5120
    %3480 = torch.prim.ListConstruct %int1920_4064, %int5120_4065 : (!torch.int, !torch.int) -> !torch.list<int>
    %3481 = torch.aten.view %3479, %3480 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %3482 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4066 = torch.constant.int 0
    %int1_4067 = torch.constant.int 1
    %3483 = torch.aten.transpose.int %3482, %int0_4066, %int1_4067 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %3484 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4068 = torch.constant.int 6
    %3485 = torch.prims.convert_element_type %3484, %int6_4068 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4069 = torch.constant.int 6
    %3486 = torch.prims.convert_element_type %3481, %int6_4069 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_4070 = torch.constant.int 6
    %3487 = torch.prims.convert_element_type %3483, %int6_4070 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3488 = torch.aten.mm %3486, %3487 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4071 = torch.constant.int 1
    %3489 = torch.aten.mul.Scalar %3488, %int1_4071 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4072 = torch.constant.int 1
    %3490 = torch.aten.mul.Scalar %3485, %int1_4072 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4073 = torch.constant.int 1
    %3491 = torch.aten.add.Tensor %3489, %3490, %int1_4073 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4074 = torch.constant.int 5
    %3492 = torch.prims.convert_element_type %3491, %int5_4074 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4075 = torch.constant.int 2
    %int960_4076 = torch.constant.int 960
    %int1280_4077 = torch.constant.int 1280
    %3493 = torch.prim.ListConstruct %int2_4075, %int960_4076, %int1280_4077 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3494 = torch.aten.view %3492, %3493 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_4078 = torch.constant.int 1
    %3495 = torch.aten.add.Tensor %3494, %3448, %int1_4078 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4079 = torch.constant.int 6
    %3496 = torch.prims.convert_element_type %3495, %int6_4079 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4080 = torch.constant.int 2
    %3497 = torch.prim.ListConstruct %int2_4080 : (!torch.int) -> !torch.list<int>
    %int0_4081 = torch.constant.int 0
    %true_4082 = torch.constant.bool true
    %result0_4083, %result1_4084 = torch.aten.var_mean.correction %3496, %3497, %int0_4081, %true_4082 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4085 = torch.constant.float 1.000000e-05
    %int1_4086 = torch.constant.int 1
    %3498 = torch.aten.add.Scalar %result0_4083, %float1.000000e-05_4085, %int1_4086 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3499 = torch.aten.rsqrt %3498 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4087 = torch.constant.int 1
    %3500 = torch.aten.sub.Tensor %3495, %result1_4084, %int1_4087 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3501 = torch.aten.mul.Tensor %3500, %3499 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %3502 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3503 = torch.aten.mul.Tensor %3501, %3502 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %3504 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4088 = torch.constant.int 1
    %3505 = torch.aten.add.Tensor %3503, %3504, %int1_4088 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4089 = torch.constant.int 5
    %3506 = torch.prims.convert_element_type %3505, %int5_4089 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %3507 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4090 = torch.constant.int 0
    %int1_4091 = torch.constant.int 1
    %3508 = torch.aten.transpose.int %3507, %int0_4090, %int1_4091 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4092 = torch.constant.int 1920
    %int1280_4093 = torch.constant.int 1280
    %3509 = torch.prim.ListConstruct %int1920_4092, %int1280_4093 : (!torch.int, !torch.int) -> !torch.list<int>
    %3510 = torch.aten.view %3506, %3509 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3511 = torch.aten.mm %3510, %3508 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4094 = torch.constant.int 2
    %int960_4095 = torch.constant.int 960
    %int1280_4096 = torch.constant.int 1280
    %3512 = torch.prim.ListConstruct %int2_4094, %int960_4095, %int1280_4096 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3513 = torch.aten.view %3511, %3512 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %3514 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4097 = torch.constant.int 0
    %int1_4098 = torch.constant.int 1
    %3515 = torch.aten.transpose.int %3514, %int0_4097, %int1_4098 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4099 = torch.constant.int 1920
    %int1280_4100 = torch.constant.int 1280
    %3516 = torch.prim.ListConstruct %int1920_4099, %int1280_4100 : (!torch.int, !torch.int) -> !torch.list<int>
    %3517 = torch.aten.view %3506, %3516 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3518 = torch.aten.mm %3517, %3515 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4101 = torch.constant.int 2
    %int960_4102 = torch.constant.int 960
    %int1280_4103 = torch.constant.int 1280
    %3519 = torch.prim.ListConstruct %int2_4101, %int960_4102, %int1280_4103 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3520 = torch.aten.view %3518, %3519 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %3521 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4104 = torch.constant.int 0
    %int1_4105 = torch.constant.int 1
    %3522 = torch.aten.transpose.int %3521, %int0_4104, %int1_4105 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4106 = torch.constant.int 1920
    %int1280_4107 = torch.constant.int 1280
    %3523 = torch.prim.ListConstruct %int1920_4106, %int1280_4107 : (!torch.int, !torch.int) -> !torch.list<int>
    %3524 = torch.aten.view %3506, %3523 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3525 = torch.aten.mm %3524, %3522 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4108 = torch.constant.int 2
    %int960_4109 = torch.constant.int 960
    %int1280_4110 = torch.constant.int 1280
    %3526 = torch.prim.ListConstruct %int2_4108, %int960_4109, %int1280_4110 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3527 = torch.aten.view %3525, %3526 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_4111 = torch.constant.int 2
    %int-1_4112 = torch.constant.int -1
    %int20_4113 = torch.constant.int 20
    %int64_4114 = torch.constant.int 64
    %3528 = torch.prim.ListConstruct %int2_4111, %int-1_4112, %int20_4113, %int64_4114 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3529 = torch.aten.view %3513, %3528 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4115 = torch.constant.int 1
    %int2_4116 = torch.constant.int 2
    %3530 = torch.aten.transpose.int %3529, %int1_4115, %int2_4116 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4117 = torch.constant.int 2
    %int-1_4118 = torch.constant.int -1
    %int20_4119 = torch.constant.int 20
    %int64_4120 = torch.constant.int 64
    %3531 = torch.prim.ListConstruct %int2_4117, %int-1_4118, %int20_4119, %int64_4120 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3532 = torch.aten.view %3520, %3531 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4121 = torch.constant.int 1
    %int2_4122 = torch.constant.int 2
    %3533 = torch.aten.transpose.int %3532, %int1_4121, %int2_4122 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4123 = torch.constant.int 2
    %int-1_4124 = torch.constant.int -1
    %int20_4125 = torch.constant.int 20
    %int64_4126 = torch.constant.int 64
    %3534 = torch.prim.ListConstruct %int2_4123, %int-1_4124, %int20_4125, %int64_4126 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3535 = torch.aten.view %3527, %3534 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4127 = torch.constant.int 1
    %int2_4128 = torch.constant.int 2
    %3536 = torch.aten.transpose.int %3535, %int1_4127, %int2_4128 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_4129 = torch.constant.float 0.000000e+00
    %false_4130 = torch.constant.bool false
    %none_4131 = torch.constant.none
    %none_4132 = torch.constant.none
    %3537:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3530, %3533, %3536, %float0.000000e00_4129, %false_4130, %none_4131, %none_4132) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4133 = torch.constant.int 1
    %int2_4134 = torch.constant.int 2
    %3538 = torch.aten.transpose.int %3537#0, %int1_4133, %int2_4134 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4135 = torch.constant.int 2
    %int-1_4136 = torch.constant.int -1
    %int1280_4137 = torch.constant.int 1280
    %3539 = torch.prim.ListConstruct %int2_4135, %int-1_4136, %int1280_4137 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3540 = torch.aten.view %3538, %3539 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4138 = torch.constant.int 5
    %3541 = torch.prims.convert_element_type %3540, %int5_4138 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4139 = torch.constant.int 1920
    %int1280_4140 = torch.constant.int 1280
    %3542 = torch.prim.ListConstruct %int1920_4139, %int1280_4140 : (!torch.int, !torch.int) -> !torch.list<int>
    %3543 = torch.aten.view %3541, %3542 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3544 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4141 = torch.constant.int 0
    %int1_4142 = torch.constant.int 1
    %3545 = torch.aten.transpose.int %3544, %int0_4141, %int1_4142 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %3546 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4143 = torch.constant.int 6
    %3547 = torch.prims.convert_element_type %3546, %int6_4143 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4144 = torch.constant.int 6
    %3548 = torch.prims.convert_element_type %3543, %int6_4144 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4145 = torch.constant.int 6
    %3549 = torch.prims.convert_element_type %3545, %int6_4145 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3550 = torch.aten.mm %3548, %3549 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4146 = torch.constant.int 1
    %3551 = torch.aten.mul.Scalar %3550, %int1_4146 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4147 = torch.constant.int 1
    %3552 = torch.aten.mul.Scalar %3547, %int1_4147 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4148 = torch.constant.int 1
    %3553 = torch.aten.add.Tensor %3551, %3552, %int1_4148 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4149 = torch.constant.int 5
    %3554 = torch.prims.convert_element_type %3553, %int5_4149 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4150 = torch.constant.int 2
    %int960_4151 = torch.constant.int 960
    %int1280_4152 = torch.constant.int 1280
    %3555 = torch.prim.ListConstruct %int2_4150, %int960_4151, %int1280_4152 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3556 = torch.aten.view %3554, %3555 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4153 = torch.constant.none
    %3557 = torch.aten.clone %3556, %none_4153 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4154 = torch.constant.float 1.000000e+00
    %3558 = torch.aten.div.Scalar %3557, %float1.000000e00_4154 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4155 = torch.constant.int 1
    %3559 = torch.aten.add.Tensor %3558, %3495, %int1_4155 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4156 = torch.constant.int 6
    %3560 = torch.prims.convert_element_type %3559, %int6_4156 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4157 = torch.constant.int 2
    %3561 = torch.prim.ListConstruct %int2_4157 : (!torch.int) -> !torch.list<int>
    %int0_4158 = torch.constant.int 0
    %true_4159 = torch.constant.bool true
    %result0_4160, %result1_4161 = torch.aten.var_mean.correction %3560, %3561, %int0_4158, %true_4159 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4162 = torch.constant.float 1.000000e-05
    %int1_4163 = torch.constant.int 1
    %3562 = torch.aten.add.Scalar %result0_4160, %float1.000000e-05_4162, %int1_4163 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3563 = torch.aten.rsqrt %3562 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4164 = torch.constant.int 1
    %3564 = torch.aten.sub.Tensor %3559, %result1_4161, %int1_4164 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3565 = torch.aten.mul.Tensor %3564, %3563 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %3566 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3567 = torch.aten.mul.Tensor %3565, %3566 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %3568 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4165 = torch.constant.int 1
    %3569 = torch.aten.add.Tensor %3567, %3568, %int1_4165 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4166 = torch.constant.int 5
    %3570 = torch.prims.convert_element_type %3569, %int5_4166 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %3571 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4167 = torch.constant.int 0
    %int1_4168 = torch.constant.int 1
    %3572 = torch.aten.transpose.int %3571, %int0_4167, %int1_4168 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4169 = torch.constant.int 1920
    %int1280_4170 = torch.constant.int 1280
    %3573 = torch.prim.ListConstruct %int1920_4169, %int1280_4170 : (!torch.int, !torch.int) -> !torch.list<int>
    %3574 = torch.aten.view %3570, %3573 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3575 = torch.aten.mm %3574, %3572 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4171 = torch.constant.int 2
    %int960_4172 = torch.constant.int 960
    %int1280_4173 = torch.constant.int 1280
    %3576 = torch.prim.ListConstruct %int2_4171, %int960_4172, %int1280_4173 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3577 = torch.aten.view %3575, %3576 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %3578 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4174 = torch.constant.int 0
    %int1_4175 = torch.constant.int 1
    %3579 = torch.aten.transpose.int %3578, %int0_4174, %int1_4175 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4176 = torch.constant.int 32
    %int2048_4177 = torch.constant.int 2048
    %3580 = torch.prim.ListConstruct %int32_4176, %int2048_4177 : (!torch.int, !torch.int) -> !torch.list<int>
    %3581 = torch.aten.view %arg6, %3580 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3582 = torch.aten.mm %3581, %3579 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4178 = torch.constant.int 2
    %int16_4179 = torch.constant.int 16
    %int1280_4180 = torch.constant.int 1280
    %3583 = torch.prim.ListConstruct %int2_4178, %int16_4179, %int1280_4180 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3584 = torch.aten.view %3582, %3583 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %3585 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4181 = torch.constant.int 0
    %int1_4182 = torch.constant.int 1
    %3586 = torch.aten.transpose.int %3585, %int0_4181, %int1_4182 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4183 = torch.constant.int 32
    %int2048_4184 = torch.constant.int 2048
    %3587 = torch.prim.ListConstruct %int32_4183, %int2048_4184 : (!torch.int, !torch.int) -> !torch.list<int>
    %3588 = torch.aten.view %arg6, %3587 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3589 = torch.aten.mm %3588, %3586 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4185 = torch.constant.int 2
    %int16_4186 = torch.constant.int 16
    %int1280_4187 = torch.constant.int 1280
    %3590 = torch.prim.ListConstruct %int2_4185, %int16_4186, %int1280_4187 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3591 = torch.aten.view %3589, %3590 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_4188 = torch.constant.int 2
    %int-1_4189 = torch.constant.int -1
    %int20_4190 = torch.constant.int 20
    %int64_4191 = torch.constant.int 64
    %3592 = torch.prim.ListConstruct %int2_4188, %int-1_4189, %int20_4190, %int64_4191 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3593 = torch.aten.view %3577, %3592 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4192 = torch.constant.int 1
    %int2_4193 = torch.constant.int 2
    %3594 = torch.aten.transpose.int %3593, %int1_4192, %int2_4193 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4194 = torch.constant.int 2
    %int-1_4195 = torch.constant.int -1
    %int20_4196 = torch.constant.int 20
    %int64_4197 = torch.constant.int 64
    %3595 = torch.prim.ListConstruct %int2_4194, %int-1_4195, %int20_4196, %int64_4197 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3596 = torch.aten.view %3584, %3595 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4198 = torch.constant.int 1
    %int2_4199 = torch.constant.int 2
    %3597 = torch.aten.transpose.int %3596, %int1_4198, %int2_4199 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_4200 = torch.constant.int 2
    %int-1_4201 = torch.constant.int -1
    %int20_4202 = torch.constant.int 20
    %int64_4203 = torch.constant.int 64
    %3598 = torch.prim.ListConstruct %int2_4200, %int-1_4201, %int20_4202, %int64_4203 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3599 = torch.aten.view %3591, %3598 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4204 = torch.constant.int 1
    %int2_4205 = torch.constant.int 2
    %3600 = torch.aten.transpose.int %3599, %int1_4204, %int2_4205 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_4206 = torch.constant.float 0.000000e+00
    %false_4207 = torch.constant.bool false
    %none_4208 = torch.constant.none
    %none_4209 = torch.constant.none
    %3601:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3594, %3597, %3600, %float0.000000e00_4206, %false_4207, %none_4208, %none_4209) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4210 = torch.constant.int 1
    %int2_4211 = torch.constant.int 2
    %3602 = torch.aten.transpose.int %3601#0, %int1_4210, %int2_4211 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4212 = torch.constant.int 2
    %int-1_4213 = torch.constant.int -1
    %int1280_4214 = torch.constant.int 1280
    %3603 = torch.prim.ListConstruct %int2_4212, %int-1_4213, %int1280_4214 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3604 = torch.aten.view %3602, %3603 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4215 = torch.constant.int 5
    %3605 = torch.prims.convert_element_type %3604, %int5_4215 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4216 = torch.constant.int 1920
    %int1280_4217 = torch.constant.int 1280
    %3606 = torch.prim.ListConstruct %int1920_4216, %int1280_4217 : (!torch.int, !torch.int) -> !torch.list<int>
    %3607 = torch.aten.view %3605, %3606 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3608 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4218 = torch.constant.int 0
    %int1_4219 = torch.constant.int 1
    %3609 = torch.aten.transpose.int %3608, %int0_4218, %int1_4219 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %3610 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4220 = torch.constant.int 6
    %3611 = torch.prims.convert_element_type %3610, %int6_4220 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4221 = torch.constant.int 6
    %3612 = torch.prims.convert_element_type %3607, %int6_4221 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4222 = torch.constant.int 6
    %3613 = torch.prims.convert_element_type %3609, %int6_4222 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3614 = torch.aten.mm %3612, %3613 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4223 = torch.constant.int 1
    %3615 = torch.aten.mul.Scalar %3614, %int1_4223 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4224 = torch.constant.int 1
    %3616 = torch.aten.mul.Scalar %3611, %int1_4224 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4225 = torch.constant.int 1
    %3617 = torch.aten.add.Tensor %3615, %3616, %int1_4225 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4226 = torch.constant.int 5
    %3618 = torch.prims.convert_element_type %3617, %int5_4226 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4227 = torch.constant.int 2
    %int960_4228 = torch.constant.int 960
    %int1280_4229 = torch.constant.int 1280
    %3619 = torch.prim.ListConstruct %int2_4227, %int960_4228, %int1280_4229 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3620 = torch.aten.view %3618, %3619 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4230 = torch.constant.none
    %3621 = torch.aten.clone %3620, %none_4230 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4231 = torch.constant.float 1.000000e+00
    %3622 = torch.aten.div.Scalar %3621, %float1.000000e00_4231 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4232 = torch.constant.int 1
    %3623 = torch.aten.add.Tensor %3622, %3559, %int1_4232 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4233 = torch.constant.int 6
    %3624 = torch.prims.convert_element_type %3623, %int6_4233 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4234 = torch.constant.int 2
    %3625 = torch.prim.ListConstruct %int2_4234 : (!torch.int) -> !torch.list<int>
    %int0_4235 = torch.constant.int 0
    %true_4236 = torch.constant.bool true
    %result0_4237, %result1_4238 = torch.aten.var_mean.correction %3624, %3625, %int0_4235, %true_4236 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4239 = torch.constant.float 1.000000e-05
    %int1_4240 = torch.constant.int 1
    %3626 = torch.aten.add.Scalar %result0_4237, %float1.000000e-05_4239, %int1_4240 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3627 = torch.aten.rsqrt %3626 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4241 = torch.constant.int 1
    %3628 = torch.aten.sub.Tensor %3623, %result1_4238, %int1_4241 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3629 = torch.aten.mul.Tensor %3628, %3627 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %3630 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3631 = torch.aten.mul.Tensor %3629, %3630 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %3632 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4242 = torch.constant.int 1
    %3633 = torch.aten.add.Tensor %3631, %3632, %int1_4242 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4243 = torch.constant.int 5
    %3634 = torch.prims.convert_element_type %3633, %int5_4243 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4244 = torch.constant.int 1920
    %int1280_4245 = torch.constant.int 1280
    %3635 = torch.prim.ListConstruct %int1920_4244, %int1280_4245 : (!torch.int, !torch.int) -> !torch.list<int>
    %3636 = torch.aten.view %3634, %3635 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3637 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_4246 = torch.constant.int 0
    %int1_4247 = torch.constant.int 1
    %3638 = torch.aten.transpose.int %3637, %int0_4246, %int1_4247 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %3639 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_4248 = torch.constant.int 6
    %3640 = torch.prims.convert_element_type %3639, %int6_4248 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_4249 = torch.constant.int 6
    %3641 = torch.prims.convert_element_type %3636, %int6_4249 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4250 = torch.constant.int 6
    %3642 = torch.prims.convert_element_type %3638, %int6_4250 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3643 = torch.aten.mm %3641, %3642 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_4251 = torch.constant.int 1
    %3644 = torch.aten.mul.Scalar %3643, %int1_4251 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_4252 = torch.constant.int 1
    %3645 = torch.aten.mul.Scalar %3640, %int1_4252 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_4253 = torch.constant.int 1
    %3646 = torch.aten.add.Tensor %3644, %3645, %int1_4253 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_4254 = torch.constant.int 5
    %3647 = torch.prims.convert_element_type %3646, %int5_4254 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_4255 = torch.constant.int 2
    %int960_4256 = torch.constant.int 960
    %int10240_4257 = torch.constant.int 10240
    %3648 = torch.prim.ListConstruct %int2_4255, %int960_4256, %int10240_4257 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3649 = torch.aten.view %3647, %3648 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_4258 = torch.constant.int -1
    %int0_4259 = torch.constant.int 0
    %int5120_4260 = torch.constant.int 5120
    %int1_4261 = torch.constant.int 1
    %3650 = torch.aten.slice.Tensor %3649, %int-1_4258, %int0_4259, %int5120_4260, %int1_4261 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_4262 = torch.constant.int -1
    %int5120_4263 = torch.constant.int 5120
    %int10240_4264 = torch.constant.int 10240
    %int1_4265 = torch.constant.int 1
    %3651 = torch.aten.slice.Tensor %3649, %int-1_4262, %int5120_4263, %int10240_4264, %int1_4265 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_4266 = torch.constant.str "none"
    %3652 = torch.aten.gelu %3651, %str_4266 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %3653 = torch.aten.mul.Tensor %3650, %3652 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_4267 = torch.constant.none
    %3654 = torch.aten.clone %3653, %none_4267 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_4268 = torch.constant.int 1920
    %int5120_4269 = torch.constant.int 5120
    %3655 = torch.prim.ListConstruct %int1920_4268, %int5120_4269 : (!torch.int, !torch.int) -> !torch.list<int>
    %3656 = torch.aten.view %3654, %3655 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %3657 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4270 = torch.constant.int 0
    %int1_4271 = torch.constant.int 1
    %3658 = torch.aten.transpose.int %3657, %int0_4270, %int1_4271 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %3659 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4272 = torch.constant.int 6
    %3660 = torch.prims.convert_element_type %3659, %int6_4272 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4273 = torch.constant.int 6
    %3661 = torch.prims.convert_element_type %3656, %int6_4273 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_4274 = torch.constant.int 6
    %3662 = torch.prims.convert_element_type %3658, %int6_4274 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3663 = torch.aten.mm %3661, %3662 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4275 = torch.constant.int 1
    %3664 = torch.aten.mul.Scalar %3663, %int1_4275 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4276 = torch.constant.int 1
    %3665 = torch.aten.mul.Scalar %3660, %int1_4276 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4277 = torch.constant.int 1
    %3666 = torch.aten.add.Tensor %3664, %3665, %int1_4277 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4278 = torch.constant.int 5
    %3667 = torch.prims.convert_element_type %3666, %int5_4278 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4279 = torch.constant.int 2
    %int960_4280 = torch.constant.int 960
    %int1280_4281 = torch.constant.int 1280
    %3668 = torch.prim.ListConstruct %int2_4279, %int960_4280, %int1280_4281 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3669 = torch.aten.view %3667, %3668 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_4282 = torch.constant.int 1
    %3670 = torch.aten.add.Tensor %3669, %3623, %int1_4282 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4283 = torch.constant.int 6
    %3671 = torch.prims.convert_element_type %3670, %int6_4283 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4284 = torch.constant.int 2
    %3672 = torch.prim.ListConstruct %int2_4284 : (!torch.int) -> !torch.list<int>
    %int0_4285 = torch.constant.int 0
    %true_4286 = torch.constant.bool true
    %result0_4287, %result1_4288 = torch.aten.var_mean.correction %3671, %3672, %int0_4285, %true_4286 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4289 = torch.constant.float 1.000000e-05
    %int1_4290 = torch.constant.int 1
    %3673 = torch.aten.add.Scalar %result0_4287, %float1.000000e-05_4289, %int1_4290 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3674 = torch.aten.rsqrt %3673 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4291 = torch.constant.int 1
    %3675 = torch.aten.sub.Tensor %3670, %result1_4288, %int1_4291 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3676 = torch.aten.mul.Tensor %3675, %3674 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %3677 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3678 = torch.aten.mul.Tensor %3676, %3677 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %3679 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4292 = torch.constant.int 1
    %3680 = torch.aten.add.Tensor %3678, %3679, %int1_4292 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4293 = torch.constant.int 5
    %3681 = torch.prims.convert_element_type %3680, %int5_4293 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %3682 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4294 = torch.constant.int 0
    %int1_4295 = torch.constant.int 1
    %3683 = torch.aten.transpose.int %3682, %int0_4294, %int1_4295 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4296 = torch.constant.int 1920
    %int1280_4297 = torch.constant.int 1280
    %3684 = torch.prim.ListConstruct %int1920_4296, %int1280_4297 : (!torch.int, !torch.int) -> !torch.list<int>
    %3685 = torch.aten.view %3681, %3684 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3686 = torch.aten.mm %3685, %3683 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4298 = torch.constant.int 2
    %int960_4299 = torch.constant.int 960
    %int1280_4300 = torch.constant.int 1280
    %3687 = torch.prim.ListConstruct %int2_4298, %int960_4299, %int1280_4300 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3688 = torch.aten.view %3686, %3687 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %3689 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4301 = torch.constant.int 0
    %int1_4302 = torch.constant.int 1
    %3690 = torch.aten.transpose.int %3689, %int0_4301, %int1_4302 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4303 = torch.constant.int 1920
    %int1280_4304 = torch.constant.int 1280
    %3691 = torch.prim.ListConstruct %int1920_4303, %int1280_4304 : (!torch.int, !torch.int) -> !torch.list<int>
    %3692 = torch.aten.view %3681, %3691 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3693 = torch.aten.mm %3692, %3690 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4305 = torch.constant.int 2
    %int960_4306 = torch.constant.int 960
    %int1280_4307 = torch.constant.int 1280
    %3694 = torch.prim.ListConstruct %int2_4305, %int960_4306, %int1280_4307 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3695 = torch.aten.view %3693, %3694 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %3696 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4308 = torch.constant.int 0
    %int1_4309 = torch.constant.int 1
    %3697 = torch.aten.transpose.int %3696, %int0_4308, %int1_4309 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4310 = torch.constant.int 1920
    %int1280_4311 = torch.constant.int 1280
    %3698 = torch.prim.ListConstruct %int1920_4310, %int1280_4311 : (!torch.int, !torch.int) -> !torch.list<int>
    %3699 = torch.aten.view %3681, %3698 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3700 = torch.aten.mm %3699, %3697 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4312 = torch.constant.int 2
    %int960_4313 = torch.constant.int 960
    %int1280_4314 = torch.constant.int 1280
    %3701 = torch.prim.ListConstruct %int2_4312, %int960_4313, %int1280_4314 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3702 = torch.aten.view %3700, %3701 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_4315 = torch.constant.int 2
    %int-1_4316 = torch.constant.int -1
    %int20_4317 = torch.constant.int 20
    %int64_4318 = torch.constant.int 64
    %3703 = torch.prim.ListConstruct %int2_4315, %int-1_4316, %int20_4317, %int64_4318 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3704 = torch.aten.view %3688, %3703 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4319 = torch.constant.int 1
    %int2_4320 = torch.constant.int 2
    %3705 = torch.aten.transpose.int %3704, %int1_4319, %int2_4320 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4321 = torch.constant.int 2
    %int-1_4322 = torch.constant.int -1
    %int20_4323 = torch.constant.int 20
    %int64_4324 = torch.constant.int 64
    %3706 = torch.prim.ListConstruct %int2_4321, %int-1_4322, %int20_4323, %int64_4324 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3707 = torch.aten.view %3695, %3706 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4325 = torch.constant.int 1
    %int2_4326 = torch.constant.int 2
    %3708 = torch.aten.transpose.int %3707, %int1_4325, %int2_4326 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4327 = torch.constant.int 2
    %int-1_4328 = torch.constant.int -1
    %int20_4329 = torch.constant.int 20
    %int64_4330 = torch.constant.int 64
    %3709 = torch.prim.ListConstruct %int2_4327, %int-1_4328, %int20_4329, %int64_4330 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3710 = torch.aten.view %3702, %3709 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4331 = torch.constant.int 1
    %int2_4332 = torch.constant.int 2
    %3711 = torch.aten.transpose.int %3710, %int1_4331, %int2_4332 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_4333 = torch.constant.float 0.000000e+00
    %false_4334 = torch.constant.bool false
    %none_4335 = torch.constant.none
    %none_4336 = torch.constant.none
    %3712:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3705, %3708, %3711, %float0.000000e00_4333, %false_4334, %none_4335, %none_4336) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4337 = torch.constant.int 1
    %int2_4338 = torch.constant.int 2
    %3713 = torch.aten.transpose.int %3712#0, %int1_4337, %int2_4338 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4339 = torch.constant.int 2
    %int-1_4340 = torch.constant.int -1
    %int1280_4341 = torch.constant.int 1280
    %3714 = torch.prim.ListConstruct %int2_4339, %int-1_4340, %int1280_4341 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3715 = torch.aten.view %3713, %3714 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4342 = torch.constant.int 5
    %3716 = torch.prims.convert_element_type %3715, %int5_4342 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4343 = torch.constant.int 1920
    %int1280_4344 = torch.constant.int 1280
    %3717 = torch.prim.ListConstruct %int1920_4343, %int1280_4344 : (!torch.int, !torch.int) -> !torch.list<int>
    %3718 = torch.aten.view %3716, %3717 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3719 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4345 = torch.constant.int 0
    %int1_4346 = torch.constant.int 1
    %3720 = torch.aten.transpose.int %3719, %int0_4345, %int1_4346 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %3721 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4347 = torch.constant.int 6
    %3722 = torch.prims.convert_element_type %3721, %int6_4347 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4348 = torch.constant.int 6
    %3723 = torch.prims.convert_element_type %3718, %int6_4348 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4349 = torch.constant.int 6
    %3724 = torch.prims.convert_element_type %3720, %int6_4349 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3725 = torch.aten.mm %3723, %3724 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4350 = torch.constant.int 1
    %3726 = torch.aten.mul.Scalar %3725, %int1_4350 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4351 = torch.constant.int 1
    %3727 = torch.aten.mul.Scalar %3722, %int1_4351 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4352 = torch.constant.int 1
    %3728 = torch.aten.add.Tensor %3726, %3727, %int1_4352 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4353 = torch.constant.int 5
    %3729 = torch.prims.convert_element_type %3728, %int5_4353 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4354 = torch.constant.int 2
    %int960_4355 = torch.constant.int 960
    %int1280_4356 = torch.constant.int 1280
    %3730 = torch.prim.ListConstruct %int2_4354, %int960_4355, %int1280_4356 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3731 = torch.aten.view %3729, %3730 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4357 = torch.constant.none
    %3732 = torch.aten.clone %3731, %none_4357 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4358 = torch.constant.float 1.000000e+00
    %3733 = torch.aten.div.Scalar %3732, %float1.000000e00_4358 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4359 = torch.constant.int 1
    %3734 = torch.aten.add.Tensor %3733, %3670, %int1_4359 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4360 = torch.constant.int 6
    %3735 = torch.prims.convert_element_type %3734, %int6_4360 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4361 = torch.constant.int 2
    %3736 = torch.prim.ListConstruct %int2_4361 : (!torch.int) -> !torch.list<int>
    %int0_4362 = torch.constant.int 0
    %true_4363 = torch.constant.bool true
    %result0_4364, %result1_4365 = torch.aten.var_mean.correction %3735, %3736, %int0_4362, %true_4363 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4366 = torch.constant.float 1.000000e-05
    %int1_4367 = torch.constant.int 1
    %3737 = torch.aten.add.Scalar %result0_4364, %float1.000000e-05_4366, %int1_4367 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3738 = torch.aten.rsqrt %3737 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4368 = torch.constant.int 1
    %3739 = torch.aten.sub.Tensor %3734, %result1_4365, %int1_4368 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3740 = torch.aten.mul.Tensor %3739, %3738 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %3741 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3742 = torch.aten.mul.Tensor %3740, %3741 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %3743 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4369 = torch.constant.int 1
    %3744 = torch.aten.add.Tensor %3742, %3743, %int1_4369 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4370 = torch.constant.int 5
    %3745 = torch.prims.convert_element_type %3744, %int5_4370 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %3746 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4371 = torch.constant.int 0
    %int1_4372 = torch.constant.int 1
    %3747 = torch.aten.transpose.int %3746, %int0_4371, %int1_4372 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4373 = torch.constant.int 1920
    %int1280_4374 = torch.constant.int 1280
    %3748 = torch.prim.ListConstruct %int1920_4373, %int1280_4374 : (!torch.int, !torch.int) -> !torch.list<int>
    %3749 = torch.aten.view %3745, %3748 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3750 = torch.aten.mm %3749, %3747 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4375 = torch.constant.int 2
    %int960_4376 = torch.constant.int 960
    %int1280_4377 = torch.constant.int 1280
    %3751 = torch.prim.ListConstruct %int2_4375, %int960_4376, %int1280_4377 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3752 = torch.aten.view %3750, %3751 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %3753 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4378 = torch.constant.int 0
    %int1_4379 = torch.constant.int 1
    %3754 = torch.aten.transpose.int %3753, %int0_4378, %int1_4379 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4380 = torch.constant.int 32
    %int2048_4381 = torch.constant.int 2048
    %3755 = torch.prim.ListConstruct %int32_4380, %int2048_4381 : (!torch.int, !torch.int) -> !torch.list<int>
    %3756 = torch.aten.view %arg6, %3755 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3757 = torch.aten.mm %3756, %3754 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4382 = torch.constant.int 2
    %int16_4383 = torch.constant.int 16
    %int1280_4384 = torch.constant.int 1280
    %3758 = torch.prim.ListConstruct %int2_4382, %int16_4383, %int1280_4384 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3759 = torch.aten.view %3757, %3758 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %3760 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4385 = torch.constant.int 0
    %int1_4386 = torch.constant.int 1
    %3761 = torch.aten.transpose.int %3760, %int0_4385, %int1_4386 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4387 = torch.constant.int 32
    %int2048_4388 = torch.constant.int 2048
    %3762 = torch.prim.ListConstruct %int32_4387, %int2048_4388 : (!torch.int, !torch.int) -> !torch.list<int>
    %3763 = torch.aten.view %arg6, %3762 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3764 = torch.aten.mm %3763, %3761 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4389 = torch.constant.int 2
    %int16_4390 = torch.constant.int 16
    %int1280_4391 = torch.constant.int 1280
    %3765 = torch.prim.ListConstruct %int2_4389, %int16_4390, %int1280_4391 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3766 = torch.aten.view %3764, %3765 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_4392 = torch.constant.int 2
    %int-1_4393 = torch.constant.int -1
    %int20_4394 = torch.constant.int 20
    %int64_4395 = torch.constant.int 64
    %3767 = torch.prim.ListConstruct %int2_4392, %int-1_4393, %int20_4394, %int64_4395 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3768 = torch.aten.view %3752, %3767 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4396 = torch.constant.int 1
    %int2_4397 = torch.constant.int 2
    %3769 = torch.aten.transpose.int %3768, %int1_4396, %int2_4397 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4398 = torch.constant.int 2
    %int-1_4399 = torch.constant.int -1
    %int20_4400 = torch.constant.int 20
    %int64_4401 = torch.constant.int 64
    %3770 = torch.prim.ListConstruct %int2_4398, %int-1_4399, %int20_4400, %int64_4401 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3771 = torch.aten.view %3759, %3770 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4402 = torch.constant.int 1
    %int2_4403 = torch.constant.int 2
    %3772 = torch.aten.transpose.int %3771, %int1_4402, %int2_4403 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_4404 = torch.constant.int 2
    %int-1_4405 = torch.constant.int -1
    %int20_4406 = torch.constant.int 20
    %int64_4407 = torch.constant.int 64
    %3773 = torch.prim.ListConstruct %int2_4404, %int-1_4405, %int20_4406, %int64_4407 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3774 = torch.aten.view %3766, %3773 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4408 = torch.constant.int 1
    %int2_4409 = torch.constant.int 2
    %3775 = torch.aten.transpose.int %3774, %int1_4408, %int2_4409 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_4410 = torch.constant.float 0.000000e+00
    %false_4411 = torch.constant.bool false
    %none_4412 = torch.constant.none
    %none_4413 = torch.constant.none
    %3776:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3769, %3772, %3775, %float0.000000e00_4410, %false_4411, %none_4412, %none_4413) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4414 = torch.constant.int 1
    %int2_4415 = torch.constant.int 2
    %3777 = torch.aten.transpose.int %3776#0, %int1_4414, %int2_4415 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4416 = torch.constant.int 2
    %int-1_4417 = torch.constant.int -1
    %int1280_4418 = torch.constant.int 1280
    %3778 = torch.prim.ListConstruct %int2_4416, %int-1_4417, %int1280_4418 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3779 = torch.aten.view %3777, %3778 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4419 = torch.constant.int 5
    %3780 = torch.prims.convert_element_type %3779, %int5_4419 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4420 = torch.constant.int 1920
    %int1280_4421 = torch.constant.int 1280
    %3781 = torch.prim.ListConstruct %int1920_4420, %int1280_4421 : (!torch.int, !torch.int) -> !torch.list<int>
    %3782 = torch.aten.view %3780, %3781 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3783 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4422 = torch.constant.int 0
    %int1_4423 = torch.constant.int 1
    %3784 = torch.aten.transpose.int %3783, %int0_4422, %int1_4423 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %3785 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4424 = torch.constant.int 6
    %3786 = torch.prims.convert_element_type %3785, %int6_4424 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4425 = torch.constant.int 6
    %3787 = torch.prims.convert_element_type %3782, %int6_4425 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4426 = torch.constant.int 6
    %3788 = torch.prims.convert_element_type %3784, %int6_4426 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3789 = torch.aten.mm %3787, %3788 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4427 = torch.constant.int 1
    %3790 = torch.aten.mul.Scalar %3789, %int1_4427 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4428 = torch.constant.int 1
    %3791 = torch.aten.mul.Scalar %3786, %int1_4428 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4429 = torch.constant.int 1
    %3792 = torch.aten.add.Tensor %3790, %3791, %int1_4429 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4430 = torch.constant.int 5
    %3793 = torch.prims.convert_element_type %3792, %int5_4430 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4431 = torch.constant.int 2
    %int960_4432 = torch.constant.int 960
    %int1280_4433 = torch.constant.int 1280
    %3794 = torch.prim.ListConstruct %int2_4431, %int960_4432, %int1280_4433 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3795 = torch.aten.view %3793, %3794 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4434 = torch.constant.none
    %3796 = torch.aten.clone %3795, %none_4434 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4435 = torch.constant.float 1.000000e+00
    %3797 = torch.aten.div.Scalar %3796, %float1.000000e00_4435 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4436 = torch.constant.int 1
    %3798 = torch.aten.add.Tensor %3797, %3734, %int1_4436 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4437 = torch.constant.int 6
    %3799 = torch.prims.convert_element_type %3798, %int6_4437 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4438 = torch.constant.int 2
    %3800 = torch.prim.ListConstruct %int2_4438 : (!torch.int) -> !torch.list<int>
    %int0_4439 = torch.constant.int 0
    %true_4440 = torch.constant.bool true
    %result0_4441, %result1_4442 = torch.aten.var_mean.correction %3799, %3800, %int0_4439, %true_4440 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4443 = torch.constant.float 1.000000e-05
    %int1_4444 = torch.constant.int 1
    %3801 = torch.aten.add.Scalar %result0_4441, %float1.000000e-05_4443, %int1_4444 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3802 = torch.aten.rsqrt %3801 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4445 = torch.constant.int 1
    %3803 = torch.aten.sub.Tensor %3798, %result1_4442, %int1_4445 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3804 = torch.aten.mul.Tensor %3803, %3802 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %3805 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3806 = torch.aten.mul.Tensor %3804, %3805 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %3807 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4446 = torch.constant.int 1
    %3808 = torch.aten.add.Tensor %3806, %3807, %int1_4446 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4447 = torch.constant.int 5
    %3809 = torch.prims.convert_element_type %3808, %int5_4447 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4448 = torch.constant.int 1920
    %int1280_4449 = torch.constant.int 1280
    %3810 = torch.prim.ListConstruct %int1920_4448, %int1280_4449 : (!torch.int, !torch.int) -> !torch.list<int>
    %3811 = torch.aten.view %3809, %3810 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3812 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_4450 = torch.constant.int 0
    %int1_4451 = torch.constant.int 1
    %3813 = torch.aten.transpose.int %3812, %int0_4450, %int1_4451 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %3814 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_4452 = torch.constant.int 6
    %3815 = torch.prims.convert_element_type %3814, %int6_4452 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_4453 = torch.constant.int 6
    %3816 = torch.prims.convert_element_type %3811, %int6_4453 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4454 = torch.constant.int 6
    %3817 = torch.prims.convert_element_type %3813, %int6_4454 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3818 = torch.aten.mm %3816, %3817 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_4455 = torch.constant.int 1
    %3819 = torch.aten.mul.Scalar %3818, %int1_4455 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_4456 = torch.constant.int 1
    %3820 = torch.aten.mul.Scalar %3815, %int1_4456 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_4457 = torch.constant.int 1
    %3821 = torch.aten.add.Tensor %3819, %3820, %int1_4457 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_4458 = torch.constant.int 5
    %3822 = torch.prims.convert_element_type %3821, %int5_4458 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_4459 = torch.constant.int 2
    %int960_4460 = torch.constant.int 960
    %int10240_4461 = torch.constant.int 10240
    %3823 = torch.prim.ListConstruct %int2_4459, %int960_4460, %int10240_4461 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3824 = torch.aten.view %3822, %3823 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_4462 = torch.constant.int -1
    %int0_4463 = torch.constant.int 0
    %int5120_4464 = torch.constant.int 5120
    %int1_4465 = torch.constant.int 1
    %3825 = torch.aten.slice.Tensor %3824, %int-1_4462, %int0_4463, %int5120_4464, %int1_4465 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_4466 = torch.constant.int -1
    %int5120_4467 = torch.constant.int 5120
    %int10240_4468 = torch.constant.int 10240
    %int1_4469 = torch.constant.int 1
    %3826 = torch.aten.slice.Tensor %3824, %int-1_4466, %int5120_4467, %int10240_4468, %int1_4469 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_4470 = torch.constant.str "none"
    %3827 = torch.aten.gelu %3826, %str_4470 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %3828 = torch.aten.mul.Tensor %3825, %3827 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_4471 = torch.constant.none
    %3829 = torch.aten.clone %3828, %none_4471 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_4472 = torch.constant.int 1920
    %int5120_4473 = torch.constant.int 5120
    %3830 = torch.prim.ListConstruct %int1920_4472, %int5120_4473 : (!torch.int, !torch.int) -> !torch.list<int>
    %3831 = torch.aten.view %3829, %3830 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %3832 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4474 = torch.constant.int 0
    %int1_4475 = torch.constant.int 1
    %3833 = torch.aten.transpose.int %3832, %int0_4474, %int1_4475 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %3834 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4476 = torch.constant.int 6
    %3835 = torch.prims.convert_element_type %3834, %int6_4476 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4477 = torch.constant.int 6
    %3836 = torch.prims.convert_element_type %3831, %int6_4477 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_4478 = torch.constant.int 6
    %3837 = torch.prims.convert_element_type %3833, %int6_4478 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %3838 = torch.aten.mm %3836, %3837 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4479 = torch.constant.int 1
    %3839 = torch.aten.mul.Scalar %3838, %int1_4479 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4480 = torch.constant.int 1
    %3840 = torch.aten.mul.Scalar %3835, %int1_4480 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4481 = torch.constant.int 1
    %3841 = torch.aten.add.Tensor %3839, %3840, %int1_4481 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4482 = torch.constant.int 5
    %3842 = torch.prims.convert_element_type %3841, %int5_4482 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4483 = torch.constant.int 2
    %int960_4484 = torch.constant.int 960
    %int1280_4485 = torch.constant.int 1280
    %3843 = torch.prim.ListConstruct %int2_4483, %int960_4484, %int1280_4485 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3844 = torch.aten.view %3842, %3843 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_4486 = torch.constant.int 1
    %3845 = torch.aten.add.Tensor %3844, %3798, %int1_4486 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4487 = torch.constant.int 6
    %3846 = torch.prims.convert_element_type %3845, %int6_4487 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4488 = torch.constant.int 2
    %3847 = torch.prim.ListConstruct %int2_4488 : (!torch.int) -> !torch.list<int>
    %int0_4489 = torch.constant.int 0
    %true_4490 = torch.constant.bool true
    %result0_4491, %result1_4492 = torch.aten.var_mean.correction %3846, %3847, %int0_4489, %true_4490 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4493 = torch.constant.float 1.000000e-05
    %int1_4494 = torch.constant.int 1
    %3848 = torch.aten.add.Scalar %result0_4491, %float1.000000e-05_4493, %int1_4494 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3849 = torch.aten.rsqrt %3848 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4495 = torch.constant.int 1
    %3850 = torch.aten.sub.Tensor %3845, %result1_4492, %int1_4495 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3851 = torch.aten.mul.Tensor %3850, %3849 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %3852 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3853 = torch.aten.mul.Tensor %3851, %3852 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %3854 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4496 = torch.constant.int 1
    %3855 = torch.aten.add.Tensor %3853, %3854, %int1_4496 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4497 = torch.constant.int 5
    %3856 = torch.prims.convert_element_type %3855, %int5_4497 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %3857 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4498 = torch.constant.int 0
    %int1_4499 = torch.constant.int 1
    %3858 = torch.aten.transpose.int %3857, %int0_4498, %int1_4499 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4500 = torch.constant.int 1920
    %int1280_4501 = torch.constant.int 1280
    %3859 = torch.prim.ListConstruct %int1920_4500, %int1280_4501 : (!torch.int, !torch.int) -> !torch.list<int>
    %3860 = torch.aten.view %3856, %3859 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3861 = torch.aten.mm %3860, %3858 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4502 = torch.constant.int 2
    %int960_4503 = torch.constant.int 960
    %int1280_4504 = torch.constant.int 1280
    %3862 = torch.prim.ListConstruct %int2_4502, %int960_4503, %int1280_4504 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3863 = torch.aten.view %3861, %3862 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %3864 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4505 = torch.constant.int 0
    %int1_4506 = torch.constant.int 1
    %3865 = torch.aten.transpose.int %3864, %int0_4505, %int1_4506 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4507 = torch.constant.int 1920
    %int1280_4508 = torch.constant.int 1280
    %3866 = torch.prim.ListConstruct %int1920_4507, %int1280_4508 : (!torch.int, !torch.int) -> !torch.list<int>
    %3867 = torch.aten.view %3856, %3866 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3868 = torch.aten.mm %3867, %3865 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4509 = torch.constant.int 2
    %int960_4510 = torch.constant.int 960
    %int1280_4511 = torch.constant.int 1280
    %3869 = torch.prim.ListConstruct %int2_4509, %int960_4510, %int1280_4511 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3870 = torch.aten.view %3868, %3869 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %3871 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4512 = torch.constant.int 0
    %int1_4513 = torch.constant.int 1
    %3872 = torch.aten.transpose.int %3871, %int0_4512, %int1_4513 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4514 = torch.constant.int 1920
    %int1280_4515 = torch.constant.int 1280
    %3873 = torch.prim.ListConstruct %int1920_4514, %int1280_4515 : (!torch.int, !torch.int) -> !torch.list<int>
    %3874 = torch.aten.view %3856, %3873 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3875 = torch.aten.mm %3874, %3872 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4516 = torch.constant.int 2
    %int960_4517 = torch.constant.int 960
    %int1280_4518 = torch.constant.int 1280
    %3876 = torch.prim.ListConstruct %int2_4516, %int960_4517, %int1280_4518 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3877 = torch.aten.view %3875, %3876 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_4519 = torch.constant.int 2
    %int-1_4520 = torch.constant.int -1
    %int20_4521 = torch.constant.int 20
    %int64_4522 = torch.constant.int 64
    %3878 = torch.prim.ListConstruct %int2_4519, %int-1_4520, %int20_4521, %int64_4522 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3879 = torch.aten.view %3863, %3878 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4523 = torch.constant.int 1
    %int2_4524 = torch.constant.int 2
    %3880 = torch.aten.transpose.int %3879, %int1_4523, %int2_4524 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4525 = torch.constant.int 2
    %int-1_4526 = torch.constant.int -1
    %int20_4527 = torch.constant.int 20
    %int64_4528 = torch.constant.int 64
    %3881 = torch.prim.ListConstruct %int2_4525, %int-1_4526, %int20_4527, %int64_4528 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3882 = torch.aten.view %3870, %3881 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4529 = torch.constant.int 1
    %int2_4530 = torch.constant.int 2
    %3883 = torch.aten.transpose.int %3882, %int1_4529, %int2_4530 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4531 = torch.constant.int 2
    %int-1_4532 = torch.constant.int -1
    %int20_4533 = torch.constant.int 20
    %int64_4534 = torch.constant.int 64
    %3884 = torch.prim.ListConstruct %int2_4531, %int-1_4532, %int20_4533, %int64_4534 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3885 = torch.aten.view %3877, %3884 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4535 = torch.constant.int 1
    %int2_4536 = torch.constant.int 2
    %3886 = torch.aten.transpose.int %3885, %int1_4535, %int2_4536 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_4537 = torch.constant.float 0.000000e+00
    %false_4538 = torch.constant.bool false
    %none_4539 = torch.constant.none
    %none_4540 = torch.constant.none
    %3887:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3880, %3883, %3886, %float0.000000e00_4537, %false_4538, %none_4539, %none_4540) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4541 = torch.constant.int 1
    %int2_4542 = torch.constant.int 2
    %3888 = torch.aten.transpose.int %3887#0, %int1_4541, %int2_4542 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4543 = torch.constant.int 2
    %int-1_4544 = torch.constant.int -1
    %int1280_4545 = torch.constant.int 1280
    %3889 = torch.prim.ListConstruct %int2_4543, %int-1_4544, %int1280_4545 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3890 = torch.aten.view %3888, %3889 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4546 = torch.constant.int 5
    %3891 = torch.prims.convert_element_type %3890, %int5_4546 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4547 = torch.constant.int 1920
    %int1280_4548 = torch.constant.int 1280
    %3892 = torch.prim.ListConstruct %int1920_4547, %int1280_4548 : (!torch.int, !torch.int) -> !torch.list<int>
    %3893 = torch.aten.view %3891, %3892 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %3894 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4549 = torch.constant.int 0
    %int1_4550 = torch.constant.int 1
    %3895 = torch.aten.transpose.int %3894, %int0_4549, %int1_4550 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %3896 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4551 = torch.constant.int 6
    %3897 = torch.prims.convert_element_type %3896, %int6_4551 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4552 = torch.constant.int 6
    %3898 = torch.prims.convert_element_type %3893, %int6_4552 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4553 = torch.constant.int 6
    %3899 = torch.prims.convert_element_type %3895, %int6_4553 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3900 = torch.aten.mm %3898, %3899 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4554 = torch.constant.int 1
    %3901 = torch.aten.mul.Scalar %3900, %int1_4554 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4555 = torch.constant.int 1
    %3902 = torch.aten.mul.Scalar %3897, %int1_4555 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4556 = torch.constant.int 1
    %3903 = torch.aten.add.Tensor %3901, %3902, %int1_4556 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4557 = torch.constant.int 5
    %3904 = torch.prims.convert_element_type %3903, %int5_4557 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4558 = torch.constant.int 2
    %int960_4559 = torch.constant.int 960
    %int1280_4560 = torch.constant.int 1280
    %3905 = torch.prim.ListConstruct %int2_4558, %int960_4559, %int1280_4560 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3906 = torch.aten.view %3904, %3905 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4561 = torch.constant.none
    %3907 = torch.aten.clone %3906, %none_4561 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4562 = torch.constant.float 1.000000e+00
    %3908 = torch.aten.div.Scalar %3907, %float1.000000e00_4562 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4563 = torch.constant.int 1
    %3909 = torch.aten.add.Tensor %3908, %3845, %int1_4563 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4564 = torch.constant.int 6
    %3910 = torch.prims.convert_element_type %3909, %int6_4564 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4565 = torch.constant.int 2
    %3911 = torch.prim.ListConstruct %int2_4565 : (!torch.int) -> !torch.list<int>
    %int0_4566 = torch.constant.int 0
    %true_4567 = torch.constant.bool true
    %result0_4568, %result1_4569 = torch.aten.var_mean.correction %3910, %3911, %int0_4566, %true_4567 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4570 = torch.constant.float 1.000000e-05
    %int1_4571 = torch.constant.int 1
    %3912 = torch.aten.add.Scalar %result0_4568, %float1.000000e-05_4570, %int1_4571 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3913 = torch.aten.rsqrt %3912 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4572 = torch.constant.int 1
    %3914 = torch.aten.sub.Tensor %3909, %result1_4569, %int1_4572 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3915 = torch.aten.mul.Tensor %3914, %3913 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %3916 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3917 = torch.aten.mul.Tensor %3915, %3916 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %3918 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4573 = torch.constant.int 1
    %3919 = torch.aten.add.Tensor %3917, %3918, %int1_4573 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4574 = torch.constant.int 5
    %3920 = torch.prims.convert_element_type %3919, %int5_4574 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %3921 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4575 = torch.constant.int 0
    %int1_4576 = torch.constant.int 1
    %3922 = torch.aten.transpose.int %3921, %int0_4575, %int1_4576 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4577 = torch.constant.int 1920
    %int1280_4578 = torch.constant.int 1280
    %3923 = torch.prim.ListConstruct %int1920_4577, %int1280_4578 : (!torch.int, !torch.int) -> !torch.list<int>
    %3924 = torch.aten.view %3920, %3923 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %3925 = torch.aten.mm %3924, %3922 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4579 = torch.constant.int 2
    %int960_4580 = torch.constant.int 960
    %int1280_4581 = torch.constant.int 1280
    %3926 = torch.prim.ListConstruct %int2_4579, %int960_4580, %int1280_4581 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3927 = torch.aten.view %3925, %3926 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %3928 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4582 = torch.constant.int 0
    %int1_4583 = torch.constant.int 1
    %3929 = torch.aten.transpose.int %3928, %int0_4582, %int1_4583 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4584 = torch.constant.int 32
    %int2048_4585 = torch.constant.int 2048
    %3930 = torch.prim.ListConstruct %int32_4584, %int2048_4585 : (!torch.int, !torch.int) -> !torch.list<int>
    %3931 = torch.aten.view %arg6, %3930 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3932 = torch.aten.mm %3931, %3929 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4586 = torch.constant.int 2
    %int16_4587 = torch.constant.int 16
    %int1280_4588 = torch.constant.int 1280
    %3933 = torch.prim.ListConstruct %int2_4586, %int16_4587, %int1280_4588 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3934 = torch.aten.view %3932, %3933 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %3935 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4589 = torch.constant.int 0
    %int1_4590 = torch.constant.int 1
    %3936 = torch.aten.transpose.int %3935, %int0_4589, %int1_4590 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4591 = torch.constant.int 32
    %int2048_4592 = torch.constant.int 2048
    %3937 = torch.prim.ListConstruct %int32_4591, %int2048_4592 : (!torch.int, !torch.int) -> !torch.list<int>
    %3938 = torch.aten.view %arg6, %3937 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %3939 = torch.aten.mm %3938, %3936 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4593 = torch.constant.int 2
    %int16_4594 = torch.constant.int 16
    %int1280_4595 = torch.constant.int 1280
    %3940 = torch.prim.ListConstruct %int2_4593, %int16_4594, %int1280_4595 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3941 = torch.aten.view %3939, %3940 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_4596 = torch.constant.int 2
    %int-1_4597 = torch.constant.int -1
    %int20_4598 = torch.constant.int 20
    %int64_4599 = torch.constant.int 64
    %3942 = torch.prim.ListConstruct %int2_4596, %int-1_4597, %int20_4598, %int64_4599 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3943 = torch.aten.view %3927, %3942 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4600 = torch.constant.int 1
    %int2_4601 = torch.constant.int 2
    %3944 = torch.aten.transpose.int %3943, %int1_4600, %int2_4601 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4602 = torch.constant.int 2
    %int-1_4603 = torch.constant.int -1
    %int20_4604 = torch.constant.int 20
    %int64_4605 = torch.constant.int 64
    %3945 = torch.prim.ListConstruct %int2_4602, %int-1_4603, %int20_4604, %int64_4605 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3946 = torch.aten.view %3934, %3945 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4606 = torch.constant.int 1
    %int2_4607 = torch.constant.int 2
    %3947 = torch.aten.transpose.int %3946, %int1_4606, %int2_4607 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_4608 = torch.constant.int 2
    %int-1_4609 = torch.constant.int -1
    %int20_4610 = torch.constant.int 20
    %int64_4611 = torch.constant.int 64
    %3948 = torch.prim.ListConstruct %int2_4608, %int-1_4609, %int20_4610, %int64_4611 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3949 = torch.aten.view %3941, %3948 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4612 = torch.constant.int 1
    %int2_4613 = torch.constant.int 2
    %3950 = torch.aten.transpose.int %3949, %int1_4612, %int2_4613 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_4614 = torch.constant.float 0.000000e+00
    %false_4615 = torch.constant.bool false
    %none_4616 = torch.constant.none
    %none_4617 = torch.constant.none
    %3951:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%3944, %3947, %3950, %float0.000000e00_4614, %false_4615, %none_4616, %none_4617) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4618 = torch.constant.int 1
    %int2_4619 = torch.constant.int 2
    %3952 = torch.aten.transpose.int %3951#0, %int1_4618, %int2_4619 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4620 = torch.constant.int 2
    %int-1_4621 = torch.constant.int -1
    %int1280_4622 = torch.constant.int 1280
    %3953 = torch.prim.ListConstruct %int2_4620, %int-1_4621, %int1280_4622 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3954 = torch.aten.view %3952, %3953 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4623 = torch.constant.int 5
    %3955 = torch.prims.convert_element_type %3954, %int5_4623 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4624 = torch.constant.int 1920
    %int1280_4625 = torch.constant.int 1280
    %3956 = torch.prim.ListConstruct %int1920_4624, %int1280_4625 : (!torch.int, !torch.int) -> !torch.list<int>
    %3957 = torch.aten.view %3955, %3956 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %3958 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4626 = torch.constant.int 0
    %int1_4627 = torch.constant.int 1
    %3959 = torch.aten.transpose.int %3958, %int0_4626, %int1_4627 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %3960 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4628 = torch.constant.int 6
    %3961 = torch.prims.convert_element_type %3960, %int6_4628 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4629 = torch.constant.int 6
    %3962 = torch.prims.convert_element_type %3957, %int6_4629 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4630 = torch.constant.int 6
    %3963 = torch.prims.convert_element_type %3959, %int6_4630 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %3964 = torch.aten.mm %3962, %3963 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4631 = torch.constant.int 1
    %3965 = torch.aten.mul.Scalar %3964, %int1_4631 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4632 = torch.constant.int 1
    %3966 = torch.aten.mul.Scalar %3961, %int1_4632 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4633 = torch.constant.int 1
    %3967 = torch.aten.add.Tensor %3965, %3966, %int1_4633 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4634 = torch.constant.int 5
    %3968 = torch.prims.convert_element_type %3967, %int5_4634 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4635 = torch.constant.int 2
    %int960_4636 = torch.constant.int 960
    %int1280_4637 = torch.constant.int 1280
    %3969 = torch.prim.ListConstruct %int2_4635, %int960_4636, %int1280_4637 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3970 = torch.aten.view %3968, %3969 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4638 = torch.constant.none
    %3971 = torch.aten.clone %3970, %none_4638 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4639 = torch.constant.float 1.000000e+00
    %3972 = torch.aten.div.Scalar %3971, %float1.000000e00_4639 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4640 = torch.constant.int 1
    %3973 = torch.aten.add.Tensor %3972, %3909, %int1_4640 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4641 = torch.constant.int 6
    %3974 = torch.prims.convert_element_type %3973, %int6_4641 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4642 = torch.constant.int 2
    %3975 = torch.prim.ListConstruct %int2_4642 : (!torch.int) -> !torch.list<int>
    %int0_4643 = torch.constant.int 0
    %true_4644 = torch.constant.bool true
    %result0_4645, %result1_4646 = torch.aten.var_mean.correction %3974, %3975, %int0_4643, %true_4644 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4647 = torch.constant.float 1.000000e-05
    %int1_4648 = torch.constant.int 1
    %3976 = torch.aten.add.Scalar %result0_4645, %float1.000000e-05_4647, %int1_4648 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %3977 = torch.aten.rsqrt %3976 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4649 = torch.constant.int 1
    %3978 = torch.aten.sub.Tensor %3973, %result1_4646, %int1_4649 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %3979 = torch.aten.mul.Tensor %3978, %3977 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %3980 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %3981 = torch.aten.mul.Tensor %3979, %3980 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %3982 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4650 = torch.constant.int 1
    %3983 = torch.aten.add.Tensor %3981, %3982, %int1_4650 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4651 = torch.constant.int 5
    %3984 = torch.prims.convert_element_type %3983, %int5_4651 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4652 = torch.constant.int 1920
    %int1280_4653 = torch.constant.int 1280
    %3985 = torch.prim.ListConstruct %int1920_4652, %int1280_4653 : (!torch.int, !torch.int) -> !torch.list<int>
    %3986 = torch.aten.view %3984, %3985 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %3987 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_4654 = torch.constant.int 0
    %int1_4655 = torch.constant.int 1
    %3988 = torch.aten.transpose.int %3987, %int0_4654, %int1_4655 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %3989 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_4656 = torch.constant.int 6
    %3990 = torch.prims.convert_element_type %3989, %int6_4656 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_4657 = torch.constant.int 6
    %3991 = torch.prims.convert_element_type %3986, %int6_4657 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4658 = torch.constant.int 6
    %3992 = torch.prims.convert_element_type %3988, %int6_4658 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %3993 = torch.aten.mm %3991, %3992 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_4659 = torch.constant.int 1
    %3994 = torch.aten.mul.Scalar %3993, %int1_4659 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_4660 = torch.constant.int 1
    %3995 = torch.aten.mul.Scalar %3990, %int1_4660 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_4661 = torch.constant.int 1
    %3996 = torch.aten.add.Tensor %3994, %3995, %int1_4661 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_4662 = torch.constant.int 5
    %3997 = torch.prims.convert_element_type %3996, %int5_4662 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_4663 = torch.constant.int 2
    %int960_4664 = torch.constant.int 960
    %int10240_4665 = torch.constant.int 10240
    %3998 = torch.prim.ListConstruct %int2_4663, %int960_4664, %int10240_4665 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %3999 = torch.aten.view %3997, %3998 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_4666 = torch.constant.int -1
    %int0_4667 = torch.constant.int 0
    %int5120_4668 = torch.constant.int 5120
    %int1_4669 = torch.constant.int 1
    %4000 = torch.aten.slice.Tensor %3999, %int-1_4666, %int0_4667, %int5120_4668, %int1_4669 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_4670 = torch.constant.int -1
    %int5120_4671 = torch.constant.int 5120
    %int10240_4672 = torch.constant.int 10240
    %int1_4673 = torch.constant.int 1
    %4001 = torch.aten.slice.Tensor %3999, %int-1_4670, %int5120_4671, %int10240_4672, %int1_4673 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_4674 = torch.constant.str "none"
    %4002 = torch.aten.gelu %4001, %str_4674 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %4003 = torch.aten.mul.Tensor %4000, %4002 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_4675 = torch.constant.none
    %4004 = torch.aten.clone %4003, %none_4675 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_4676 = torch.constant.int 1920
    %int5120_4677 = torch.constant.int 5120
    %4005 = torch.prim.ListConstruct %int1920_4676, %int5120_4677 : (!torch.int, !torch.int) -> !torch.list<int>
    %4006 = torch.aten.view %4004, %4005 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %4007 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4678 = torch.constant.int 0
    %int1_4679 = torch.constant.int 1
    %4008 = torch.aten.transpose.int %4007, %int0_4678, %int1_4679 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %4009 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4680 = torch.constant.int 6
    %4010 = torch.prims.convert_element_type %4009, %int6_4680 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4681 = torch.constant.int 6
    %4011 = torch.prims.convert_element_type %4006, %int6_4681 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_4682 = torch.constant.int 6
    %4012 = torch.prims.convert_element_type %4008, %int6_4682 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4013 = torch.aten.mm %4011, %4012 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4683 = torch.constant.int 1
    %4014 = torch.aten.mul.Scalar %4013, %int1_4683 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4684 = torch.constant.int 1
    %4015 = torch.aten.mul.Scalar %4010, %int1_4684 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4685 = torch.constant.int 1
    %4016 = torch.aten.add.Tensor %4014, %4015, %int1_4685 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4686 = torch.constant.int 5
    %4017 = torch.prims.convert_element_type %4016, %int5_4686 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4687 = torch.constant.int 2
    %int960_4688 = torch.constant.int 960
    %int1280_4689 = torch.constant.int 1280
    %4018 = torch.prim.ListConstruct %int2_4687, %int960_4688, %int1280_4689 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4019 = torch.aten.view %4017, %4018 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_4690 = torch.constant.int 1
    %4020 = torch.aten.add.Tensor %4019, %3973, %int1_4690 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4691 = torch.constant.int 6
    %4021 = torch.prims.convert_element_type %4020, %int6_4691 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4692 = torch.constant.int 2
    %4022 = torch.prim.ListConstruct %int2_4692 : (!torch.int) -> !torch.list<int>
    %int0_4693 = torch.constant.int 0
    %true_4694 = torch.constant.bool true
    %result0_4695, %result1_4696 = torch.aten.var_mean.correction %4021, %4022, %int0_4693, %true_4694 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4697 = torch.constant.float 1.000000e-05
    %int1_4698 = torch.constant.int 1
    %4023 = torch.aten.add.Scalar %result0_4695, %float1.000000e-05_4697, %int1_4698 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4024 = torch.aten.rsqrt %4023 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4699 = torch.constant.int 1
    %4025 = torch.aten.sub.Tensor %4020, %result1_4696, %int1_4699 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4026 = torch.aten.mul.Tensor %4025, %4024 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %4027 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4028 = torch.aten.mul.Tensor %4026, %4027 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %4029 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4700 = torch.constant.int 1
    %4030 = torch.aten.add.Tensor %4028, %4029, %int1_4700 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4701 = torch.constant.int 5
    %4031 = torch.prims.convert_element_type %4030, %int5_4701 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %4032 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4702 = torch.constant.int 0
    %int1_4703 = torch.constant.int 1
    %4033 = torch.aten.transpose.int %4032, %int0_4702, %int1_4703 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4704 = torch.constant.int 1920
    %int1280_4705 = torch.constant.int 1280
    %4034 = torch.prim.ListConstruct %int1920_4704, %int1280_4705 : (!torch.int, !torch.int) -> !torch.list<int>
    %4035 = torch.aten.view %4031, %4034 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4036 = torch.aten.mm %4035, %4033 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4706 = torch.constant.int 2
    %int960_4707 = torch.constant.int 960
    %int1280_4708 = torch.constant.int 1280
    %4037 = torch.prim.ListConstruct %int2_4706, %int960_4707, %int1280_4708 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4038 = torch.aten.view %4036, %4037 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %4039 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4709 = torch.constant.int 0
    %int1_4710 = torch.constant.int 1
    %4040 = torch.aten.transpose.int %4039, %int0_4709, %int1_4710 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4711 = torch.constant.int 1920
    %int1280_4712 = torch.constant.int 1280
    %4041 = torch.prim.ListConstruct %int1920_4711, %int1280_4712 : (!torch.int, !torch.int) -> !torch.list<int>
    %4042 = torch.aten.view %4031, %4041 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4043 = torch.aten.mm %4042, %4040 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4713 = torch.constant.int 2
    %int960_4714 = torch.constant.int 960
    %int1280_4715 = torch.constant.int 1280
    %4044 = torch.prim.ListConstruct %int2_4713, %int960_4714, %int1280_4715 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4045 = torch.aten.view %4043, %4044 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %4046 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4716 = torch.constant.int 0
    %int1_4717 = torch.constant.int 1
    %4047 = torch.aten.transpose.int %4046, %int0_4716, %int1_4717 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4718 = torch.constant.int 1920
    %int1280_4719 = torch.constant.int 1280
    %4048 = torch.prim.ListConstruct %int1920_4718, %int1280_4719 : (!torch.int, !torch.int) -> !torch.list<int>
    %4049 = torch.aten.view %4031, %4048 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4050 = torch.aten.mm %4049, %4047 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4720 = torch.constant.int 2
    %int960_4721 = torch.constant.int 960
    %int1280_4722 = torch.constant.int 1280
    %4051 = torch.prim.ListConstruct %int2_4720, %int960_4721, %int1280_4722 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4052 = torch.aten.view %4050, %4051 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_4723 = torch.constant.int 2
    %int-1_4724 = torch.constant.int -1
    %int20_4725 = torch.constant.int 20
    %int64_4726 = torch.constant.int 64
    %4053 = torch.prim.ListConstruct %int2_4723, %int-1_4724, %int20_4725, %int64_4726 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4054 = torch.aten.view %4038, %4053 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4727 = torch.constant.int 1
    %int2_4728 = torch.constant.int 2
    %4055 = torch.aten.transpose.int %4054, %int1_4727, %int2_4728 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4729 = torch.constant.int 2
    %int-1_4730 = torch.constant.int -1
    %int20_4731 = torch.constant.int 20
    %int64_4732 = torch.constant.int 64
    %4056 = torch.prim.ListConstruct %int2_4729, %int-1_4730, %int20_4731, %int64_4732 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4057 = torch.aten.view %4045, %4056 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4733 = torch.constant.int 1
    %int2_4734 = torch.constant.int 2
    %4058 = torch.aten.transpose.int %4057, %int1_4733, %int2_4734 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4735 = torch.constant.int 2
    %int-1_4736 = torch.constant.int -1
    %int20_4737 = torch.constant.int 20
    %int64_4738 = torch.constant.int 64
    %4059 = torch.prim.ListConstruct %int2_4735, %int-1_4736, %int20_4737, %int64_4738 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4060 = torch.aten.view %4052, %4059 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4739 = torch.constant.int 1
    %int2_4740 = torch.constant.int 2
    %4061 = torch.aten.transpose.int %4060, %int1_4739, %int2_4740 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_4741 = torch.constant.float 0.000000e+00
    %false_4742 = torch.constant.bool false
    %none_4743 = torch.constant.none
    %none_4744 = torch.constant.none
    %4062:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4055, %4058, %4061, %float0.000000e00_4741, %false_4742, %none_4743, %none_4744) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4745 = torch.constant.int 1
    %int2_4746 = torch.constant.int 2
    %4063 = torch.aten.transpose.int %4062#0, %int1_4745, %int2_4746 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4747 = torch.constant.int 2
    %int-1_4748 = torch.constant.int -1
    %int1280_4749 = torch.constant.int 1280
    %4064 = torch.prim.ListConstruct %int2_4747, %int-1_4748, %int1280_4749 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4065 = torch.aten.view %4063, %4064 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4750 = torch.constant.int 5
    %4066 = torch.prims.convert_element_type %4065, %int5_4750 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4751 = torch.constant.int 1920
    %int1280_4752 = torch.constant.int 1280
    %4067 = torch.prim.ListConstruct %int1920_4751, %int1280_4752 : (!torch.int, !torch.int) -> !torch.list<int>
    %4068 = torch.aten.view %4066, %4067 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4069 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4753 = torch.constant.int 0
    %int1_4754 = torch.constant.int 1
    %4070 = torch.aten.transpose.int %4069, %int0_4753, %int1_4754 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %4071 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4755 = torch.constant.int 6
    %4072 = torch.prims.convert_element_type %4071, %int6_4755 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4756 = torch.constant.int 6
    %4073 = torch.prims.convert_element_type %4068, %int6_4756 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4757 = torch.constant.int 6
    %4074 = torch.prims.convert_element_type %4070, %int6_4757 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4075 = torch.aten.mm %4073, %4074 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4758 = torch.constant.int 1
    %4076 = torch.aten.mul.Scalar %4075, %int1_4758 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4759 = torch.constant.int 1
    %4077 = torch.aten.mul.Scalar %4072, %int1_4759 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4760 = torch.constant.int 1
    %4078 = torch.aten.add.Tensor %4076, %4077, %int1_4760 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4761 = torch.constant.int 5
    %4079 = torch.prims.convert_element_type %4078, %int5_4761 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4762 = torch.constant.int 2
    %int960_4763 = torch.constant.int 960
    %int1280_4764 = torch.constant.int 1280
    %4080 = torch.prim.ListConstruct %int2_4762, %int960_4763, %int1280_4764 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4081 = torch.aten.view %4079, %4080 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4765 = torch.constant.none
    %4082 = torch.aten.clone %4081, %none_4765 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4766 = torch.constant.float 1.000000e+00
    %4083 = torch.aten.div.Scalar %4082, %float1.000000e00_4766 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4767 = torch.constant.int 1
    %4084 = torch.aten.add.Tensor %4083, %4020, %int1_4767 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4768 = torch.constant.int 6
    %4085 = torch.prims.convert_element_type %4084, %int6_4768 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4769 = torch.constant.int 2
    %4086 = torch.prim.ListConstruct %int2_4769 : (!torch.int) -> !torch.list<int>
    %int0_4770 = torch.constant.int 0
    %true_4771 = torch.constant.bool true
    %result0_4772, %result1_4773 = torch.aten.var_mean.correction %4085, %4086, %int0_4770, %true_4771 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4774 = torch.constant.float 1.000000e-05
    %int1_4775 = torch.constant.int 1
    %4087 = torch.aten.add.Scalar %result0_4772, %float1.000000e-05_4774, %int1_4775 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4088 = torch.aten.rsqrt %4087 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4776 = torch.constant.int 1
    %4089 = torch.aten.sub.Tensor %4084, %result1_4773, %int1_4776 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4090 = torch.aten.mul.Tensor %4089, %4088 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %4091 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4092 = torch.aten.mul.Tensor %4090, %4091 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %4093 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4777 = torch.constant.int 1
    %4094 = torch.aten.add.Tensor %4092, %4093, %int1_4777 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4778 = torch.constant.int 5
    %4095 = torch.prims.convert_element_type %4094, %int5_4778 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %4096 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4779 = torch.constant.int 0
    %int1_4780 = torch.constant.int 1
    %4097 = torch.aten.transpose.int %4096, %int0_4779, %int1_4780 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4781 = torch.constant.int 1920
    %int1280_4782 = torch.constant.int 1280
    %4098 = torch.prim.ListConstruct %int1920_4781, %int1280_4782 : (!torch.int, !torch.int) -> !torch.list<int>
    %4099 = torch.aten.view %4095, %4098 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4100 = torch.aten.mm %4099, %4097 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4783 = torch.constant.int 2
    %int960_4784 = torch.constant.int 960
    %int1280_4785 = torch.constant.int 1280
    %4101 = torch.prim.ListConstruct %int2_4783, %int960_4784, %int1280_4785 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4102 = torch.aten.view %4100, %4101 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %4103 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4786 = torch.constant.int 0
    %int1_4787 = torch.constant.int 1
    %4104 = torch.aten.transpose.int %4103, %int0_4786, %int1_4787 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4788 = torch.constant.int 32
    %int2048_4789 = torch.constant.int 2048
    %4105 = torch.prim.ListConstruct %int32_4788, %int2048_4789 : (!torch.int, !torch.int) -> !torch.list<int>
    %4106 = torch.aten.view %arg6, %4105 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4107 = torch.aten.mm %4106, %4104 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4790 = torch.constant.int 2
    %int16_4791 = torch.constant.int 16
    %int1280_4792 = torch.constant.int 1280
    %4108 = torch.prim.ListConstruct %int2_4790, %int16_4791, %int1280_4792 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4109 = torch.aten.view %4107, %4108 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %4110 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4793 = torch.constant.int 0
    %int1_4794 = torch.constant.int 1
    %4111 = torch.aten.transpose.int %4110, %int0_4793, %int1_4794 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4795 = torch.constant.int 32
    %int2048_4796 = torch.constant.int 2048
    %4112 = torch.prim.ListConstruct %int32_4795, %int2048_4796 : (!torch.int, !torch.int) -> !torch.list<int>
    %4113 = torch.aten.view %arg6, %4112 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4114 = torch.aten.mm %4113, %4111 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4797 = torch.constant.int 2
    %int16_4798 = torch.constant.int 16
    %int1280_4799 = torch.constant.int 1280
    %4115 = torch.prim.ListConstruct %int2_4797, %int16_4798, %int1280_4799 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4116 = torch.aten.view %4114, %4115 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_4800 = torch.constant.int 2
    %int-1_4801 = torch.constant.int -1
    %int20_4802 = torch.constant.int 20
    %int64_4803 = torch.constant.int 64
    %4117 = torch.prim.ListConstruct %int2_4800, %int-1_4801, %int20_4802, %int64_4803 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4118 = torch.aten.view %4102, %4117 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4804 = torch.constant.int 1
    %int2_4805 = torch.constant.int 2
    %4119 = torch.aten.transpose.int %4118, %int1_4804, %int2_4805 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4806 = torch.constant.int 2
    %int-1_4807 = torch.constant.int -1
    %int20_4808 = torch.constant.int 20
    %int64_4809 = torch.constant.int 64
    %4120 = torch.prim.ListConstruct %int2_4806, %int-1_4807, %int20_4808, %int64_4809 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4121 = torch.aten.view %4109, %4120 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4810 = torch.constant.int 1
    %int2_4811 = torch.constant.int 2
    %4122 = torch.aten.transpose.int %4121, %int1_4810, %int2_4811 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_4812 = torch.constant.int 2
    %int-1_4813 = torch.constant.int -1
    %int20_4814 = torch.constant.int 20
    %int64_4815 = torch.constant.int 64
    %4123 = torch.prim.ListConstruct %int2_4812, %int-1_4813, %int20_4814, %int64_4815 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4124 = torch.aten.view %4116, %4123 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_4816 = torch.constant.int 1
    %int2_4817 = torch.constant.int 2
    %4125 = torch.aten.transpose.int %4124, %int1_4816, %int2_4817 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_4818 = torch.constant.float 0.000000e+00
    %false_4819 = torch.constant.bool false
    %none_4820 = torch.constant.none
    %none_4821 = torch.constant.none
    %4126:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4119, %4122, %4125, %float0.000000e00_4818, %false_4819, %none_4820, %none_4821) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4822 = torch.constant.int 1
    %int2_4823 = torch.constant.int 2
    %4127 = torch.aten.transpose.int %4126#0, %int1_4822, %int2_4823 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4824 = torch.constant.int 2
    %int-1_4825 = torch.constant.int -1
    %int1280_4826 = torch.constant.int 1280
    %4128 = torch.prim.ListConstruct %int2_4824, %int-1_4825, %int1280_4826 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4129 = torch.aten.view %4127, %4128 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4827 = torch.constant.int 5
    %4130 = torch.prims.convert_element_type %4129, %int5_4827 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4828 = torch.constant.int 1920
    %int1280_4829 = torch.constant.int 1280
    %4131 = torch.prim.ListConstruct %int1920_4828, %int1280_4829 : (!torch.int, !torch.int) -> !torch.list<int>
    %4132 = torch.aten.view %4130, %4131 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4133 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4830 = torch.constant.int 0
    %int1_4831 = torch.constant.int 1
    %4134 = torch.aten.transpose.int %4133, %int0_4830, %int1_4831 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %4135 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4832 = torch.constant.int 6
    %4136 = torch.prims.convert_element_type %4135, %int6_4832 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4833 = torch.constant.int 6
    %4137 = torch.prims.convert_element_type %4132, %int6_4833 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4834 = torch.constant.int 6
    %4138 = torch.prims.convert_element_type %4134, %int6_4834 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4139 = torch.aten.mm %4137, %4138 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4835 = torch.constant.int 1
    %4140 = torch.aten.mul.Scalar %4139, %int1_4835 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4836 = torch.constant.int 1
    %4141 = torch.aten.mul.Scalar %4136, %int1_4836 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4837 = torch.constant.int 1
    %4142 = torch.aten.add.Tensor %4140, %4141, %int1_4837 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4838 = torch.constant.int 5
    %4143 = torch.prims.convert_element_type %4142, %int5_4838 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4839 = torch.constant.int 2
    %int960_4840 = torch.constant.int 960
    %int1280_4841 = torch.constant.int 1280
    %4144 = torch.prim.ListConstruct %int2_4839, %int960_4840, %int1280_4841 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4145 = torch.aten.view %4143, %4144 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4842 = torch.constant.none
    %4146 = torch.aten.clone %4145, %none_4842 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4843 = torch.constant.float 1.000000e+00
    %4147 = torch.aten.div.Scalar %4146, %float1.000000e00_4843 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4844 = torch.constant.int 1
    %4148 = torch.aten.add.Tensor %4147, %4084, %int1_4844 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4845 = torch.constant.int 6
    %4149 = torch.prims.convert_element_type %4148, %int6_4845 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4846 = torch.constant.int 2
    %4150 = torch.prim.ListConstruct %int2_4846 : (!torch.int) -> !torch.list<int>
    %int0_4847 = torch.constant.int 0
    %true_4848 = torch.constant.bool true
    %result0_4849, %result1_4850 = torch.aten.var_mean.correction %4149, %4150, %int0_4847, %true_4848 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4851 = torch.constant.float 1.000000e-05
    %int1_4852 = torch.constant.int 1
    %4151 = torch.aten.add.Scalar %result0_4849, %float1.000000e-05_4851, %int1_4852 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4152 = torch.aten.rsqrt %4151 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4853 = torch.constant.int 1
    %4153 = torch.aten.sub.Tensor %4148, %result1_4850, %int1_4853 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4154 = torch.aten.mul.Tensor %4153, %4152 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %4155 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4156 = torch.aten.mul.Tensor %4154, %4155 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %4157 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4854 = torch.constant.int 1
    %4158 = torch.aten.add.Tensor %4156, %4157, %int1_4854 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4855 = torch.constant.int 5
    %4159 = torch.prims.convert_element_type %4158, %int5_4855 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4856 = torch.constant.int 1920
    %int1280_4857 = torch.constant.int 1280
    %4160 = torch.prim.ListConstruct %int1920_4856, %int1280_4857 : (!torch.int, !torch.int) -> !torch.list<int>
    %4161 = torch.aten.view %4159, %4160 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4162 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_4858 = torch.constant.int 0
    %int1_4859 = torch.constant.int 1
    %4163 = torch.aten.transpose.int %4162, %int0_4858, %int1_4859 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %4164 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_4860 = torch.constant.int 6
    %4165 = torch.prims.convert_element_type %4164, %int6_4860 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_4861 = torch.constant.int 6
    %4166 = torch.prims.convert_element_type %4161, %int6_4861 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4862 = torch.constant.int 6
    %4167 = torch.prims.convert_element_type %4163, %int6_4862 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4168 = torch.aten.mm %4166, %4167 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_4863 = torch.constant.int 1
    %4169 = torch.aten.mul.Scalar %4168, %int1_4863 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_4864 = torch.constant.int 1
    %4170 = torch.aten.mul.Scalar %4165, %int1_4864 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_4865 = torch.constant.int 1
    %4171 = torch.aten.add.Tensor %4169, %4170, %int1_4865 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_4866 = torch.constant.int 5
    %4172 = torch.prims.convert_element_type %4171, %int5_4866 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_4867 = torch.constant.int 2
    %int960_4868 = torch.constant.int 960
    %int10240_4869 = torch.constant.int 10240
    %4173 = torch.prim.ListConstruct %int2_4867, %int960_4868, %int10240_4869 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4174 = torch.aten.view %4172, %4173 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_4870 = torch.constant.int -1
    %int0_4871 = torch.constant.int 0
    %int5120_4872 = torch.constant.int 5120
    %int1_4873 = torch.constant.int 1
    %4175 = torch.aten.slice.Tensor %4174, %int-1_4870, %int0_4871, %int5120_4872, %int1_4873 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_4874 = torch.constant.int -1
    %int5120_4875 = torch.constant.int 5120
    %int10240_4876 = torch.constant.int 10240
    %int1_4877 = torch.constant.int 1
    %4176 = torch.aten.slice.Tensor %4174, %int-1_4874, %int5120_4875, %int10240_4876, %int1_4877 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_4878 = torch.constant.str "none"
    %4177 = torch.aten.gelu %4176, %str_4878 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %4178 = torch.aten.mul.Tensor %4175, %4177 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_4879 = torch.constant.none
    %4179 = torch.aten.clone %4178, %none_4879 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_4880 = torch.constant.int 1920
    %int5120_4881 = torch.constant.int 5120
    %4180 = torch.prim.ListConstruct %int1920_4880, %int5120_4881 : (!torch.int, !torch.int) -> !torch.list<int>
    %4181 = torch.aten.view %4179, %4180 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %4182 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_4882 = torch.constant.int 0
    %int1_4883 = torch.constant.int 1
    %4183 = torch.aten.transpose.int %4182, %int0_4882, %int1_4883 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %4184 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4884 = torch.constant.int 6
    %4185 = torch.prims.convert_element_type %4184, %int6_4884 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4885 = torch.constant.int 6
    %4186 = torch.prims.convert_element_type %4181, %int6_4885 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_4886 = torch.constant.int 6
    %4187 = torch.prims.convert_element_type %4183, %int6_4886 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4188 = torch.aten.mm %4186, %4187 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4887 = torch.constant.int 1
    %4189 = torch.aten.mul.Scalar %4188, %int1_4887 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4888 = torch.constant.int 1
    %4190 = torch.aten.mul.Scalar %4185, %int1_4888 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4889 = torch.constant.int 1
    %4191 = torch.aten.add.Tensor %4189, %4190, %int1_4889 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4890 = torch.constant.int 5
    %4192 = torch.prims.convert_element_type %4191, %int5_4890 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4891 = torch.constant.int 2
    %int960_4892 = torch.constant.int 960
    %int1280_4893 = torch.constant.int 1280
    %4193 = torch.prim.ListConstruct %int2_4891, %int960_4892, %int1280_4893 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4194 = torch.aten.view %4192, %4193 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_4894 = torch.constant.int 1
    %4195 = torch.aten.add.Tensor %4194, %4148, %int1_4894 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4895 = torch.constant.int 6
    %4196 = torch.prims.convert_element_type %4195, %int6_4895 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4896 = torch.constant.int 2
    %4197 = torch.prim.ListConstruct %int2_4896 : (!torch.int) -> !torch.list<int>
    %int0_4897 = torch.constant.int 0
    %true_4898 = torch.constant.bool true
    %result0_4899, %result1_4900 = torch.aten.var_mean.correction %4196, %4197, %int0_4897, %true_4898 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4901 = torch.constant.float 1.000000e-05
    %int1_4902 = torch.constant.int 1
    %4198 = torch.aten.add.Scalar %result0_4899, %float1.000000e-05_4901, %int1_4902 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4199 = torch.aten.rsqrt %4198 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4903 = torch.constant.int 1
    %4200 = torch.aten.sub.Tensor %4195, %result1_4900, %int1_4903 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4201 = torch.aten.mul.Tensor %4200, %4199 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %4202 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4203 = torch.aten.mul.Tensor %4201, %4202 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %4204 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4904 = torch.constant.int 1
    %4205 = torch.aten.add.Tensor %4203, %4204, %int1_4904 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4905 = torch.constant.int 5
    %4206 = torch.prims.convert_element_type %4205, %int5_4905 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %4207 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4906 = torch.constant.int 0
    %int1_4907 = torch.constant.int 1
    %4208 = torch.aten.transpose.int %4207, %int0_4906, %int1_4907 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4908 = torch.constant.int 1920
    %int1280_4909 = torch.constant.int 1280
    %4209 = torch.prim.ListConstruct %int1920_4908, %int1280_4909 : (!torch.int, !torch.int) -> !torch.list<int>
    %4210 = torch.aten.view %4206, %4209 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4211 = torch.aten.mm %4210, %4208 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4910 = torch.constant.int 2
    %int960_4911 = torch.constant.int 960
    %int1280_4912 = torch.constant.int 1280
    %4212 = torch.prim.ListConstruct %int2_4910, %int960_4911, %int1280_4912 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4213 = torch.aten.view %4211, %4212 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %4214 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4913 = torch.constant.int 0
    %int1_4914 = torch.constant.int 1
    %4215 = torch.aten.transpose.int %4214, %int0_4913, %int1_4914 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4915 = torch.constant.int 1920
    %int1280_4916 = torch.constant.int 1280
    %4216 = torch.prim.ListConstruct %int1920_4915, %int1280_4916 : (!torch.int, !torch.int) -> !torch.list<int>
    %4217 = torch.aten.view %4206, %4216 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4218 = torch.aten.mm %4217, %4215 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4917 = torch.constant.int 2
    %int960_4918 = torch.constant.int 960
    %int1280_4919 = torch.constant.int 1280
    %4219 = torch.prim.ListConstruct %int2_4917, %int960_4918, %int1280_4919 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4220 = torch.aten.view %4218, %4219 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %4221 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4920 = torch.constant.int 0
    %int1_4921 = torch.constant.int 1
    %4222 = torch.aten.transpose.int %4221, %int0_4920, %int1_4921 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4922 = torch.constant.int 1920
    %int1280_4923 = torch.constant.int 1280
    %4223 = torch.prim.ListConstruct %int1920_4922, %int1280_4923 : (!torch.int, !torch.int) -> !torch.list<int>
    %4224 = torch.aten.view %4206, %4223 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4225 = torch.aten.mm %4224, %4222 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4924 = torch.constant.int 2
    %int960_4925 = torch.constant.int 960
    %int1280_4926 = torch.constant.int 1280
    %4226 = torch.prim.ListConstruct %int2_4924, %int960_4925, %int1280_4926 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4227 = torch.aten.view %4225, %4226 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_4927 = torch.constant.int 2
    %int-1_4928 = torch.constant.int -1
    %int20_4929 = torch.constant.int 20
    %int64_4930 = torch.constant.int 64
    %4228 = torch.prim.ListConstruct %int2_4927, %int-1_4928, %int20_4929, %int64_4930 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4229 = torch.aten.view %4213, %4228 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4931 = torch.constant.int 1
    %int2_4932 = torch.constant.int 2
    %4230 = torch.aten.transpose.int %4229, %int1_4931, %int2_4932 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4933 = torch.constant.int 2
    %int-1_4934 = torch.constant.int -1
    %int20_4935 = torch.constant.int 20
    %int64_4936 = torch.constant.int 64
    %4231 = torch.prim.ListConstruct %int2_4933, %int-1_4934, %int20_4935, %int64_4936 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4232 = torch.aten.view %4220, %4231 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4937 = torch.constant.int 1
    %int2_4938 = torch.constant.int 2
    %4233 = torch.aten.transpose.int %4232, %int1_4937, %int2_4938 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_4939 = torch.constant.int 2
    %int-1_4940 = torch.constant.int -1
    %int20_4941 = torch.constant.int 20
    %int64_4942 = torch.constant.int 64
    %4234 = torch.prim.ListConstruct %int2_4939, %int-1_4940, %int20_4941, %int64_4942 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4235 = torch.aten.view %4227, %4234 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_4943 = torch.constant.int 1
    %int2_4944 = torch.constant.int 2
    %4236 = torch.aten.transpose.int %4235, %int1_4943, %int2_4944 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_4945 = torch.constant.float 0.000000e+00
    %false_4946 = torch.constant.bool false
    %none_4947 = torch.constant.none
    %none_4948 = torch.constant.none
    %4237:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4230, %4233, %4236, %float0.000000e00_4945, %false_4946, %none_4947, %none_4948) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_4949 = torch.constant.int 1
    %int2_4950 = torch.constant.int 2
    %4238 = torch.aten.transpose.int %4237#0, %int1_4949, %int2_4950 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_4951 = torch.constant.int 2
    %int-1_4952 = torch.constant.int -1
    %int1280_4953 = torch.constant.int 1280
    %4239 = torch.prim.ListConstruct %int2_4951, %int-1_4952, %int1280_4953 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4240 = torch.aten.view %4238, %4239 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_4954 = torch.constant.int 5
    %4241 = torch.prims.convert_element_type %4240, %int5_4954 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_4955 = torch.constant.int 1920
    %int1280_4956 = torch.constant.int 1280
    %4242 = torch.prim.ListConstruct %int1920_4955, %int1280_4956 : (!torch.int, !torch.int) -> !torch.list<int>
    %4243 = torch.aten.view %4241, %4242 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4244 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4957 = torch.constant.int 0
    %int1_4958 = torch.constant.int 1
    %4245 = torch.aten.transpose.int %4244, %int0_4957, %int1_4958 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %4246 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_4959 = torch.constant.int 6
    %4247 = torch.prims.convert_element_type %4246, %int6_4959 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_4960 = torch.constant.int 6
    %4248 = torch.prims.convert_element_type %4243, %int6_4960 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_4961 = torch.constant.int 6
    %4249 = torch.prims.convert_element_type %4245, %int6_4961 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4250 = torch.aten.mm %4248, %4249 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_4962 = torch.constant.int 1
    %4251 = torch.aten.mul.Scalar %4250, %int1_4962 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_4963 = torch.constant.int 1
    %4252 = torch.aten.mul.Scalar %4247, %int1_4963 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_4964 = torch.constant.int 1
    %4253 = torch.aten.add.Tensor %4251, %4252, %int1_4964 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_4965 = torch.constant.int 5
    %4254 = torch.prims.convert_element_type %4253, %int5_4965 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_4966 = torch.constant.int 2
    %int960_4967 = torch.constant.int 960
    %int1280_4968 = torch.constant.int 1280
    %4255 = torch.prim.ListConstruct %int2_4966, %int960_4967, %int1280_4968 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4256 = torch.aten.view %4254, %4255 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_4969 = torch.constant.none
    %4257 = torch.aten.clone %4256, %none_4969 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_4970 = torch.constant.float 1.000000e+00
    %4258 = torch.aten.div.Scalar %4257, %float1.000000e00_4970 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_4971 = torch.constant.int 1
    %4259 = torch.aten.add.Tensor %4258, %4195, %int1_4971 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_4972 = torch.constant.int 6
    %4260 = torch.prims.convert_element_type %4259, %int6_4972 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_4973 = torch.constant.int 2
    %4261 = torch.prim.ListConstruct %int2_4973 : (!torch.int) -> !torch.list<int>
    %int0_4974 = torch.constant.int 0
    %true_4975 = torch.constant.bool true
    %result0_4976, %result1_4977 = torch.aten.var_mean.correction %4260, %4261, %int0_4974, %true_4975 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_4978 = torch.constant.float 1.000000e-05
    %int1_4979 = torch.constant.int 1
    %4262 = torch.aten.add.Scalar %result0_4976, %float1.000000e-05_4978, %int1_4979 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4263 = torch.aten.rsqrt %4262 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_4980 = torch.constant.int 1
    %4264 = torch.aten.sub.Tensor %4259, %result1_4977, %int1_4980 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4265 = torch.aten.mul.Tensor %4264, %4263 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %4266 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4267 = torch.aten.mul.Tensor %4265, %4266 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %4268 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_4981 = torch.constant.int 1
    %4269 = torch.aten.add.Tensor %4267, %4268, %int1_4981 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_4982 = torch.constant.int 5
    %4270 = torch.prims.convert_element_type %4269, %int5_4982 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %4271 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_4983 = torch.constant.int 0
    %int1_4984 = torch.constant.int 1
    %4272 = torch.aten.transpose.int %4271, %int0_4983, %int1_4984 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_4985 = torch.constant.int 1920
    %int1280_4986 = torch.constant.int 1280
    %4273 = torch.prim.ListConstruct %int1920_4985, %int1280_4986 : (!torch.int, !torch.int) -> !torch.list<int>
    %4274 = torch.aten.view %4270, %4273 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4275 = torch.aten.mm %4274, %4272 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_4987 = torch.constant.int 2
    %int960_4988 = torch.constant.int 960
    %int1280_4989 = torch.constant.int 1280
    %4276 = torch.prim.ListConstruct %int2_4987, %int960_4988, %int1280_4989 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4277 = torch.aten.view %4275, %4276 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %4278 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4990 = torch.constant.int 0
    %int1_4991 = torch.constant.int 1
    %4279 = torch.aten.transpose.int %4278, %int0_4990, %int1_4991 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4992 = torch.constant.int 32
    %int2048_4993 = torch.constant.int 2048
    %4280 = torch.prim.ListConstruct %int32_4992, %int2048_4993 : (!torch.int, !torch.int) -> !torch.list<int>
    %4281 = torch.aten.view %arg6, %4280 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4282 = torch.aten.mm %4281, %4279 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_4994 = torch.constant.int 2
    %int16_4995 = torch.constant.int 16
    %int1280_4996 = torch.constant.int 1280
    %4283 = torch.prim.ListConstruct %int2_4994, %int16_4995, %int1280_4996 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4284 = torch.aten.view %4282, %4283 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %4285 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_4997 = torch.constant.int 0
    %int1_4998 = torch.constant.int 1
    %4286 = torch.aten.transpose.int %4285, %int0_4997, %int1_4998 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_4999 = torch.constant.int 32
    %int2048_5000 = torch.constant.int 2048
    %4287 = torch.prim.ListConstruct %int32_4999, %int2048_5000 : (!torch.int, !torch.int) -> !torch.list<int>
    %4288 = torch.aten.view %arg6, %4287 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4289 = torch.aten.mm %4288, %4286 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5001 = torch.constant.int 2
    %int16_5002 = torch.constant.int 16
    %int1280_5003 = torch.constant.int 1280
    %4290 = torch.prim.ListConstruct %int2_5001, %int16_5002, %int1280_5003 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4291 = torch.aten.view %4289, %4290 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_5004 = torch.constant.int 2
    %int-1_5005 = torch.constant.int -1
    %int20_5006 = torch.constant.int 20
    %int64_5007 = torch.constant.int 64
    %4292 = torch.prim.ListConstruct %int2_5004, %int-1_5005, %int20_5006, %int64_5007 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4293 = torch.aten.view %4277, %4292 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5008 = torch.constant.int 1
    %int2_5009 = torch.constant.int 2
    %4294 = torch.aten.transpose.int %4293, %int1_5008, %int2_5009 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5010 = torch.constant.int 2
    %int-1_5011 = torch.constant.int -1
    %int20_5012 = torch.constant.int 20
    %int64_5013 = torch.constant.int 64
    %4295 = torch.prim.ListConstruct %int2_5010, %int-1_5011, %int20_5012, %int64_5013 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4296 = torch.aten.view %4284, %4295 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5014 = torch.constant.int 1
    %int2_5015 = torch.constant.int 2
    %4297 = torch.aten.transpose.int %4296, %int1_5014, %int2_5015 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_5016 = torch.constant.int 2
    %int-1_5017 = torch.constant.int -1
    %int20_5018 = torch.constant.int 20
    %int64_5019 = torch.constant.int 64
    %4298 = torch.prim.ListConstruct %int2_5016, %int-1_5017, %int20_5018, %int64_5019 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4299 = torch.aten.view %4291, %4298 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5020 = torch.constant.int 1
    %int2_5021 = torch.constant.int 2
    %4300 = torch.aten.transpose.int %4299, %int1_5020, %int2_5021 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_5022 = torch.constant.float 0.000000e+00
    %false_5023 = torch.constant.bool false
    %none_5024 = torch.constant.none
    %none_5025 = torch.constant.none
    %4301:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4294, %4297, %4300, %float0.000000e00_5022, %false_5023, %none_5024, %none_5025) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5026 = torch.constant.int 1
    %int2_5027 = torch.constant.int 2
    %4302 = torch.aten.transpose.int %4301#0, %int1_5026, %int2_5027 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5028 = torch.constant.int 2
    %int-1_5029 = torch.constant.int -1
    %int1280_5030 = torch.constant.int 1280
    %4303 = torch.prim.ListConstruct %int2_5028, %int-1_5029, %int1280_5030 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4304 = torch.aten.view %4302, %4303 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5031 = torch.constant.int 5
    %4305 = torch.prims.convert_element_type %4304, %int5_5031 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5032 = torch.constant.int 1920
    %int1280_5033 = torch.constant.int 1280
    %4306 = torch.prim.ListConstruct %int1920_5032, %int1280_5033 : (!torch.int, !torch.int) -> !torch.list<int>
    %4307 = torch.aten.view %4305, %4306 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4308 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5034 = torch.constant.int 0
    %int1_5035 = torch.constant.int 1
    %4309 = torch.aten.transpose.int %4308, %int0_5034, %int1_5035 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %4310 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5036 = torch.constant.int 6
    %4311 = torch.prims.convert_element_type %4310, %int6_5036 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5037 = torch.constant.int 6
    %4312 = torch.prims.convert_element_type %4307, %int6_5037 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5038 = torch.constant.int 6
    %4313 = torch.prims.convert_element_type %4309, %int6_5038 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4314 = torch.aten.mm %4312, %4313 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5039 = torch.constant.int 1
    %4315 = torch.aten.mul.Scalar %4314, %int1_5039 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5040 = torch.constant.int 1
    %4316 = torch.aten.mul.Scalar %4311, %int1_5040 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5041 = torch.constant.int 1
    %4317 = torch.aten.add.Tensor %4315, %4316, %int1_5041 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5042 = torch.constant.int 5
    %4318 = torch.prims.convert_element_type %4317, %int5_5042 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5043 = torch.constant.int 2
    %int960_5044 = torch.constant.int 960
    %int1280_5045 = torch.constant.int 1280
    %4319 = torch.prim.ListConstruct %int2_5043, %int960_5044, %int1280_5045 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4320 = torch.aten.view %4318, %4319 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5046 = torch.constant.none
    %4321 = torch.aten.clone %4320, %none_5046 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5047 = torch.constant.float 1.000000e+00
    %4322 = torch.aten.div.Scalar %4321, %float1.000000e00_5047 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5048 = torch.constant.int 1
    %4323 = torch.aten.add.Tensor %4322, %4259, %int1_5048 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5049 = torch.constant.int 6
    %4324 = torch.prims.convert_element_type %4323, %int6_5049 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5050 = torch.constant.int 2
    %4325 = torch.prim.ListConstruct %int2_5050 : (!torch.int) -> !torch.list<int>
    %int0_5051 = torch.constant.int 0
    %true_5052 = torch.constant.bool true
    %result0_5053, %result1_5054 = torch.aten.var_mean.correction %4324, %4325, %int0_5051, %true_5052 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5055 = torch.constant.float 1.000000e-05
    %int1_5056 = torch.constant.int 1
    %4326 = torch.aten.add.Scalar %result0_5053, %float1.000000e-05_5055, %int1_5056 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4327 = torch.aten.rsqrt %4326 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5057 = torch.constant.int 1
    %4328 = torch.aten.sub.Tensor %4323, %result1_5054, %int1_5057 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4329 = torch.aten.mul.Tensor %4328, %4327 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %4330 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4331 = torch.aten.mul.Tensor %4329, %4330 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %4332 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5058 = torch.constant.int 1
    %4333 = torch.aten.add.Tensor %4331, %4332, %int1_5058 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5059 = torch.constant.int 5
    %4334 = torch.prims.convert_element_type %4333, %int5_5059 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5060 = torch.constant.int 1920
    %int1280_5061 = torch.constant.int 1280
    %4335 = torch.prim.ListConstruct %int1920_5060, %int1280_5061 : (!torch.int, !torch.int) -> !torch.list<int>
    %4336 = torch.aten.view %4334, %4335 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4337 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_5062 = torch.constant.int 0
    %int1_5063 = torch.constant.int 1
    %4338 = torch.aten.transpose.int %4337, %int0_5062, %int1_5063 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %4339 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_5064 = torch.constant.int 6
    %4340 = torch.prims.convert_element_type %4339, %int6_5064 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_5065 = torch.constant.int 6
    %4341 = torch.prims.convert_element_type %4336, %int6_5065 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5066 = torch.constant.int 6
    %4342 = torch.prims.convert_element_type %4338, %int6_5066 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4343 = torch.aten.mm %4341, %4342 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_5067 = torch.constant.int 1
    %4344 = torch.aten.mul.Scalar %4343, %int1_5067 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_5068 = torch.constant.int 1
    %4345 = torch.aten.mul.Scalar %4340, %int1_5068 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_5069 = torch.constant.int 1
    %4346 = torch.aten.add.Tensor %4344, %4345, %int1_5069 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_5070 = torch.constant.int 5
    %4347 = torch.prims.convert_element_type %4346, %int5_5070 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_5071 = torch.constant.int 2
    %int960_5072 = torch.constant.int 960
    %int10240_5073 = torch.constant.int 10240
    %4348 = torch.prim.ListConstruct %int2_5071, %int960_5072, %int10240_5073 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4349 = torch.aten.view %4347, %4348 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_5074 = torch.constant.int -1
    %int0_5075 = torch.constant.int 0
    %int5120_5076 = torch.constant.int 5120
    %int1_5077 = torch.constant.int 1
    %4350 = torch.aten.slice.Tensor %4349, %int-1_5074, %int0_5075, %int5120_5076, %int1_5077 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_5078 = torch.constant.int -1
    %int5120_5079 = torch.constant.int 5120
    %int10240_5080 = torch.constant.int 10240
    %int1_5081 = torch.constant.int 1
    %4351 = torch.aten.slice.Tensor %4349, %int-1_5078, %int5120_5079, %int10240_5080, %int1_5081 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_5082 = torch.constant.str "none"
    %4352 = torch.aten.gelu %4351, %str_5082 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %4353 = torch.aten.mul.Tensor %4350, %4352 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_5083 = torch.constant.none
    %4354 = torch.aten.clone %4353, %none_5083 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_5084 = torch.constant.int 1920
    %int5120_5085 = torch.constant.int 5120
    %4355 = torch.prim.ListConstruct %int1920_5084, %int5120_5085 : (!torch.int, !torch.int) -> !torch.list<int>
    %4356 = torch.aten.view %4354, %4355 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %4357 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5086 = torch.constant.int 0
    %int1_5087 = torch.constant.int 1
    %4358 = torch.aten.transpose.int %4357, %int0_5086, %int1_5087 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %4359 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5088 = torch.constant.int 6
    %4360 = torch.prims.convert_element_type %4359, %int6_5088 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5089 = torch.constant.int 6
    %4361 = torch.prims.convert_element_type %4356, %int6_5089 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_5090 = torch.constant.int 6
    %4362 = torch.prims.convert_element_type %4358, %int6_5090 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4363 = torch.aten.mm %4361, %4362 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5091 = torch.constant.int 1
    %4364 = torch.aten.mul.Scalar %4363, %int1_5091 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5092 = torch.constant.int 1
    %4365 = torch.aten.mul.Scalar %4360, %int1_5092 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5093 = torch.constant.int 1
    %4366 = torch.aten.add.Tensor %4364, %4365, %int1_5093 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5094 = torch.constant.int 5
    %4367 = torch.prims.convert_element_type %4366, %int5_5094 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5095 = torch.constant.int 2
    %int960_5096 = torch.constant.int 960
    %int1280_5097 = torch.constant.int 1280
    %4368 = torch.prim.ListConstruct %int2_5095, %int960_5096, %int1280_5097 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4369 = torch.aten.view %4367, %4368 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_5098 = torch.constant.int 1
    %4370 = torch.aten.add.Tensor %4369, %4323, %int1_5098 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5099 = torch.constant.int 6
    %4371 = torch.prims.convert_element_type %4370, %int6_5099 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5100 = torch.constant.int 2
    %4372 = torch.prim.ListConstruct %int2_5100 : (!torch.int) -> !torch.list<int>
    %int0_5101 = torch.constant.int 0
    %true_5102 = torch.constant.bool true
    %result0_5103, %result1_5104 = torch.aten.var_mean.correction %4371, %4372, %int0_5101, %true_5102 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5105 = torch.constant.float 1.000000e-05
    %int1_5106 = torch.constant.int 1
    %4373 = torch.aten.add.Scalar %result0_5103, %float1.000000e-05_5105, %int1_5106 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4374 = torch.aten.rsqrt %4373 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5107 = torch.constant.int 1
    %4375 = torch.aten.sub.Tensor %4370, %result1_5104, %int1_5107 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4376 = torch.aten.mul.Tensor %4375, %4374 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %4377 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4378 = torch.aten.mul.Tensor %4376, %4377 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %4379 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5108 = torch.constant.int 1
    %4380 = torch.aten.add.Tensor %4378, %4379, %int1_5108 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5109 = torch.constant.int 5
    %4381 = torch.prims.convert_element_type %4380, %int5_5109 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %4382 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5110 = torch.constant.int 0
    %int1_5111 = torch.constant.int 1
    %4383 = torch.aten.transpose.int %4382, %int0_5110, %int1_5111 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5112 = torch.constant.int 1920
    %int1280_5113 = torch.constant.int 1280
    %4384 = torch.prim.ListConstruct %int1920_5112, %int1280_5113 : (!torch.int, !torch.int) -> !torch.list<int>
    %4385 = torch.aten.view %4381, %4384 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4386 = torch.aten.mm %4385, %4383 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5114 = torch.constant.int 2
    %int960_5115 = torch.constant.int 960
    %int1280_5116 = torch.constant.int 1280
    %4387 = torch.prim.ListConstruct %int2_5114, %int960_5115, %int1280_5116 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4388 = torch.aten.view %4386, %4387 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %4389 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5117 = torch.constant.int 0
    %int1_5118 = torch.constant.int 1
    %4390 = torch.aten.transpose.int %4389, %int0_5117, %int1_5118 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5119 = torch.constant.int 1920
    %int1280_5120 = torch.constant.int 1280
    %4391 = torch.prim.ListConstruct %int1920_5119, %int1280_5120 : (!torch.int, !torch.int) -> !torch.list<int>
    %4392 = torch.aten.view %4381, %4391 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4393 = torch.aten.mm %4392, %4390 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5121 = torch.constant.int 2
    %int960_5122 = torch.constant.int 960
    %int1280_5123 = torch.constant.int 1280
    %4394 = torch.prim.ListConstruct %int2_5121, %int960_5122, %int1280_5123 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4395 = torch.aten.view %4393, %4394 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %4396 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5124 = torch.constant.int 0
    %int1_5125 = torch.constant.int 1
    %4397 = torch.aten.transpose.int %4396, %int0_5124, %int1_5125 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5126 = torch.constant.int 1920
    %int1280_5127 = torch.constant.int 1280
    %4398 = torch.prim.ListConstruct %int1920_5126, %int1280_5127 : (!torch.int, !torch.int) -> !torch.list<int>
    %4399 = torch.aten.view %4381, %4398 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4400 = torch.aten.mm %4399, %4397 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5128 = torch.constant.int 2
    %int960_5129 = torch.constant.int 960
    %int1280_5130 = torch.constant.int 1280
    %4401 = torch.prim.ListConstruct %int2_5128, %int960_5129, %int1280_5130 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4402 = torch.aten.view %4400, %4401 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_5131 = torch.constant.int 2
    %int-1_5132 = torch.constant.int -1
    %int20_5133 = torch.constant.int 20
    %int64_5134 = torch.constant.int 64
    %4403 = torch.prim.ListConstruct %int2_5131, %int-1_5132, %int20_5133, %int64_5134 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4404 = torch.aten.view %4388, %4403 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5135 = torch.constant.int 1
    %int2_5136 = torch.constant.int 2
    %4405 = torch.aten.transpose.int %4404, %int1_5135, %int2_5136 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5137 = torch.constant.int 2
    %int-1_5138 = torch.constant.int -1
    %int20_5139 = torch.constant.int 20
    %int64_5140 = torch.constant.int 64
    %4406 = torch.prim.ListConstruct %int2_5137, %int-1_5138, %int20_5139, %int64_5140 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4407 = torch.aten.view %4395, %4406 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5141 = torch.constant.int 1
    %int2_5142 = torch.constant.int 2
    %4408 = torch.aten.transpose.int %4407, %int1_5141, %int2_5142 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5143 = torch.constant.int 2
    %int-1_5144 = torch.constant.int -1
    %int20_5145 = torch.constant.int 20
    %int64_5146 = torch.constant.int 64
    %4409 = torch.prim.ListConstruct %int2_5143, %int-1_5144, %int20_5145, %int64_5146 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4410 = torch.aten.view %4402, %4409 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5147 = torch.constant.int 1
    %int2_5148 = torch.constant.int 2
    %4411 = torch.aten.transpose.int %4410, %int1_5147, %int2_5148 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_5149 = torch.constant.float 0.000000e+00
    %false_5150 = torch.constant.bool false
    %none_5151 = torch.constant.none
    %none_5152 = torch.constant.none
    %4412:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4405, %4408, %4411, %float0.000000e00_5149, %false_5150, %none_5151, %none_5152) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5153 = torch.constant.int 1
    %int2_5154 = torch.constant.int 2
    %4413 = torch.aten.transpose.int %4412#0, %int1_5153, %int2_5154 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5155 = torch.constant.int 2
    %int-1_5156 = torch.constant.int -1
    %int1280_5157 = torch.constant.int 1280
    %4414 = torch.prim.ListConstruct %int2_5155, %int-1_5156, %int1280_5157 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4415 = torch.aten.view %4413, %4414 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5158 = torch.constant.int 5
    %4416 = torch.prims.convert_element_type %4415, %int5_5158 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5159 = torch.constant.int 1920
    %int1280_5160 = torch.constant.int 1280
    %4417 = torch.prim.ListConstruct %int1920_5159, %int1280_5160 : (!torch.int, !torch.int) -> !torch.list<int>
    %4418 = torch.aten.view %4416, %4417 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4419 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5161 = torch.constant.int 0
    %int1_5162 = torch.constant.int 1
    %4420 = torch.aten.transpose.int %4419, %int0_5161, %int1_5162 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %4421 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5163 = torch.constant.int 6
    %4422 = torch.prims.convert_element_type %4421, %int6_5163 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5164 = torch.constant.int 6
    %4423 = torch.prims.convert_element_type %4418, %int6_5164 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5165 = torch.constant.int 6
    %4424 = torch.prims.convert_element_type %4420, %int6_5165 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4425 = torch.aten.mm %4423, %4424 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5166 = torch.constant.int 1
    %4426 = torch.aten.mul.Scalar %4425, %int1_5166 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5167 = torch.constant.int 1
    %4427 = torch.aten.mul.Scalar %4422, %int1_5167 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5168 = torch.constant.int 1
    %4428 = torch.aten.add.Tensor %4426, %4427, %int1_5168 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5169 = torch.constant.int 5
    %4429 = torch.prims.convert_element_type %4428, %int5_5169 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5170 = torch.constant.int 2
    %int960_5171 = torch.constant.int 960
    %int1280_5172 = torch.constant.int 1280
    %4430 = torch.prim.ListConstruct %int2_5170, %int960_5171, %int1280_5172 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4431 = torch.aten.view %4429, %4430 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5173 = torch.constant.none
    %4432 = torch.aten.clone %4431, %none_5173 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5174 = torch.constant.float 1.000000e+00
    %4433 = torch.aten.div.Scalar %4432, %float1.000000e00_5174 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5175 = torch.constant.int 1
    %4434 = torch.aten.add.Tensor %4433, %4370, %int1_5175 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5176 = torch.constant.int 6
    %4435 = torch.prims.convert_element_type %4434, %int6_5176 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5177 = torch.constant.int 2
    %4436 = torch.prim.ListConstruct %int2_5177 : (!torch.int) -> !torch.list<int>
    %int0_5178 = torch.constant.int 0
    %true_5179 = torch.constant.bool true
    %result0_5180, %result1_5181 = torch.aten.var_mean.correction %4435, %4436, %int0_5178, %true_5179 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5182 = torch.constant.float 1.000000e-05
    %int1_5183 = torch.constant.int 1
    %4437 = torch.aten.add.Scalar %result0_5180, %float1.000000e-05_5182, %int1_5183 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4438 = torch.aten.rsqrt %4437 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5184 = torch.constant.int 1
    %4439 = torch.aten.sub.Tensor %4434, %result1_5181, %int1_5184 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4440 = torch.aten.mul.Tensor %4439, %4438 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %4441 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4442 = torch.aten.mul.Tensor %4440, %4441 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %4443 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5185 = torch.constant.int 1
    %4444 = torch.aten.add.Tensor %4442, %4443, %int1_5185 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5186 = torch.constant.int 5
    %4445 = torch.prims.convert_element_type %4444, %int5_5186 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %4446 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5187 = torch.constant.int 0
    %int1_5188 = torch.constant.int 1
    %4447 = torch.aten.transpose.int %4446, %int0_5187, %int1_5188 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5189 = torch.constant.int 1920
    %int1280_5190 = torch.constant.int 1280
    %4448 = torch.prim.ListConstruct %int1920_5189, %int1280_5190 : (!torch.int, !torch.int) -> !torch.list<int>
    %4449 = torch.aten.view %4445, %4448 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4450 = torch.aten.mm %4449, %4447 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5191 = torch.constant.int 2
    %int960_5192 = torch.constant.int 960
    %int1280_5193 = torch.constant.int 1280
    %4451 = torch.prim.ListConstruct %int2_5191, %int960_5192, %int1280_5193 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4452 = torch.aten.view %4450, %4451 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %4453 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5194 = torch.constant.int 0
    %int1_5195 = torch.constant.int 1
    %4454 = torch.aten.transpose.int %4453, %int0_5194, %int1_5195 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5196 = torch.constant.int 32
    %int2048_5197 = torch.constant.int 2048
    %4455 = torch.prim.ListConstruct %int32_5196, %int2048_5197 : (!torch.int, !torch.int) -> !torch.list<int>
    %4456 = torch.aten.view %arg6, %4455 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4457 = torch.aten.mm %4456, %4454 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5198 = torch.constant.int 2
    %int16_5199 = torch.constant.int 16
    %int1280_5200 = torch.constant.int 1280
    %4458 = torch.prim.ListConstruct %int2_5198, %int16_5199, %int1280_5200 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4459 = torch.aten.view %4457, %4458 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %4460 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5201 = torch.constant.int 0
    %int1_5202 = torch.constant.int 1
    %4461 = torch.aten.transpose.int %4460, %int0_5201, %int1_5202 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5203 = torch.constant.int 32
    %int2048_5204 = torch.constant.int 2048
    %4462 = torch.prim.ListConstruct %int32_5203, %int2048_5204 : (!torch.int, !torch.int) -> !torch.list<int>
    %4463 = torch.aten.view %arg6, %4462 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4464 = torch.aten.mm %4463, %4461 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5205 = torch.constant.int 2
    %int16_5206 = torch.constant.int 16
    %int1280_5207 = torch.constant.int 1280
    %4465 = torch.prim.ListConstruct %int2_5205, %int16_5206, %int1280_5207 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4466 = torch.aten.view %4464, %4465 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_5208 = torch.constant.int 2
    %int-1_5209 = torch.constant.int -1
    %int20_5210 = torch.constant.int 20
    %int64_5211 = torch.constant.int 64
    %4467 = torch.prim.ListConstruct %int2_5208, %int-1_5209, %int20_5210, %int64_5211 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4468 = torch.aten.view %4452, %4467 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5212 = torch.constant.int 1
    %int2_5213 = torch.constant.int 2
    %4469 = torch.aten.transpose.int %4468, %int1_5212, %int2_5213 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5214 = torch.constant.int 2
    %int-1_5215 = torch.constant.int -1
    %int20_5216 = torch.constant.int 20
    %int64_5217 = torch.constant.int 64
    %4470 = torch.prim.ListConstruct %int2_5214, %int-1_5215, %int20_5216, %int64_5217 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4471 = torch.aten.view %4459, %4470 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5218 = torch.constant.int 1
    %int2_5219 = torch.constant.int 2
    %4472 = torch.aten.transpose.int %4471, %int1_5218, %int2_5219 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_5220 = torch.constant.int 2
    %int-1_5221 = torch.constant.int -1
    %int20_5222 = torch.constant.int 20
    %int64_5223 = torch.constant.int 64
    %4473 = torch.prim.ListConstruct %int2_5220, %int-1_5221, %int20_5222, %int64_5223 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4474 = torch.aten.view %4466, %4473 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5224 = torch.constant.int 1
    %int2_5225 = torch.constant.int 2
    %4475 = torch.aten.transpose.int %4474, %int1_5224, %int2_5225 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_5226 = torch.constant.float 0.000000e+00
    %false_5227 = torch.constant.bool false
    %none_5228 = torch.constant.none
    %none_5229 = torch.constant.none
    %4476:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4469, %4472, %4475, %float0.000000e00_5226, %false_5227, %none_5228, %none_5229) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5230 = torch.constant.int 1
    %int2_5231 = torch.constant.int 2
    %4477 = torch.aten.transpose.int %4476#0, %int1_5230, %int2_5231 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5232 = torch.constant.int 2
    %int-1_5233 = torch.constant.int -1
    %int1280_5234 = torch.constant.int 1280
    %4478 = torch.prim.ListConstruct %int2_5232, %int-1_5233, %int1280_5234 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4479 = torch.aten.view %4477, %4478 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5235 = torch.constant.int 5
    %4480 = torch.prims.convert_element_type %4479, %int5_5235 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5236 = torch.constant.int 1920
    %int1280_5237 = torch.constant.int 1280
    %4481 = torch.prim.ListConstruct %int1920_5236, %int1280_5237 : (!torch.int, !torch.int) -> !torch.list<int>
    %4482 = torch.aten.view %4480, %4481 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4483 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5238 = torch.constant.int 0
    %int1_5239 = torch.constant.int 1
    %4484 = torch.aten.transpose.int %4483, %int0_5238, %int1_5239 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %4485 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5240 = torch.constant.int 6
    %4486 = torch.prims.convert_element_type %4485, %int6_5240 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5241 = torch.constant.int 6
    %4487 = torch.prims.convert_element_type %4482, %int6_5241 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5242 = torch.constant.int 6
    %4488 = torch.prims.convert_element_type %4484, %int6_5242 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4489 = torch.aten.mm %4487, %4488 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5243 = torch.constant.int 1
    %4490 = torch.aten.mul.Scalar %4489, %int1_5243 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5244 = torch.constant.int 1
    %4491 = torch.aten.mul.Scalar %4486, %int1_5244 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5245 = torch.constant.int 1
    %4492 = torch.aten.add.Tensor %4490, %4491, %int1_5245 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5246 = torch.constant.int 5
    %4493 = torch.prims.convert_element_type %4492, %int5_5246 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5247 = torch.constant.int 2
    %int960_5248 = torch.constant.int 960
    %int1280_5249 = torch.constant.int 1280
    %4494 = torch.prim.ListConstruct %int2_5247, %int960_5248, %int1280_5249 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4495 = torch.aten.view %4493, %4494 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5250 = torch.constant.none
    %4496 = torch.aten.clone %4495, %none_5250 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5251 = torch.constant.float 1.000000e+00
    %4497 = torch.aten.div.Scalar %4496, %float1.000000e00_5251 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5252 = torch.constant.int 1
    %4498 = torch.aten.add.Tensor %4497, %4434, %int1_5252 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5253 = torch.constant.int 6
    %4499 = torch.prims.convert_element_type %4498, %int6_5253 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5254 = torch.constant.int 2
    %4500 = torch.prim.ListConstruct %int2_5254 : (!torch.int) -> !torch.list<int>
    %int0_5255 = torch.constant.int 0
    %true_5256 = torch.constant.bool true
    %result0_5257, %result1_5258 = torch.aten.var_mean.correction %4499, %4500, %int0_5255, %true_5256 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5259 = torch.constant.float 1.000000e-05
    %int1_5260 = torch.constant.int 1
    %4501 = torch.aten.add.Scalar %result0_5257, %float1.000000e-05_5259, %int1_5260 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4502 = torch.aten.rsqrt %4501 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5261 = torch.constant.int 1
    %4503 = torch.aten.sub.Tensor %4498, %result1_5258, %int1_5261 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4504 = torch.aten.mul.Tensor %4503, %4502 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %4505 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4506 = torch.aten.mul.Tensor %4504, %4505 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %4507 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5262 = torch.constant.int 1
    %4508 = torch.aten.add.Tensor %4506, %4507, %int1_5262 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5263 = torch.constant.int 5
    %4509 = torch.prims.convert_element_type %4508, %int5_5263 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5264 = torch.constant.int 1920
    %int1280_5265 = torch.constant.int 1280
    %4510 = torch.prim.ListConstruct %int1920_5264, %int1280_5265 : (!torch.int, !torch.int) -> !torch.list<int>
    %4511 = torch.aten.view %4509, %4510 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4512 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_5266 = torch.constant.int 0
    %int1_5267 = torch.constant.int 1
    %4513 = torch.aten.transpose.int %4512, %int0_5266, %int1_5267 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %4514 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_5268 = torch.constant.int 6
    %4515 = torch.prims.convert_element_type %4514, %int6_5268 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_5269 = torch.constant.int 6
    %4516 = torch.prims.convert_element_type %4511, %int6_5269 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5270 = torch.constant.int 6
    %4517 = torch.prims.convert_element_type %4513, %int6_5270 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4518 = torch.aten.mm %4516, %4517 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_5271 = torch.constant.int 1
    %4519 = torch.aten.mul.Scalar %4518, %int1_5271 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_5272 = torch.constant.int 1
    %4520 = torch.aten.mul.Scalar %4515, %int1_5272 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_5273 = torch.constant.int 1
    %4521 = torch.aten.add.Tensor %4519, %4520, %int1_5273 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_5274 = torch.constant.int 5
    %4522 = torch.prims.convert_element_type %4521, %int5_5274 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_5275 = torch.constant.int 2
    %int960_5276 = torch.constant.int 960
    %int10240_5277 = torch.constant.int 10240
    %4523 = torch.prim.ListConstruct %int2_5275, %int960_5276, %int10240_5277 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4524 = torch.aten.view %4522, %4523 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_5278 = torch.constant.int -1
    %int0_5279 = torch.constant.int 0
    %int5120_5280 = torch.constant.int 5120
    %int1_5281 = torch.constant.int 1
    %4525 = torch.aten.slice.Tensor %4524, %int-1_5278, %int0_5279, %int5120_5280, %int1_5281 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_5282 = torch.constant.int -1
    %int5120_5283 = torch.constant.int 5120
    %int10240_5284 = torch.constant.int 10240
    %int1_5285 = torch.constant.int 1
    %4526 = torch.aten.slice.Tensor %4524, %int-1_5282, %int5120_5283, %int10240_5284, %int1_5285 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_5286 = torch.constant.str "none"
    %4527 = torch.aten.gelu %4526, %str_5286 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %4528 = torch.aten.mul.Tensor %4525, %4527 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_5287 = torch.constant.none
    %4529 = torch.aten.clone %4528, %none_5287 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_5288 = torch.constant.int 1920
    %int5120_5289 = torch.constant.int 5120
    %4530 = torch.prim.ListConstruct %int1920_5288, %int5120_5289 : (!torch.int, !torch.int) -> !torch.list<int>
    %4531 = torch.aten.view %4529, %4530 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %4532 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5290 = torch.constant.int 0
    %int1_5291 = torch.constant.int 1
    %4533 = torch.aten.transpose.int %4532, %int0_5290, %int1_5291 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %4534 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5292 = torch.constant.int 6
    %4535 = torch.prims.convert_element_type %4534, %int6_5292 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5293 = torch.constant.int 6
    %4536 = torch.prims.convert_element_type %4531, %int6_5293 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_5294 = torch.constant.int 6
    %4537 = torch.prims.convert_element_type %4533, %int6_5294 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4538 = torch.aten.mm %4536, %4537 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5295 = torch.constant.int 1
    %4539 = torch.aten.mul.Scalar %4538, %int1_5295 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5296 = torch.constant.int 1
    %4540 = torch.aten.mul.Scalar %4535, %int1_5296 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5297 = torch.constant.int 1
    %4541 = torch.aten.add.Tensor %4539, %4540, %int1_5297 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5298 = torch.constant.int 5
    %4542 = torch.prims.convert_element_type %4541, %int5_5298 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5299 = torch.constant.int 2
    %int960_5300 = torch.constant.int 960
    %int1280_5301 = torch.constant.int 1280
    %4543 = torch.prim.ListConstruct %int2_5299, %int960_5300, %int1280_5301 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4544 = torch.aten.view %4542, %4543 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_5302 = torch.constant.int 1
    %4545 = torch.aten.add.Tensor %4544, %4498, %int1_5302 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5303 = torch.constant.int 6
    %4546 = torch.prims.convert_element_type %4545, %int6_5303 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5304 = torch.constant.int 2
    %4547 = torch.prim.ListConstruct %int2_5304 : (!torch.int) -> !torch.list<int>
    %int0_5305 = torch.constant.int 0
    %true_5306 = torch.constant.bool true
    %result0_5307, %result1_5308 = torch.aten.var_mean.correction %4546, %4547, %int0_5305, %true_5306 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5309 = torch.constant.float 1.000000e-05
    %int1_5310 = torch.constant.int 1
    %4548 = torch.aten.add.Scalar %result0_5307, %float1.000000e-05_5309, %int1_5310 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4549 = torch.aten.rsqrt %4548 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5311 = torch.constant.int 1
    %4550 = torch.aten.sub.Tensor %4545, %result1_5308, %int1_5311 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4551 = torch.aten.mul.Tensor %4550, %4549 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %4552 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4553 = torch.aten.mul.Tensor %4551, %4552 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %4554 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5312 = torch.constant.int 1
    %4555 = torch.aten.add.Tensor %4553, %4554, %int1_5312 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5313 = torch.constant.int 5
    %4556 = torch.prims.convert_element_type %4555, %int5_5313 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %4557 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5314 = torch.constant.int 0
    %int1_5315 = torch.constant.int 1
    %4558 = torch.aten.transpose.int %4557, %int0_5314, %int1_5315 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5316 = torch.constant.int 1920
    %int1280_5317 = torch.constant.int 1280
    %4559 = torch.prim.ListConstruct %int1920_5316, %int1280_5317 : (!torch.int, !torch.int) -> !torch.list<int>
    %4560 = torch.aten.view %4556, %4559 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4561 = torch.aten.mm %4560, %4558 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5318 = torch.constant.int 2
    %int960_5319 = torch.constant.int 960
    %int1280_5320 = torch.constant.int 1280
    %4562 = torch.prim.ListConstruct %int2_5318, %int960_5319, %int1280_5320 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4563 = torch.aten.view %4561, %4562 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %4564 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5321 = torch.constant.int 0
    %int1_5322 = torch.constant.int 1
    %4565 = torch.aten.transpose.int %4564, %int0_5321, %int1_5322 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5323 = torch.constant.int 1920
    %int1280_5324 = torch.constant.int 1280
    %4566 = torch.prim.ListConstruct %int1920_5323, %int1280_5324 : (!torch.int, !torch.int) -> !torch.list<int>
    %4567 = torch.aten.view %4556, %4566 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4568 = torch.aten.mm %4567, %4565 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5325 = torch.constant.int 2
    %int960_5326 = torch.constant.int 960
    %int1280_5327 = torch.constant.int 1280
    %4569 = torch.prim.ListConstruct %int2_5325, %int960_5326, %int1280_5327 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4570 = torch.aten.view %4568, %4569 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %4571 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5328 = torch.constant.int 0
    %int1_5329 = torch.constant.int 1
    %4572 = torch.aten.transpose.int %4571, %int0_5328, %int1_5329 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5330 = torch.constant.int 1920
    %int1280_5331 = torch.constant.int 1280
    %4573 = torch.prim.ListConstruct %int1920_5330, %int1280_5331 : (!torch.int, !torch.int) -> !torch.list<int>
    %4574 = torch.aten.view %4556, %4573 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4575 = torch.aten.mm %4574, %4572 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5332 = torch.constant.int 2
    %int960_5333 = torch.constant.int 960
    %int1280_5334 = torch.constant.int 1280
    %4576 = torch.prim.ListConstruct %int2_5332, %int960_5333, %int1280_5334 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4577 = torch.aten.view %4575, %4576 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_5335 = torch.constant.int 2
    %int-1_5336 = torch.constant.int -1
    %int20_5337 = torch.constant.int 20
    %int64_5338 = torch.constant.int 64
    %4578 = torch.prim.ListConstruct %int2_5335, %int-1_5336, %int20_5337, %int64_5338 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4579 = torch.aten.view %4563, %4578 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5339 = torch.constant.int 1
    %int2_5340 = torch.constant.int 2
    %4580 = torch.aten.transpose.int %4579, %int1_5339, %int2_5340 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5341 = torch.constant.int 2
    %int-1_5342 = torch.constant.int -1
    %int20_5343 = torch.constant.int 20
    %int64_5344 = torch.constant.int 64
    %4581 = torch.prim.ListConstruct %int2_5341, %int-1_5342, %int20_5343, %int64_5344 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4582 = torch.aten.view %4570, %4581 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5345 = torch.constant.int 1
    %int2_5346 = torch.constant.int 2
    %4583 = torch.aten.transpose.int %4582, %int1_5345, %int2_5346 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5347 = torch.constant.int 2
    %int-1_5348 = torch.constant.int -1
    %int20_5349 = torch.constant.int 20
    %int64_5350 = torch.constant.int 64
    %4584 = torch.prim.ListConstruct %int2_5347, %int-1_5348, %int20_5349, %int64_5350 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4585 = torch.aten.view %4577, %4584 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5351 = torch.constant.int 1
    %int2_5352 = torch.constant.int 2
    %4586 = torch.aten.transpose.int %4585, %int1_5351, %int2_5352 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_5353 = torch.constant.float 0.000000e+00
    %false_5354 = torch.constant.bool false
    %none_5355 = torch.constant.none
    %none_5356 = torch.constant.none
    %4587:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4580, %4583, %4586, %float0.000000e00_5353, %false_5354, %none_5355, %none_5356) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5357 = torch.constant.int 1
    %int2_5358 = torch.constant.int 2
    %4588 = torch.aten.transpose.int %4587#0, %int1_5357, %int2_5358 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5359 = torch.constant.int 2
    %int-1_5360 = torch.constant.int -1
    %int1280_5361 = torch.constant.int 1280
    %4589 = torch.prim.ListConstruct %int2_5359, %int-1_5360, %int1280_5361 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4590 = torch.aten.view %4588, %4589 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5362 = torch.constant.int 5
    %4591 = torch.prims.convert_element_type %4590, %int5_5362 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5363 = torch.constant.int 1920
    %int1280_5364 = torch.constant.int 1280
    %4592 = torch.prim.ListConstruct %int1920_5363, %int1280_5364 : (!torch.int, !torch.int) -> !torch.list<int>
    %4593 = torch.aten.view %4591, %4592 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4594 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5365 = torch.constant.int 0
    %int1_5366 = torch.constant.int 1
    %4595 = torch.aten.transpose.int %4594, %int0_5365, %int1_5366 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %4596 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5367 = torch.constant.int 6
    %4597 = torch.prims.convert_element_type %4596, %int6_5367 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5368 = torch.constant.int 6
    %4598 = torch.prims.convert_element_type %4593, %int6_5368 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5369 = torch.constant.int 6
    %4599 = torch.prims.convert_element_type %4595, %int6_5369 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4600 = torch.aten.mm %4598, %4599 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5370 = torch.constant.int 1
    %4601 = torch.aten.mul.Scalar %4600, %int1_5370 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5371 = torch.constant.int 1
    %4602 = torch.aten.mul.Scalar %4597, %int1_5371 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5372 = torch.constant.int 1
    %4603 = torch.aten.add.Tensor %4601, %4602, %int1_5372 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5373 = torch.constant.int 5
    %4604 = torch.prims.convert_element_type %4603, %int5_5373 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5374 = torch.constant.int 2
    %int960_5375 = torch.constant.int 960
    %int1280_5376 = torch.constant.int 1280
    %4605 = torch.prim.ListConstruct %int2_5374, %int960_5375, %int1280_5376 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4606 = torch.aten.view %4604, %4605 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5377 = torch.constant.none
    %4607 = torch.aten.clone %4606, %none_5377 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5378 = torch.constant.float 1.000000e+00
    %4608 = torch.aten.div.Scalar %4607, %float1.000000e00_5378 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5379 = torch.constant.int 1
    %4609 = torch.aten.add.Tensor %4608, %4545, %int1_5379 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5380 = torch.constant.int 6
    %4610 = torch.prims.convert_element_type %4609, %int6_5380 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5381 = torch.constant.int 2
    %4611 = torch.prim.ListConstruct %int2_5381 : (!torch.int) -> !torch.list<int>
    %int0_5382 = torch.constant.int 0
    %true_5383 = torch.constant.bool true
    %result0_5384, %result1_5385 = torch.aten.var_mean.correction %4610, %4611, %int0_5382, %true_5383 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5386 = torch.constant.float 1.000000e-05
    %int1_5387 = torch.constant.int 1
    %4612 = torch.aten.add.Scalar %result0_5384, %float1.000000e-05_5386, %int1_5387 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4613 = torch.aten.rsqrt %4612 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5388 = torch.constant.int 1
    %4614 = torch.aten.sub.Tensor %4609, %result1_5385, %int1_5388 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4615 = torch.aten.mul.Tensor %4614, %4613 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %4616 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4617 = torch.aten.mul.Tensor %4615, %4616 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %4618 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5389 = torch.constant.int 1
    %4619 = torch.aten.add.Tensor %4617, %4618, %int1_5389 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5390 = torch.constant.int 5
    %4620 = torch.prims.convert_element_type %4619, %int5_5390 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %4621 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5391 = torch.constant.int 0
    %int1_5392 = torch.constant.int 1
    %4622 = torch.aten.transpose.int %4621, %int0_5391, %int1_5392 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5393 = torch.constant.int 1920
    %int1280_5394 = torch.constant.int 1280
    %4623 = torch.prim.ListConstruct %int1920_5393, %int1280_5394 : (!torch.int, !torch.int) -> !torch.list<int>
    %4624 = torch.aten.view %4620, %4623 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4625 = torch.aten.mm %4624, %4622 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5395 = torch.constant.int 2
    %int960_5396 = torch.constant.int 960
    %int1280_5397 = torch.constant.int 1280
    %4626 = torch.prim.ListConstruct %int2_5395, %int960_5396, %int1280_5397 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4627 = torch.aten.view %4625, %4626 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %4628 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5398 = torch.constant.int 0
    %int1_5399 = torch.constant.int 1
    %4629 = torch.aten.transpose.int %4628, %int0_5398, %int1_5399 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5400 = torch.constant.int 32
    %int2048_5401 = torch.constant.int 2048
    %4630 = torch.prim.ListConstruct %int32_5400, %int2048_5401 : (!torch.int, !torch.int) -> !torch.list<int>
    %4631 = torch.aten.view %arg6, %4630 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4632 = torch.aten.mm %4631, %4629 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5402 = torch.constant.int 2
    %int16_5403 = torch.constant.int 16
    %int1280_5404 = torch.constant.int 1280
    %4633 = torch.prim.ListConstruct %int2_5402, %int16_5403, %int1280_5404 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4634 = torch.aten.view %4632, %4633 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %4635 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5405 = torch.constant.int 0
    %int1_5406 = torch.constant.int 1
    %4636 = torch.aten.transpose.int %4635, %int0_5405, %int1_5406 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5407 = torch.constant.int 32
    %int2048_5408 = torch.constant.int 2048
    %4637 = torch.prim.ListConstruct %int32_5407, %int2048_5408 : (!torch.int, !torch.int) -> !torch.list<int>
    %4638 = torch.aten.view %arg6, %4637 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4639 = torch.aten.mm %4638, %4636 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5409 = torch.constant.int 2
    %int16_5410 = torch.constant.int 16
    %int1280_5411 = torch.constant.int 1280
    %4640 = torch.prim.ListConstruct %int2_5409, %int16_5410, %int1280_5411 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4641 = torch.aten.view %4639, %4640 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_5412 = torch.constant.int 2
    %int-1_5413 = torch.constant.int -1
    %int20_5414 = torch.constant.int 20
    %int64_5415 = torch.constant.int 64
    %4642 = torch.prim.ListConstruct %int2_5412, %int-1_5413, %int20_5414, %int64_5415 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4643 = torch.aten.view %4627, %4642 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5416 = torch.constant.int 1
    %int2_5417 = torch.constant.int 2
    %4644 = torch.aten.transpose.int %4643, %int1_5416, %int2_5417 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5418 = torch.constant.int 2
    %int-1_5419 = torch.constant.int -1
    %int20_5420 = torch.constant.int 20
    %int64_5421 = torch.constant.int 64
    %4645 = torch.prim.ListConstruct %int2_5418, %int-1_5419, %int20_5420, %int64_5421 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4646 = torch.aten.view %4634, %4645 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5422 = torch.constant.int 1
    %int2_5423 = torch.constant.int 2
    %4647 = torch.aten.transpose.int %4646, %int1_5422, %int2_5423 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_5424 = torch.constant.int 2
    %int-1_5425 = torch.constant.int -1
    %int20_5426 = torch.constant.int 20
    %int64_5427 = torch.constant.int 64
    %4648 = torch.prim.ListConstruct %int2_5424, %int-1_5425, %int20_5426, %int64_5427 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4649 = torch.aten.view %4641, %4648 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5428 = torch.constant.int 1
    %int2_5429 = torch.constant.int 2
    %4650 = torch.aten.transpose.int %4649, %int1_5428, %int2_5429 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_5430 = torch.constant.float 0.000000e+00
    %false_5431 = torch.constant.bool false
    %none_5432 = torch.constant.none
    %none_5433 = torch.constant.none
    %4651:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4644, %4647, %4650, %float0.000000e00_5430, %false_5431, %none_5432, %none_5433) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5434 = torch.constant.int 1
    %int2_5435 = torch.constant.int 2
    %4652 = torch.aten.transpose.int %4651#0, %int1_5434, %int2_5435 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5436 = torch.constant.int 2
    %int-1_5437 = torch.constant.int -1
    %int1280_5438 = torch.constant.int 1280
    %4653 = torch.prim.ListConstruct %int2_5436, %int-1_5437, %int1280_5438 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4654 = torch.aten.view %4652, %4653 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5439 = torch.constant.int 5
    %4655 = torch.prims.convert_element_type %4654, %int5_5439 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5440 = torch.constant.int 1920
    %int1280_5441 = torch.constant.int 1280
    %4656 = torch.prim.ListConstruct %int1920_5440, %int1280_5441 : (!torch.int, !torch.int) -> !torch.list<int>
    %4657 = torch.aten.view %4655, %4656 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4658 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5442 = torch.constant.int 0
    %int1_5443 = torch.constant.int 1
    %4659 = torch.aten.transpose.int %4658, %int0_5442, %int1_5443 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %4660 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5444 = torch.constant.int 6
    %4661 = torch.prims.convert_element_type %4660, %int6_5444 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5445 = torch.constant.int 6
    %4662 = torch.prims.convert_element_type %4657, %int6_5445 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5446 = torch.constant.int 6
    %4663 = torch.prims.convert_element_type %4659, %int6_5446 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4664 = torch.aten.mm %4662, %4663 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5447 = torch.constant.int 1
    %4665 = torch.aten.mul.Scalar %4664, %int1_5447 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5448 = torch.constant.int 1
    %4666 = torch.aten.mul.Scalar %4661, %int1_5448 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5449 = torch.constant.int 1
    %4667 = torch.aten.add.Tensor %4665, %4666, %int1_5449 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5450 = torch.constant.int 5
    %4668 = torch.prims.convert_element_type %4667, %int5_5450 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5451 = torch.constant.int 2
    %int960_5452 = torch.constant.int 960
    %int1280_5453 = torch.constant.int 1280
    %4669 = torch.prim.ListConstruct %int2_5451, %int960_5452, %int1280_5453 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4670 = torch.aten.view %4668, %4669 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5454 = torch.constant.none
    %4671 = torch.aten.clone %4670, %none_5454 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5455 = torch.constant.float 1.000000e+00
    %4672 = torch.aten.div.Scalar %4671, %float1.000000e00_5455 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5456 = torch.constant.int 1
    %4673 = torch.aten.add.Tensor %4672, %4609, %int1_5456 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5457 = torch.constant.int 6
    %4674 = torch.prims.convert_element_type %4673, %int6_5457 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5458 = torch.constant.int 2
    %4675 = torch.prim.ListConstruct %int2_5458 : (!torch.int) -> !torch.list<int>
    %int0_5459 = torch.constant.int 0
    %true_5460 = torch.constant.bool true
    %result0_5461, %result1_5462 = torch.aten.var_mean.correction %4674, %4675, %int0_5459, %true_5460 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5463 = torch.constant.float 1.000000e-05
    %int1_5464 = torch.constant.int 1
    %4676 = torch.aten.add.Scalar %result0_5461, %float1.000000e-05_5463, %int1_5464 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4677 = torch.aten.rsqrt %4676 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5465 = torch.constant.int 1
    %4678 = torch.aten.sub.Tensor %4673, %result1_5462, %int1_5465 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4679 = torch.aten.mul.Tensor %4678, %4677 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %4680 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4681 = torch.aten.mul.Tensor %4679, %4680 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %4682 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5466 = torch.constant.int 1
    %4683 = torch.aten.add.Tensor %4681, %4682, %int1_5466 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5467 = torch.constant.int 5
    %4684 = torch.prims.convert_element_type %4683, %int5_5467 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5468 = torch.constant.int 1920
    %int1280_5469 = torch.constant.int 1280
    %4685 = torch.prim.ListConstruct %int1920_5468, %int1280_5469 : (!torch.int, !torch.int) -> !torch.list<int>
    %4686 = torch.aten.view %4684, %4685 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4687 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_5470 = torch.constant.int 0
    %int1_5471 = torch.constant.int 1
    %4688 = torch.aten.transpose.int %4687, %int0_5470, %int1_5471 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %4689 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_5472 = torch.constant.int 6
    %4690 = torch.prims.convert_element_type %4689, %int6_5472 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_5473 = torch.constant.int 6
    %4691 = torch.prims.convert_element_type %4686, %int6_5473 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5474 = torch.constant.int 6
    %4692 = torch.prims.convert_element_type %4688, %int6_5474 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4693 = torch.aten.mm %4691, %4692 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_5475 = torch.constant.int 1
    %4694 = torch.aten.mul.Scalar %4693, %int1_5475 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_5476 = torch.constant.int 1
    %4695 = torch.aten.mul.Scalar %4690, %int1_5476 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_5477 = torch.constant.int 1
    %4696 = torch.aten.add.Tensor %4694, %4695, %int1_5477 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_5478 = torch.constant.int 5
    %4697 = torch.prims.convert_element_type %4696, %int5_5478 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_5479 = torch.constant.int 2
    %int960_5480 = torch.constant.int 960
    %int10240_5481 = torch.constant.int 10240
    %4698 = torch.prim.ListConstruct %int2_5479, %int960_5480, %int10240_5481 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4699 = torch.aten.view %4697, %4698 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_5482 = torch.constant.int -1
    %int0_5483 = torch.constant.int 0
    %int5120_5484 = torch.constant.int 5120
    %int1_5485 = torch.constant.int 1
    %4700 = torch.aten.slice.Tensor %4699, %int-1_5482, %int0_5483, %int5120_5484, %int1_5485 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_5486 = torch.constant.int -1
    %int5120_5487 = torch.constant.int 5120
    %int10240_5488 = torch.constant.int 10240
    %int1_5489 = torch.constant.int 1
    %4701 = torch.aten.slice.Tensor %4699, %int-1_5486, %int5120_5487, %int10240_5488, %int1_5489 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_5490 = torch.constant.str "none"
    %4702 = torch.aten.gelu %4701, %str_5490 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %4703 = torch.aten.mul.Tensor %4700, %4702 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_5491 = torch.constant.none
    %4704 = torch.aten.clone %4703, %none_5491 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_5492 = torch.constant.int 1920
    %int5120_5493 = torch.constant.int 5120
    %4705 = torch.prim.ListConstruct %int1920_5492, %int5120_5493 : (!torch.int, !torch.int) -> !torch.list<int>
    %4706 = torch.aten.view %4704, %4705 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %4707 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5494 = torch.constant.int 0
    %int1_5495 = torch.constant.int 1
    %4708 = torch.aten.transpose.int %4707, %int0_5494, %int1_5495 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %4709 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5496 = torch.constant.int 6
    %4710 = torch.prims.convert_element_type %4709, %int6_5496 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5497 = torch.constant.int 6
    %4711 = torch.prims.convert_element_type %4706, %int6_5497 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_5498 = torch.constant.int 6
    %4712 = torch.prims.convert_element_type %4708, %int6_5498 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4713 = torch.aten.mm %4711, %4712 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5499 = torch.constant.int 1
    %4714 = torch.aten.mul.Scalar %4713, %int1_5499 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5500 = torch.constant.int 1
    %4715 = torch.aten.mul.Scalar %4710, %int1_5500 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5501 = torch.constant.int 1
    %4716 = torch.aten.add.Tensor %4714, %4715, %int1_5501 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5502 = torch.constant.int 5
    %4717 = torch.prims.convert_element_type %4716, %int5_5502 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5503 = torch.constant.int 2
    %int960_5504 = torch.constant.int 960
    %int1280_5505 = torch.constant.int 1280
    %4718 = torch.prim.ListConstruct %int2_5503, %int960_5504, %int1280_5505 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4719 = torch.aten.view %4717, %4718 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_5506 = torch.constant.int 1
    %4720 = torch.aten.add.Tensor %4719, %4673, %int1_5506 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5507 = torch.constant.int 6
    %4721 = torch.prims.convert_element_type %4720, %int6_5507 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5508 = torch.constant.int 2
    %4722 = torch.prim.ListConstruct %int2_5508 : (!torch.int) -> !torch.list<int>
    %int0_5509 = torch.constant.int 0
    %true_5510 = torch.constant.bool true
    %result0_5511, %result1_5512 = torch.aten.var_mean.correction %4721, %4722, %int0_5509, %true_5510 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5513 = torch.constant.float 1.000000e-05
    %int1_5514 = torch.constant.int 1
    %4723 = torch.aten.add.Scalar %result0_5511, %float1.000000e-05_5513, %int1_5514 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4724 = torch.aten.rsqrt %4723 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5515 = torch.constant.int 1
    %4725 = torch.aten.sub.Tensor %4720, %result1_5512, %int1_5515 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4726 = torch.aten.mul.Tensor %4725, %4724 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %4727 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4728 = torch.aten.mul.Tensor %4726, %4727 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %4729 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5516 = torch.constant.int 1
    %4730 = torch.aten.add.Tensor %4728, %4729, %int1_5516 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5517 = torch.constant.int 5
    %4731 = torch.prims.convert_element_type %4730, %int5_5517 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %4732 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5518 = torch.constant.int 0
    %int1_5519 = torch.constant.int 1
    %4733 = torch.aten.transpose.int %4732, %int0_5518, %int1_5519 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5520 = torch.constant.int 1920
    %int1280_5521 = torch.constant.int 1280
    %4734 = torch.prim.ListConstruct %int1920_5520, %int1280_5521 : (!torch.int, !torch.int) -> !torch.list<int>
    %4735 = torch.aten.view %4731, %4734 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4736 = torch.aten.mm %4735, %4733 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5522 = torch.constant.int 2
    %int960_5523 = torch.constant.int 960
    %int1280_5524 = torch.constant.int 1280
    %4737 = torch.prim.ListConstruct %int2_5522, %int960_5523, %int1280_5524 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4738 = torch.aten.view %4736, %4737 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %4739 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5525 = torch.constant.int 0
    %int1_5526 = torch.constant.int 1
    %4740 = torch.aten.transpose.int %4739, %int0_5525, %int1_5526 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5527 = torch.constant.int 1920
    %int1280_5528 = torch.constant.int 1280
    %4741 = torch.prim.ListConstruct %int1920_5527, %int1280_5528 : (!torch.int, !torch.int) -> !torch.list<int>
    %4742 = torch.aten.view %4731, %4741 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4743 = torch.aten.mm %4742, %4740 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5529 = torch.constant.int 2
    %int960_5530 = torch.constant.int 960
    %int1280_5531 = torch.constant.int 1280
    %4744 = torch.prim.ListConstruct %int2_5529, %int960_5530, %int1280_5531 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4745 = torch.aten.view %4743, %4744 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %4746 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5532 = torch.constant.int 0
    %int1_5533 = torch.constant.int 1
    %4747 = torch.aten.transpose.int %4746, %int0_5532, %int1_5533 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5534 = torch.constant.int 1920
    %int1280_5535 = torch.constant.int 1280
    %4748 = torch.prim.ListConstruct %int1920_5534, %int1280_5535 : (!torch.int, !torch.int) -> !torch.list<int>
    %4749 = torch.aten.view %4731, %4748 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4750 = torch.aten.mm %4749, %4747 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5536 = torch.constant.int 2
    %int960_5537 = torch.constant.int 960
    %int1280_5538 = torch.constant.int 1280
    %4751 = torch.prim.ListConstruct %int2_5536, %int960_5537, %int1280_5538 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4752 = torch.aten.view %4750, %4751 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_5539 = torch.constant.int 2
    %int-1_5540 = torch.constant.int -1
    %int20_5541 = torch.constant.int 20
    %int64_5542 = torch.constant.int 64
    %4753 = torch.prim.ListConstruct %int2_5539, %int-1_5540, %int20_5541, %int64_5542 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4754 = torch.aten.view %4738, %4753 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5543 = torch.constant.int 1
    %int2_5544 = torch.constant.int 2
    %4755 = torch.aten.transpose.int %4754, %int1_5543, %int2_5544 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5545 = torch.constant.int 2
    %int-1_5546 = torch.constant.int -1
    %int20_5547 = torch.constant.int 20
    %int64_5548 = torch.constant.int 64
    %4756 = torch.prim.ListConstruct %int2_5545, %int-1_5546, %int20_5547, %int64_5548 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4757 = torch.aten.view %4745, %4756 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5549 = torch.constant.int 1
    %int2_5550 = torch.constant.int 2
    %4758 = torch.aten.transpose.int %4757, %int1_5549, %int2_5550 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5551 = torch.constant.int 2
    %int-1_5552 = torch.constant.int -1
    %int20_5553 = torch.constant.int 20
    %int64_5554 = torch.constant.int 64
    %4759 = torch.prim.ListConstruct %int2_5551, %int-1_5552, %int20_5553, %int64_5554 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4760 = torch.aten.view %4752, %4759 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5555 = torch.constant.int 1
    %int2_5556 = torch.constant.int 2
    %4761 = torch.aten.transpose.int %4760, %int1_5555, %int2_5556 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_5557 = torch.constant.float 0.000000e+00
    %false_5558 = torch.constant.bool false
    %none_5559 = torch.constant.none
    %none_5560 = torch.constant.none
    %4762:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4755, %4758, %4761, %float0.000000e00_5557, %false_5558, %none_5559, %none_5560) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5561 = torch.constant.int 1
    %int2_5562 = torch.constant.int 2
    %4763 = torch.aten.transpose.int %4762#0, %int1_5561, %int2_5562 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5563 = torch.constant.int 2
    %int-1_5564 = torch.constant.int -1
    %int1280_5565 = torch.constant.int 1280
    %4764 = torch.prim.ListConstruct %int2_5563, %int-1_5564, %int1280_5565 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4765 = torch.aten.view %4763, %4764 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5566 = torch.constant.int 5
    %4766 = torch.prims.convert_element_type %4765, %int5_5566 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5567 = torch.constant.int 1920
    %int1280_5568 = torch.constant.int 1280
    %4767 = torch.prim.ListConstruct %int1920_5567, %int1280_5568 : (!torch.int, !torch.int) -> !torch.list<int>
    %4768 = torch.aten.view %4766, %4767 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4769 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5569 = torch.constant.int 0
    %int1_5570 = torch.constant.int 1
    %4770 = torch.aten.transpose.int %4769, %int0_5569, %int1_5570 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %4771 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5571 = torch.constant.int 6
    %4772 = torch.prims.convert_element_type %4771, %int6_5571 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5572 = torch.constant.int 6
    %4773 = torch.prims.convert_element_type %4768, %int6_5572 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5573 = torch.constant.int 6
    %4774 = torch.prims.convert_element_type %4770, %int6_5573 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4775 = torch.aten.mm %4773, %4774 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5574 = torch.constant.int 1
    %4776 = torch.aten.mul.Scalar %4775, %int1_5574 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5575 = torch.constant.int 1
    %4777 = torch.aten.mul.Scalar %4772, %int1_5575 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5576 = torch.constant.int 1
    %4778 = torch.aten.add.Tensor %4776, %4777, %int1_5576 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5577 = torch.constant.int 5
    %4779 = torch.prims.convert_element_type %4778, %int5_5577 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5578 = torch.constant.int 2
    %int960_5579 = torch.constant.int 960
    %int1280_5580 = torch.constant.int 1280
    %4780 = torch.prim.ListConstruct %int2_5578, %int960_5579, %int1280_5580 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4781 = torch.aten.view %4779, %4780 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5581 = torch.constant.none
    %4782 = torch.aten.clone %4781, %none_5581 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5582 = torch.constant.float 1.000000e+00
    %4783 = torch.aten.div.Scalar %4782, %float1.000000e00_5582 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5583 = torch.constant.int 1
    %4784 = torch.aten.add.Tensor %4783, %4720, %int1_5583 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5584 = torch.constant.int 6
    %4785 = torch.prims.convert_element_type %4784, %int6_5584 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5585 = torch.constant.int 2
    %4786 = torch.prim.ListConstruct %int2_5585 : (!torch.int) -> !torch.list<int>
    %int0_5586 = torch.constant.int 0
    %true_5587 = torch.constant.bool true
    %result0_5588, %result1_5589 = torch.aten.var_mean.correction %4785, %4786, %int0_5586, %true_5587 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5590 = torch.constant.float 1.000000e-05
    %int1_5591 = torch.constant.int 1
    %4787 = torch.aten.add.Scalar %result0_5588, %float1.000000e-05_5590, %int1_5591 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4788 = torch.aten.rsqrt %4787 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5592 = torch.constant.int 1
    %4789 = torch.aten.sub.Tensor %4784, %result1_5589, %int1_5592 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4790 = torch.aten.mul.Tensor %4789, %4788 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %4791 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4792 = torch.aten.mul.Tensor %4790, %4791 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %4793 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5593 = torch.constant.int 1
    %4794 = torch.aten.add.Tensor %4792, %4793, %int1_5593 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5594 = torch.constant.int 5
    %4795 = torch.prims.convert_element_type %4794, %int5_5594 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %4796 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5595 = torch.constant.int 0
    %int1_5596 = torch.constant.int 1
    %4797 = torch.aten.transpose.int %4796, %int0_5595, %int1_5596 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5597 = torch.constant.int 1920
    %int1280_5598 = torch.constant.int 1280
    %4798 = torch.prim.ListConstruct %int1920_5597, %int1280_5598 : (!torch.int, !torch.int) -> !torch.list<int>
    %4799 = torch.aten.view %4795, %4798 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4800 = torch.aten.mm %4799, %4797 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5599 = torch.constant.int 2
    %int960_5600 = torch.constant.int 960
    %int1280_5601 = torch.constant.int 1280
    %4801 = torch.prim.ListConstruct %int2_5599, %int960_5600, %int1280_5601 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4802 = torch.aten.view %4800, %4801 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %4803 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5602 = torch.constant.int 0
    %int1_5603 = torch.constant.int 1
    %4804 = torch.aten.transpose.int %4803, %int0_5602, %int1_5603 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5604 = torch.constant.int 32
    %int2048_5605 = torch.constant.int 2048
    %4805 = torch.prim.ListConstruct %int32_5604, %int2048_5605 : (!torch.int, !torch.int) -> !torch.list<int>
    %4806 = torch.aten.view %arg6, %4805 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4807 = torch.aten.mm %4806, %4804 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5606 = torch.constant.int 2
    %int16_5607 = torch.constant.int 16
    %int1280_5608 = torch.constant.int 1280
    %4808 = torch.prim.ListConstruct %int2_5606, %int16_5607, %int1280_5608 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4809 = torch.aten.view %4807, %4808 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %4810 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5609 = torch.constant.int 0
    %int1_5610 = torch.constant.int 1
    %4811 = torch.aten.transpose.int %4810, %int0_5609, %int1_5610 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5611 = torch.constant.int 32
    %int2048_5612 = torch.constant.int 2048
    %4812 = torch.prim.ListConstruct %int32_5611, %int2048_5612 : (!torch.int, !torch.int) -> !torch.list<int>
    %4813 = torch.aten.view %arg6, %4812 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4814 = torch.aten.mm %4813, %4811 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5613 = torch.constant.int 2
    %int16_5614 = torch.constant.int 16
    %int1280_5615 = torch.constant.int 1280
    %4815 = torch.prim.ListConstruct %int2_5613, %int16_5614, %int1280_5615 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4816 = torch.aten.view %4814, %4815 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_5616 = torch.constant.int 2
    %int-1_5617 = torch.constant.int -1
    %int20_5618 = torch.constant.int 20
    %int64_5619 = torch.constant.int 64
    %4817 = torch.prim.ListConstruct %int2_5616, %int-1_5617, %int20_5618, %int64_5619 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4818 = torch.aten.view %4802, %4817 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5620 = torch.constant.int 1
    %int2_5621 = torch.constant.int 2
    %4819 = torch.aten.transpose.int %4818, %int1_5620, %int2_5621 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5622 = torch.constant.int 2
    %int-1_5623 = torch.constant.int -1
    %int20_5624 = torch.constant.int 20
    %int64_5625 = torch.constant.int 64
    %4820 = torch.prim.ListConstruct %int2_5622, %int-1_5623, %int20_5624, %int64_5625 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4821 = torch.aten.view %4809, %4820 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5626 = torch.constant.int 1
    %int2_5627 = torch.constant.int 2
    %4822 = torch.aten.transpose.int %4821, %int1_5626, %int2_5627 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_5628 = torch.constant.int 2
    %int-1_5629 = torch.constant.int -1
    %int20_5630 = torch.constant.int 20
    %int64_5631 = torch.constant.int 64
    %4823 = torch.prim.ListConstruct %int2_5628, %int-1_5629, %int20_5630, %int64_5631 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4824 = torch.aten.view %4816, %4823 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5632 = torch.constant.int 1
    %int2_5633 = torch.constant.int 2
    %4825 = torch.aten.transpose.int %4824, %int1_5632, %int2_5633 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_5634 = torch.constant.float 0.000000e+00
    %false_5635 = torch.constant.bool false
    %none_5636 = torch.constant.none
    %none_5637 = torch.constant.none
    %4826:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4819, %4822, %4825, %float0.000000e00_5634, %false_5635, %none_5636, %none_5637) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5638 = torch.constant.int 1
    %int2_5639 = torch.constant.int 2
    %4827 = torch.aten.transpose.int %4826#0, %int1_5638, %int2_5639 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5640 = torch.constant.int 2
    %int-1_5641 = torch.constant.int -1
    %int1280_5642 = torch.constant.int 1280
    %4828 = torch.prim.ListConstruct %int2_5640, %int-1_5641, %int1280_5642 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4829 = torch.aten.view %4827, %4828 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5643 = torch.constant.int 5
    %4830 = torch.prims.convert_element_type %4829, %int5_5643 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5644 = torch.constant.int 1920
    %int1280_5645 = torch.constant.int 1280
    %4831 = torch.prim.ListConstruct %int1920_5644, %int1280_5645 : (!torch.int, !torch.int) -> !torch.list<int>
    %4832 = torch.aten.view %4830, %4831 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %4833 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5646 = torch.constant.int 0
    %int1_5647 = torch.constant.int 1
    %4834 = torch.aten.transpose.int %4833, %int0_5646, %int1_5647 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %4835 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5648 = torch.constant.int 6
    %4836 = torch.prims.convert_element_type %4835, %int6_5648 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5649 = torch.constant.int 6
    %4837 = torch.prims.convert_element_type %4832, %int6_5649 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5650 = torch.constant.int 6
    %4838 = torch.prims.convert_element_type %4834, %int6_5650 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4839 = torch.aten.mm %4837, %4838 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5651 = torch.constant.int 1
    %4840 = torch.aten.mul.Scalar %4839, %int1_5651 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5652 = torch.constant.int 1
    %4841 = torch.aten.mul.Scalar %4836, %int1_5652 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5653 = torch.constant.int 1
    %4842 = torch.aten.add.Tensor %4840, %4841, %int1_5653 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5654 = torch.constant.int 5
    %4843 = torch.prims.convert_element_type %4842, %int5_5654 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5655 = torch.constant.int 2
    %int960_5656 = torch.constant.int 960
    %int1280_5657 = torch.constant.int 1280
    %4844 = torch.prim.ListConstruct %int2_5655, %int960_5656, %int1280_5657 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4845 = torch.aten.view %4843, %4844 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5658 = torch.constant.none
    %4846 = torch.aten.clone %4845, %none_5658 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5659 = torch.constant.float 1.000000e+00
    %4847 = torch.aten.div.Scalar %4846, %float1.000000e00_5659 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5660 = torch.constant.int 1
    %4848 = torch.aten.add.Tensor %4847, %4784, %int1_5660 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5661 = torch.constant.int 6
    %4849 = torch.prims.convert_element_type %4848, %int6_5661 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5662 = torch.constant.int 2
    %4850 = torch.prim.ListConstruct %int2_5662 : (!torch.int) -> !torch.list<int>
    %int0_5663 = torch.constant.int 0
    %true_5664 = torch.constant.bool true
    %result0_5665, %result1_5666 = torch.aten.var_mean.correction %4849, %4850, %int0_5663, %true_5664 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5667 = torch.constant.float 1.000000e-05
    %int1_5668 = torch.constant.int 1
    %4851 = torch.aten.add.Scalar %result0_5665, %float1.000000e-05_5667, %int1_5668 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4852 = torch.aten.rsqrt %4851 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5669 = torch.constant.int 1
    %4853 = torch.aten.sub.Tensor %4848, %result1_5666, %int1_5669 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4854 = torch.aten.mul.Tensor %4853, %4852 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %4855 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4856 = torch.aten.mul.Tensor %4854, %4855 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %4857 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5670 = torch.constant.int 1
    %4858 = torch.aten.add.Tensor %4856, %4857, %int1_5670 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5671 = torch.constant.int 5
    %4859 = torch.prims.convert_element_type %4858, %int5_5671 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5672 = torch.constant.int 1920
    %int1280_5673 = torch.constant.int 1280
    %4860 = torch.prim.ListConstruct %int1920_5672, %int1280_5673 : (!torch.int, !torch.int) -> !torch.list<int>
    %4861 = torch.aten.view %4859, %4860 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %4862 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_5674 = torch.constant.int 0
    %int1_5675 = torch.constant.int 1
    %4863 = torch.aten.transpose.int %4862, %int0_5674, %int1_5675 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %4864 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_5676 = torch.constant.int 6
    %4865 = torch.prims.convert_element_type %4864, %int6_5676 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_5677 = torch.constant.int 6
    %4866 = torch.prims.convert_element_type %4861, %int6_5677 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5678 = torch.constant.int 6
    %4867 = torch.prims.convert_element_type %4863, %int6_5678 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %4868 = torch.aten.mm %4866, %4867 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_5679 = torch.constant.int 1
    %4869 = torch.aten.mul.Scalar %4868, %int1_5679 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_5680 = torch.constant.int 1
    %4870 = torch.aten.mul.Scalar %4865, %int1_5680 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_5681 = torch.constant.int 1
    %4871 = torch.aten.add.Tensor %4869, %4870, %int1_5681 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_5682 = torch.constant.int 5
    %4872 = torch.prims.convert_element_type %4871, %int5_5682 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_5683 = torch.constant.int 2
    %int960_5684 = torch.constant.int 960
    %int10240_5685 = torch.constant.int 10240
    %4873 = torch.prim.ListConstruct %int2_5683, %int960_5684, %int10240_5685 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4874 = torch.aten.view %4872, %4873 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_5686 = torch.constant.int -1
    %int0_5687 = torch.constant.int 0
    %int5120_5688 = torch.constant.int 5120
    %int1_5689 = torch.constant.int 1
    %4875 = torch.aten.slice.Tensor %4874, %int-1_5686, %int0_5687, %int5120_5688, %int1_5689 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_5690 = torch.constant.int -1
    %int5120_5691 = torch.constant.int 5120
    %int10240_5692 = torch.constant.int 10240
    %int1_5693 = torch.constant.int 1
    %4876 = torch.aten.slice.Tensor %4874, %int-1_5690, %int5120_5691, %int10240_5692, %int1_5693 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_5694 = torch.constant.str "none"
    %4877 = torch.aten.gelu %4876, %str_5694 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %4878 = torch.aten.mul.Tensor %4875, %4877 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_5695 = torch.constant.none
    %4879 = torch.aten.clone %4878, %none_5695 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_5696 = torch.constant.int 1920
    %int5120_5697 = torch.constant.int 5120
    %4880 = torch.prim.ListConstruct %int1920_5696, %int5120_5697 : (!torch.int, !torch.int) -> !torch.list<int>
    %4881 = torch.aten.view %4879, %4880 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %4882 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5698 = torch.constant.int 0
    %int1_5699 = torch.constant.int 1
    %4883 = torch.aten.transpose.int %4882, %int0_5698, %int1_5699 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %4884 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5700 = torch.constant.int 6
    %4885 = torch.prims.convert_element_type %4884, %int6_5700 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5701 = torch.constant.int 6
    %4886 = torch.prims.convert_element_type %4881, %int6_5701 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_5702 = torch.constant.int 6
    %4887 = torch.prims.convert_element_type %4883, %int6_5702 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %4888 = torch.aten.mm %4886, %4887 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5703 = torch.constant.int 1
    %4889 = torch.aten.mul.Scalar %4888, %int1_5703 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5704 = torch.constant.int 1
    %4890 = torch.aten.mul.Scalar %4885, %int1_5704 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5705 = torch.constant.int 1
    %4891 = torch.aten.add.Tensor %4889, %4890, %int1_5705 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5706 = torch.constant.int 5
    %4892 = torch.prims.convert_element_type %4891, %int5_5706 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5707 = torch.constant.int 2
    %int960_5708 = torch.constant.int 960
    %int1280_5709 = torch.constant.int 1280
    %4893 = torch.prim.ListConstruct %int2_5707, %int960_5708, %int1280_5709 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4894 = torch.aten.view %4892, %4893 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_5710 = torch.constant.int 1
    %4895 = torch.aten.add.Tensor %4894, %4848, %int1_5710 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5711 = torch.constant.int 6
    %4896 = torch.prims.convert_element_type %4895, %int6_5711 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5712 = torch.constant.int 2
    %4897 = torch.prim.ListConstruct %int2_5712 : (!torch.int) -> !torch.list<int>
    %int0_5713 = torch.constant.int 0
    %true_5714 = torch.constant.bool true
    %result0_5715, %result1_5716 = torch.aten.var_mean.correction %4896, %4897, %int0_5713, %true_5714 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5717 = torch.constant.float 1.000000e-05
    %int1_5718 = torch.constant.int 1
    %4898 = torch.aten.add.Scalar %result0_5715, %float1.000000e-05_5717, %int1_5718 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4899 = torch.aten.rsqrt %4898 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5719 = torch.constant.int 1
    %4900 = torch.aten.sub.Tensor %4895, %result1_5716, %int1_5719 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4901 = torch.aten.mul.Tensor %4900, %4899 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %4902 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4903 = torch.aten.mul.Tensor %4901, %4902 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %4904 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5720 = torch.constant.int 1
    %4905 = torch.aten.add.Tensor %4903, %4904, %int1_5720 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5721 = torch.constant.int 5
    %4906 = torch.prims.convert_element_type %4905, %int5_5721 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %4907 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5722 = torch.constant.int 0
    %int1_5723 = torch.constant.int 1
    %4908 = torch.aten.transpose.int %4907, %int0_5722, %int1_5723 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5724 = torch.constant.int 1920
    %int1280_5725 = torch.constant.int 1280
    %4909 = torch.prim.ListConstruct %int1920_5724, %int1280_5725 : (!torch.int, !torch.int) -> !torch.list<int>
    %4910 = torch.aten.view %4906, %4909 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4911 = torch.aten.mm %4910, %4908 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5726 = torch.constant.int 2
    %int960_5727 = torch.constant.int 960
    %int1280_5728 = torch.constant.int 1280
    %4912 = torch.prim.ListConstruct %int2_5726, %int960_5727, %int1280_5728 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4913 = torch.aten.view %4911, %4912 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %4914 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5729 = torch.constant.int 0
    %int1_5730 = torch.constant.int 1
    %4915 = torch.aten.transpose.int %4914, %int0_5729, %int1_5730 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5731 = torch.constant.int 1920
    %int1280_5732 = torch.constant.int 1280
    %4916 = torch.prim.ListConstruct %int1920_5731, %int1280_5732 : (!torch.int, !torch.int) -> !torch.list<int>
    %4917 = torch.aten.view %4906, %4916 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4918 = torch.aten.mm %4917, %4915 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5733 = torch.constant.int 2
    %int960_5734 = torch.constant.int 960
    %int1280_5735 = torch.constant.int 1280
    %4919 = torch.prim.ListConstruct %int2_5733, %int960_5734, %int1280_5735 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4920 = torch.aten.view %4918, %4919 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %4921 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5736 = torch.constant.int 0
    %int1_5737 = torch.constant.int 1
    %4922 = torch.aten.transpose.int %4921, %int0_5736, %int1_5737 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5738 = torch.constant.int 1920
    %int1280_5739 = torch.constant.int 1280
    %4923 = torch.prim.ListConstruct %int1920_5738, %int1280_5739 : (!torch.int, !torch.int) -> !torch.list<int>
    %4924 = torch.aten.view %4906, %4923 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4925 = torch.aten.mm %4924, %4922 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5740 = torch.constant.int 2
    %int960_5741 = torch.constant.int 960
    %int1280_5742 = torch.constant.int 1280
    %4926 = torch.prim.ListConstruct %int2_5740, %int960_5741, %int1280_5742 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4927 = torch.aten.view %4925, %4926 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_5743 = torch.constant.int 2
    %int-1_5744 = torch.constant.int -1
    %int20_5745 = torch.constant.int 20
    %int64_5746 = torch.constant.int 64
    %4928 = torch.prim.ListConstruct %int2_5743, %int-1_5744, %int20_5745, %int64_5746 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4929 = torch.aten.view %4913, %4928 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5747 = torch.constant.int 1
    %int2_5748 = torch.constant.int 2
    %4930 = torch.aten.transpose.int %4929, %int1_5747, %int2_5748 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5749 = torch.constant.int 2
    %int-1_5750 = torch.constant.int -1
    %int20_5751 = torch.constant.int 20
    %int64_5752 = torch.constant.int 64
    %4931 = torch.prim.ListConstruct %int2_5749, %int-1_5750, %int20_5751, %int64_5752 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4932 = torch.aten.view %4920, %4931 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5753 = torch.constant.int 1
    %int2_5754 = torch.constant.int 2
    %4933 = torch.aten.transpose.int %4932, %int1_5753, %int2_5754 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5755 = torch.constant.int 2
    %int-1_5756 = torch.constant.int -1
    %int20_5757 = torch.constant.int 20
    %int64_5758 = torch.constant.int 64
    %4934 = torch.prim.ListConstruct %int2_5755, %int-1_5756, %int20_5757, %int64_5758 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4935 = torch.aten.view %4927, %4934 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5759 = torch.constant.int 1
    %int2_5760 = torch.constant.int 2
    %4936 = torch.aten.transpose.int %4935, %int1_5759, %int2_5760 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_5761 = torch.constant.float 0.000000e+00
    %false_5762 = torch.constant.bool false
    %none_5763 = torch.constant.none
    %none_5764 = torch.constant.none
    %4937:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4930, %4933, %4936, %float0.000000e00_5761, %false_5762, %none_5763, %none_5764) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5765 = torch.constant.int 1
    %int2_5766 = torch.constant.int 2
    %4938 = torch.aten.transpose.int %4937#0, %int1_5765, %int2_5766 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5767 = torch.constant.int 2
    %int-1_5768 = torch.constant.int -1
    %int1280_5769 = torch.constant.int 1280
    %4939 = torch.prim.ListConstruct %int2_5767, %int-1_5768, %int1280_5769 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4940 = torch.aten.view %4938, %4939 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5770 = torch.constant.int 5
    %4941 = torch.prims.convert_element_type %4940, %int5_5770 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5771 = torch.constant.int 1920
    %int1280_5772 = torch.constant.int 1280
    %4942 = torch.prim.ListConstruct %int1920_5771, %int1280_5772 : (!torch.int, !torch.int) -> !torch.list<int>
    %4943 = torch.aten.view %4941, %4942 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %4944 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5773 = torch.constant.int 0
    %int1_5774 = torch.constant.int 1
    %4945 = torch.aten.transpose.int %4944, %int0_5773, %int1_5774 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %4946 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5775 = torch.constant.int 6
    %4947 = torch.prims.convert_element_type %4946, %int6_5775 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5776 = torch.constant.int 6
    %4948 = torch.prims.convert_element_type %4943, %int6_5776 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5777 = torch.constant.int 6
    %4949 = torch.prims.convert_element_type %4945, %int6_5777 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %4950 = torch.aten.mm %4948, %4949 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5778 = torch.constant.int 1
    %4951 = torch.aten.mul.Scalar %4950, %int1_5778 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5779 = torch.constant.int 1
    %4952 = torch.aten.mul.Scalar %4947, %int1_5779 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5780 = torch.constant.int 1
    %4953 = torch.aten.add.Tensor %4951, %4952, %int1_5780 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5781 = torch.constant.int 5
    %4954 = torch.prims.convert_element_type %4953, %int5_5781 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5782 = torch.constant.int 2
    %int960_5783 = torch.constant.int 960
    %int1280_5784 = torch.constant.int 1280
    %4955 = torch.prim.ListConstruct %int2_5782, %int960_5783, %int1280_5784 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4956 = torch.aten.view %4954, %4955 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5785 = torch.constant.none
    %4957 = torch.aten.clone %4956, %none_5785 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5786 = torch.constant.float 1.000000e+00
    %4958 = torch.aten.div.Scalar %4957, %float1.000000e00_5786 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5787 = torch.constant.int 1
    %4959 = torch.aten.add.Tensor %4958, %4895, %int1_5787 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5788 = torch.constant.int 6
    %4960 = torch.prims.convert_element_type %4959, %int6_5788 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5789 = torch.constant.int 2
    %4961 = torch.prim.ListConstruct %int2_5789 : (!torch.int) -> !torch.list<int>
    %int0_5790 = torch.constant.int 0
    %true_5791 = torch.constant.bool true
    %result0_5792, %result1_5793 = torch.aten.var_mean.correction %4960, %4961, %int0_5790, %true_5791 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5794 = torch.constant.float 1.000000e-05
    %int1_5795 = torch.constant.int 1
    %4962 = torch.aten.add.Scalar %result0_5792, %float1.000000e-05_5794, %int1_5795 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %4963 = torch.aten.rsqrt %4962 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5796 = torch.constant.int 1
    %4964 = torch.aten.sub.Tensor %4959, %result1_5793, %int1_5796 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %4965 = torch.aten.mul.Tensor %4964, %4963 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %4966 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %4967 = torch.aten.mul.Tensor %4965, %4966 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %4968 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5797 = torch.constant.int 1
    %4969 = torch.aten.add.Tensor %4967, %4968, %int1_5797 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5798 = torch.constant.int 5
    %4970 = torch.prims.convert_element_type %4969, %int5_5798 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %4971 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5799 = torch.constant.int 0
    %int1_5800 = torch.constant.int 1
    %4972 = torch.aten.transpose.int %4971, %int0_5799, %int1_5800 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_5801 = torch.constant.int 1920
    %int1280_5802 = torch.constant.int 1280
    %4973 = torch.prim.ListConstruct %int1920_5801, %int1280_5802 : (!torch.int, !torch.int) -> !torch.list<int>
    %4974 = torch.aten.view %4970, %4973 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %4975 = torch.aten.mm %4974, %4972 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_5803 = torch.constant.int 2
    %int960_5804 = torch.constant.int 960
    %int1280_5805 = torch.constant.int 1280
    %4976 = torch.prim.ListConstruct %int2_5803, %int960_5804, %int1280_5805 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4977 = torch.aten.view %4975, %4976 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %4978 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5806 = torch.constant.int 0
    %int1_5807 = torch.constant.int 1
    %4979 = torch.aten.transpose.int %4978, %int0_5806, %int1_5807 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5808 = torch.constant.int 32
    %int2048_5809 = torch.constant.int 2048
    %4980 = torch.prim.ListConstruct %int32_5808, %int2048_5809 : (!torch.int, !torch.int) -> !torch.list<int>
    %4981 = torch.aten.view %arg6, %4980 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4982 = torch.aten.mm %4981, %4979 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5810 = torch.constant.int 2
    %int16_5811 = torch.constant.int 16
    %int1280_5812 = torch.constant.int 1280
    %4983 = torch.prim.ListConstruct %int2_5810, %int16_5811, %int1280_5812 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4984 = torch.aten.view %4982, %4983 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %4985 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_5813 = torch.constant.int 0
    %int1_5814 = torch.constant.int 1
    %4986 = torch.aten.transpose.int %4985, %int0_5813, %int1_5814 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_5815 = torch.constant.int 32
    %int2048_5816 = torch.constant.int 2048
    %4987 = torch.prim.ListConstruct %int32_5815, %int2048_5816 : (!torch.int, !torch.int) -> !torch.list<int>
    %4988 = torch.aten.view %arg6, %4987 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %4989 = torch.aten.mm %4988, %4986 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_5817 = torch.constant.int 2
    %int16_5818 = torch.constant.int 16
    %int1280_5819 = torch.constant.int 1280
    %4990 = torch.prim.ListConstruct %int2_5817, %int16_5818, %int1280_5819 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4991 = torch.aten.view %4989, %4990 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_5820 = torch.constant.int 2
    %int-1_5821 = torch.constant.int -1
    %int20_5822 = torch.constant.int 20
    %int64_5823 = torch.constant.int 64
    %4992 = torch.prim.ListConstruct %int2_5820, %int-1_5821, %int20_5822, %int64_5823 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4993 = torch.aten.view %4977, %4992 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_5824 = torch.constant.int 1
    %int2_5825 = torch.constant.int 2
    %4994 = torch.aten.transpose.int %4993, %int1_5824, %int2_5825 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_5826 = torch.constant.int 2
    %int-1_5827 = torch.constant.int -1
    %int20_5828 = torch.constant.int 20
    %int64_5829 = torch.constant.int 64
    %4995 = torch.prim.ListConstruct %int2_5826, %int-1_5827, %int20_5828, %int64_5829 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4996 = torch.aten.view %4984, %4995 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5830 = torch.constant.int 1
    %int2_5831 = torch.constant.int 2
    %4997 = torch.aten.transpose.int %4996, %int1_5830, %int2_5831 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_5832 = torch.constant.int 2
    %int-1_5833 = torch.constant.int -1
    %int20_5834 = torch.constant.int 20
    %int64_5835 = torch.constant.int 64
    %4998 = torch.prim.ListConstruct %int2_5832, %int-1_5833, %int20_5834, %int64_5835 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %4999 = torch.aten.view %4991, %4998 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_5836 = torch.constant.int 1
    %int2_5837 = torch.constant.int 2
    %5000 = torch.aten.transpose.int %4999, %int1_5836, %int2_5837 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_5838 = torch.constant.float 0.000000e+00
    %false_5839 = torch.constant.bool false
    %none_5840 = torch.constant.none
    %none_5841 = torch.constant.none
    %5001:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%4994, %4997, %5000, %float0.000000e00_5838, %false_5839, %none_5840, %none_5841) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_5842 = torch.constant.int 1
    %int2_5843 = torch.constant.int 2
    %5002 = torch.aten.transpose.int %5001#0, %int1_5842, %int2_5843 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_5844 = torch.constant.int 2
    %int-1_5845 = torch.constant.int -1
    %int1280_5846 = torch.constant.int 1280
    %5003 = torch.prim.ListConstruct %int2_5844, %int-1_5845, %int1280_5846 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5004 = torch.aten.view %5002, %5003 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_5847 = torch.constant.int 5
    %5005 = torch.prims.convert_element_type %5004, %int5_5847 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5848 = torch.constant.int 1920
    %int1280_5849 = torch.constant.int 1280
    %5006 = torch.prim.ListConstruct %int1920_5848, %int1280_5849 : (!torch.int, !torch.int) -> !torch.list<int>
    %5007 = torch.aten.view %5005, %5006 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5008 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5850 = torch.constant.int 0
    %int1_5851 = torch.constant.int 1
    %5009 = torch.aten.transpose.int %5008, %int0_5850, %int1_5851 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %5010 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5852 = torch.constant.int 6
    %5011 = torch.prims.convert_element_type %5010, %int6_5852 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5853 = torch.constant.int 6
    %5012 = torch.prims.convert_element_type %5007, %int6_5853 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5854 = torch.constant.int 6
    %5013 = torch.prims.convert_element_type %5009, %int6_5854 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5014 = torch.aten.mm %5012, %5013 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5855 = torch.constant.int 1
    %5015 = torch.aten.mul.Scalar %5014, %int1_5855 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5856 = torch.constant.int 1
    %5016 = torch.aten.mul.Scalar %5011, %int1_5856 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5857 = torch.constant.int 1
    %5017 = torch.aten.add.Tensor %5015, %5016, %int1_5857 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5858 = torch.constant.int 5
    %5018 = torch.prims.convert_element_type %5017, %int5_5858 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5859 = torch.constant.int 2
    %int960_5860 = torch.constant.int 960
    %int1280_5861 = torch.constant.int 1280
    %5019 = torch.prim.ListConstruct %int2_5859, %int960_5860, %int1280_5861 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5020 = torch.aten.view %5018, %5019 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_5862 = torch.constant.none
    %5021 = torch.aten.clone %5020, %none_5862 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_5863 = torch.constant.float 1.000000e+00
    %5022 = torch.aten.div.Scalar %5021, %float1.000000e00_5863 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_5864 = torch.constant.int 1
    %5023 = torch.aten.add.Tensor %5022, %4959, %int1_5864 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_5865 = torch.constant.int 6
    %5024 = torch.prims.convert_element_type %5023, %int6_5865 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_5866 = torch.constant.int 2
    %5025 = torch.prim.ListConstruct %int2_5866 : (!torch.int) -> !torch.list<int>
    %int0_5867 = torch.constant.int 0
    %true_5868 = torch.constant.bool true
    %result0_5869, %result1_5870 = torch.aten.var_mean.correction %5024, %5025, %int0_5867, %true_5868 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_5871 = torch.constant.float 1.000000e-05
    %int1_5872 = torch.constant.int 1
    %5026 = torch.aten.add.Scalar %result0_5869, %float1.000000e-05_5871, %int1_5872 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5027 = torch.aten.rsqrt %5026 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_5873 = torch.constant.int 1
    %5028 = torch.aten.sub.Tensor %5023, %result1_5870, %int1_5873 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5029 = torch.aten.mul.Tensor %5028, %5027 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %5030 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5031 = torch.aten.mul.Tensor %5029, %5030 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %5032 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5874 = torch.constant.int 1
    %5033 = torch.aten.add.Tensor %5031, %5032, %int1_5874 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_5875 = torch.constant.int 5
    %5034 = torch.prims.convert_element_type %5033, %int5_5875 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5876 = torch.constant.int 1920
    %int1280_5877 = torch.constant.int 1280
    %5035 = torch.prim.ListConstruct %int1920_5876, %int1280_5877 : (!torch.int, !torch.int) -> !torch.list<int>
    %5036 = torch.aten.view %5034, %5035 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5037 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_5878 = torch.constant.int 0
    %int1_5879 = torch.constant.int 1
    %5038 = torch.aten.transpose.int %5037, %int0_5878, %int1_5879 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %5039 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_5880 = torch.constant.int 6
    %5040 = torch.prims.convert_element_type %5039, %int6_5880 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_5881 = torch.constant.int 6
    %5041 = torch.prims.convert_element_type %5036, %int6_5881 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5882 = torch.constant.int 6
    %5042 = torch.prims.convert_element_type %5038, %int6_5882 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5043 = torch.aten.mm %5041, %5042 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_5883 = torch.constant.int 1
    %5044 = torch.aten.mul.Scalar %5043, %int1_5883 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_5884 = torch.constant.int 1
    %5045 = torch.aten.mul.Scalar %5040, %int1_5884 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_5885 = torch.constant.int 1
    %5046 = torch.aten.add.Tensor %5044, %5045, %int1_5885 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_5886 = torch.constant.int 5
    %5047 = torch.prims.convert_element_type %5046, %int5_5886 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_5887 = torch.constant.int 2
    %int960_5888 = torch.constant.int 960
    %int10240_5889 = torch.constant.int 10240
    %5048 = torch.prim.ListConstruct %int2_5887, %int960_5888, %int10240_5889 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5049 = torch.aten.view %5047, %5048 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_5890 = torch.constant.int -1
    %int0_5891 = torch.constant.int 0
    %int5120_5892 = torch.constant.int 5120
    %int1_5893 = torch.constant.int 1
    %5050 = torch.aten.slice.Tensor %5049, %int-1_5890, %int0_5891, %int5120_5892, %int1_5893 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_5894 = torch.constant.int -1
    %int5120_5895 = torch.constant.int 5120
    %int10240_5896 = torch.constant.int 10240
    %int1_5897 = torch.constant.int 1
    %5051 = torch.aten.slice.Tensor %5049, %int-1_5894, %int5120_5895, %int10240_5896, %int1_5897 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_5898 = torch.constant.str "none"
    %5052 = torch.aten.gelu %5051, %str_5898 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %5053 = torch.aten.mul.Tensor %5050, %5052 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_5899 = torch.constant.none
    %5054 = torch.aten.clone %5053, %none_5899 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_5900 = torch.constant.int 1920
    %int5120_5901 = torch.constant.int 5120
    %5055 = torch.prim.ListConstruct %int1920_5900, %int5120_5901 : (!torch.int, !torch.int) -> !torch.list<int>
    %5056 = torch.aten.view %5054, %5055 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %5057 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_5902 = torch.constant.int 0
    %int1_5903 = torch.constant.int 1
    %5058 = torch.aten.transpose.int %5057, %int0_5902, %int1_5903 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %5059 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5904 = torch.constant.int 6
    %5060 = torch.prims.convert_element_type %5059, %int6_5904 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5905 = torch.constant.int 6
    %5061 = torch.prims.convert_element_type %5056, %int6_5905 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_5906 = torch.constant.int 6
    %5062 = torch.prims.convert_element_type %5058, %int6_5906 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5063 = torch.aten.mm %5061, %5062 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5907 = torch.constant.int 1
    %5064 = torch.aten.mul.Scalar %5063, %int1_5907 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5908 = torch.constant.int 1
    %5065 = torch.aten.mul.Scalar %5060, %int1_5908 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5909 = torch.constant.int 1
    %5066 = torch.aten.add.Tensor %5064, %5065, %int1_5909 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5910 = torch.constant.int 5
    %5067 = torch.prims.convert_element_type %5066, %int5_5910 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5911 = torch.constant.int 2
    %int960_5912 = torch.constant.int 960
    %int1280_5913 = torch.constant.int 1280
    %5068 = torch.prim.ListConstruct %int2_5911, %int960_5912, %int1280_5913 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5069 = torch.aten.view %5067, %5068 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_5914 = torch.constant.int 1
    %5070 = torch.aten.add.Tensor %5069, %5023, %int1_5914 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_5915 = torch.constant.int 1920
    %int1280_5916 = torch.constant.int 1280
    %5071 = torch.prim.ListConstruct %int1920_5915, %int1280_5916 : (!torch.int, !torch.int) -> !torch.list<int>
    %5072 = torch.aten.view %5070, %5071 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.proj_out.weight = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.proj_out.weight : tensor<1280x1280xf16>
    %5073 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5917 = torch.constant.int 0
    %int1_5918 = torch.constant.int 1
    %5074 = torch.aten.transpose.int %5073, %int0_5917, %int1_5918 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.down_blocks.2.attentions.1.proj_out.bias = util.global.load @__auto.controlnet.down_blocks.2.attentions.1.proj_out.bias : tensor<1280xf16>
    %5075 = torch_c.from_builtin_tensor %__auto.controlnet.down_blocks.2.attentions.1.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5919 = torch.constant.int 6
    %5076 = torch.prims.convert_element_type %5075, %int6_5919 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5920 = torch.constant.int 6
    %5077 = torch.prims.convert_element_type %5072, %int6_5920 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_5921 = torch.constant.int 6
    %5078 = torch.prims.convert_element_type %5074, %int6_5921 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5079 = torch.aten.mm %5077, %5078 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_5922 = torch.constant.int 1
    %5080 = torch.aten.mul.Scalar %5079, %int1_5922 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_5923 = torch.constant.int 1
    %5081 = torch.aten.mul.Scalar %5076, %int1_5923 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5924 = torch.constant.int 1
    %5082 = torch.aten.add.Tensor %5080, %5081, %int1_5924 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_5925 = torch.constant.int 5
    %5083 = torch.prims.convert_element_type %5082, %int5_5925 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_5926 = torch.constant.int 2
    %int960_5927 = torch.constant.int 960
    %int1280_5928 = torch.constant.int 1280
    %5084 = torch.prim.ListConstruct %int2_5926, %int960_5927, %int1280_5928 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5085 = torch.aten.view %5083, %5084 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_5929 = torch.constant.int 2
    %int30_5930 = torch.constant.int 30
    %int32_5931 = torch.constant.int 32
    %int1280_5932 = torch.constant.int 1280
    %5086 = torch.prim.ListConstruct %int2_5929, %int30_5930, %int32_5931, %int1280_5932 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5087 = torch.aten.view %5085, %5086 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,30,32,1280],f16>
    %int0_5933 = torch.constant.int 0
    %int3_5934 = torch.constant.int 3
    %int1_5935 = torch.constant.int 1
    %int2_5936 = torch.constant.int 2
    %5088 = torch.prim.ListConstruct %int0_5933, %int3_5934, %int1_5935, %int2_5936 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5089 = torch.aten.permute %5087, %5088 : !torch.vtensor<[2,30,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f16>
    %int0_5937 = torch.constant.int 0
    %5090 = torch.aten.clone %5089, %int0_5937 : !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_5938 = torch.constant.int 1
    %5091 = torch.aten.add.Tensor %5090, %3285, %int1_5938 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_5939 = torch.constant.int 2
    %int32_5940 = torch.constant.int 32
    %int40_5941 = torch.constant.int 40
    %int960_5942 = torch.constant.int 960
    %5092 = torch.prim.ListConstruct %int2_5939, %int32_5940, %int40_5941, %int960_5942 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5093 = torch.aten.view %5091, %5092 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_5943 = torch.constant.int 6
    %5094 = torch.prims.convert_element_type %5093, %int6_5943 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_5944 = torch.constant.int 2
    %int3_5945 = torch.constant.int 3
    %5095 = torch.prim.ListConstruct %int2_5944, %int3_5945 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_5946 = torch.constant.int 0
    %true_5947 = torch.constant.bool true
    %result0_5948, %result1_5949 = torch.aten.var_mean.correction %5094, %5095, %int0_5946, %true_5947 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_5950 = torch.constant.float 1.000000e-05
    %int1_5951 = torch.constant.int 1
    %5096 = torch.aten.add.Scalar %result0_5948, %float1.000000e-05_5950, %int1_5951 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %5097 = torch.aten.rsqrt %5096 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_5952 = torch.constant.int 1
    %5098 = torch.aten.sub.Tensor %5093, %result1_5949, %int1_5952 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %5099 = torch.aten.mul.Tensor %5098, %5097 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_5953 = torch.constant.int 2
    %int1280_5954 = torch.constant.int 1280
    %int30_5955 = torch.constant.int 30
    %int32_5956 = torch.constant.int 32
    %5100 = torch.prim.ListConstruct %int2_5953, %int1280_5954, %int30_5955, %int32_5956 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5101 = torch.aten.view %5099, %5100 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.mid_block.resnets.0.norm1.bias = util.global.load @__auto.controlnet.mid_block.resnets.0.norm1.bias : tensor<1280xf16>
    %5102 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_5957 = torch.constant.int 0
    %5103 = torch.aten.unsqueeze %5102, %int0_5957 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_5958 = torch.constant.int 2
    %5104 = torch.aten.unsqueeze %5103, %int2_5958 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_5959 = torch.constant.int 3
    %5105 = torch.aten.unsqueeze %5104, %int3_5959 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.mid_block.resnets.0.norm1.weight = util.global.load @__auto.controlnet.mid_block.resnets.0.norm1.weight : tensor<1280xf16>
    %5106 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_5960 = torch.constant.int 0
    %5107 = torch.aten.unsqueeze %5106, %int0_5960 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_5961 = torch.constant.int 2
    %5108 = torch.aten.unsqueeze %5107, %int2_5961 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_5962 = torch.constant.int 3
    %5109 = torch.aten.unsqueeze %5108, %int3_5962 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %5110 = torch.aten.mul.Tensor %5101, %5109 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_5963 = torch.constant.int 1
    %5111 = torch.aten.add.Tensor %5110, %5105, %int1_5963 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_5964 = torch.constant.int 5
    %5112 = torch.prims.convert_element_type %5111, %int5_5964 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %5113 = torch.aten.silu %5112 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.mid_block.resnets.0.conv1.weight = util.global.load @__auto.controlnet.mid_block.resnets.0.conv1.weight : tensor<1280x1280x3x3xf16>
    %5114 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.mid_block.resnets.0.conv1.bias = util.global.load @__auto.controlnet.mid_block.resnets.0.conv1.bias : tensor<1280xf16>
    %5115 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_5965 = torch.constant.int 1
    %int1_5966 = torch.constant.int 1
    %5116 = torch.prim.ListConstruct %int1_5965, %int1_5966 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5967 = torch.constant.int 1
    %int1_5968 = torch.constant.int 1
    %5117 = torch.prim.ListConstruct %int1_5967, %int1_5968 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5969 = torch.constant.int 1
    %int1_5970 = torch.constant.int 1
    %5118 = torch.prim.ListConstruct %int1_5969, %int1_5970 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_5971 = torch.constant.bool false
    %int0_5972 = torch.constant.int 0
    %int0_5973 = torch.constant.int 0
    %5119 = torch.prim.ListConstruct %int0_5972, %int0_5973 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_5974 = torch.constant.int 1
    %5120 = torch.aten.convolution %5113, %5114, %5115, %5116, %5117, %5118, %false_5971, %5119, %int1_5974 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %5121 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.mid_block.resnets.0.time_emb_proj.weight = util.global.load @__auto.controlnet.mid_block.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16>
    %5122 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_5975 = torch.constant.int 0
    %int1_5976 = torch.constant.int 1
    %5123 = torch.aten.transpose.int %5122, %int0_5975, %int1_5976 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.resnets.0.time_emb_proj.bias = util.global.load @__auto.controlnet.mid_block.resnets.0.time_emb_proj.bias : tensor<1280xf16>
    %5124 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_5977 = torch.constant.int 6
    %5125 = torch.prims.convert_element_type %5124, %int6_5977 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_5978 = torch.constant.int 6
    %5126 = torch.prims.convert_element_type %5121, %int6_5978 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_5979 = torch.constant.int 6
    %5127 = torch.prims.convert_element_type %5123, %int6_5979 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5128 = torch.aten.mm %5126, %5127 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_5980 = torch.constant.int 1
    %5129 = torch.aten.mul.Scalar %5128, %int1_5980 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_5981 = torch.constant.int 1
    %5130 = torch.aten.mul.Scalar %5125, %int1_5981 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_5982 = torch.constant.int 1
    %5131 = torch.aten.add.Tensor %5129, %5130, %int1_5982 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_5983 = torch.constant.int 5
    %5132 = torch.prims.convert_element_type %5131, %int5_5983 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int0_5984 = torch.constant.int 0
    %int0_5985 = torch.constant.int 0
    %int9223372036854775807_5986 = torch.constant.int 9223372036854775807
    %int1_5987 = torch.constant.int 1
    %5133 = torch.aten.slice.Tensor %5132, %int0_5984, %int0_5985, %int9223372036854775807_5986, %int1_5987 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int1_5988 = torch.constant.int 1
    %int0_5989 = torch.constant.int 0
    %int9223372036854775807_5990 = torch.constant.int 9223372036854775807
    %int1_5991 = torch.constant.int 1
    %5134 = torch.aten.slice.Tensor %5133, %int1_5988, %int0_5989, %int9223372036854775807_5990, %int1_5991 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int2_5992 = torch.constant.int 2
    %5135 = torch.aten.unsqueeze %5134, %int2_5992 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %int3_5993 = torch.constant.int 3
    %5136 = torch.aten.unsqueeze %5135, %int3_5993 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %int1_5994 = torch.constant.int 1
    %5137 = torch.aten.add.Tensor %5120, %5136, %int1_5994 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_5995 = torch.constant.int 2
    %int32_5996 = torch.constant.int 32
    %int40_5997 = torch.constant.int 40
    %int960_5998 = torch.constant.int 960
    %5138 = torch.prim.ListConstruct %int2_5995, %int32_5996, %int40_5997, %int960_5998 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5139 = torch.aten.view %5137, %5138 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_5999 = torch.constant.int 6
    %5140 = torch.prims.convert_element_type %5139, %int6_5999 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_6000 = torch.constant.int 2
    %int3_6001 = torch.constant.int 3
    %5141 = torch.prim.ListConstruct %int2_6000, %int3_6001 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_6002 = torch.constant.int 0
    %true_6003 = torch.constant.bool true
    %result0_6004, %result1_6005 = torch.aten.var_mean.correction %5140, %5141, %int0_6002, %true_6003 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_6006 = torch.constant.float 1.000000e-05
    %int1_6007 = torch.constant.int 1
    %5142 = torch.aten.add.Scalar %result0_6004, %float1.000000e-05_6006, %int1_6007 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %5143 = torch.aten.rsqrt %5142 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_6008 = torch.constant.int 1
    %5144 = torch.aten.sub.Tensor %5139, %result1_6005, %int1_6008 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %5145 = torch.aten.mul.Tensor %5144, %5143 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_6009 = torch.constant.int 2
    %int1280_6010 = torch.constant.int 1280
    %int30_6011 = torch.constant.int 30
    %int32_6012 = torch.constant.int 32
    %5146 = torch.prim.ListConstruct %int2_6009, %int1280_6010, %int30_6011, %int32_6012 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5147 = torch.aten.view %5145, %5146 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.mid_block.resnets.0.norm2.bias = util.global.load @__auto.controlnet.mid_block.resnets.0.norm2.bias : tensor<1280xf16>
    %5148 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_6013 = torch.constant.int 0
    %5149 = torch.aten.unsqueeze %5148, %int0_6013 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_6014 = torch.constant.int 2
    %5150 = torch.aten.unsqueeze %5149, %int2_6014 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_6015 = torch.constant.int 3
    %5151 = torch.aten.unsqueeze %5150, %int3_6015 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.mid_block.resnets.0.norm2.weight = util.global.load @__auto.controlnet.mid_block.resnets.0.norm2.weight : tensor<1280xf16>
    %5152 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_6016 = torch.constant.int 0
    %5153 = torch.aten.unsqueeze %5152, %int0_6016 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_6017 = torch.constant.int 2
    %5154 = torch.aten.unsqueeze %5153, %int2_6017 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_6018 = torch.constant.int 3
    %5155 = torch.aten.unsqueeze %5154, %int3_6018 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %5156 = torch.aten.mul.Tensor %5147, %5155 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_6019 = torch.constant.int 1
    %5157 = torch.aten.add.Tensor %5156, %5151, %int1_6019 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_6020 = torch.constant.int 5
    %5158 = torch.prims.convert_element_type %5157, %int5_6020 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %5159 = torch.aten.silu %5158 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %none_6021 = torch.constant.none
    %5160 = torch.aten.clone %5159, %none_6021 : !torch.vtensor<[2,1280,30,32],f16>, !torch.none -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.mid_block.resnets.0.conv2.weight = util.global.load @__auto.controlnet.mid_block.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16>
    %5161 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.mid_block.resnets.0.conv2.bias = util.global.load @__auto.controlnet.mid_block.resnets.0.conv2.bias : tensor<1280xf16>
    %5162 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.0.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6022 = torch.constant.int 1
    %int1_6023 = torch.constant.int 1
    %5163 = torch.prim.ListConstruct %int1_6022, %int1_6023 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6024 = torch.constant.int 1
    %int1_6025 = torch.constant.int 1
    %5164 = torch.prim.ListConstruct %int1_6024, %int1_6025 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6026 = torch.constant.int 1
    %int1_6027 = torch.constant.int 1
    %5165 = torch.prim.ListConstruct %int1_6026, %int1_6027 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_6028 = torch.constant.bool false
    %int0_6029 = torch.constant.int 0
    %int0_6030 = torch.constant.int 0
    %5166 = torch.prim.ListConstruct %int0_6029, %int0_6030 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_6031 = torch.constant.int 1
    %5167 = torch.aten.convolution %5160, %5161, %5162, %5163, %5164, %5165, %false_6028, %5166, %int1_6031 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_6032 = torch.constant.int 1
    %5168 = torch.aten.add.Tensor %5091, %5167, %int1_6032 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_6033 = torch.constant.int 1
    %5169 = torch.aten.div.Scalar %5168, %int1_6033 : !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_6034 = torch.constant.int 2
    %int32_6035 = torch.constant.int 32
    %int40_6036 = torch.constant.int 40
    %int960_6037 = torch.constant.int 960
    %5170 = torch.prim.ListConstruct %int2_6034, %int32_6035, %int40_6036, %int960_6037 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5171 = torch.aten.view %5169, %5170 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_6038 = torch.constant.int 6
    %5172 = torch.prims.convert_element_type %5171, %int6_6038 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_6039 = torch.constant.int 2
    %int3_6040 = torch.constant.int 3
    %5173 = torch.prim.ListConstruct %int2_6039, %int3_6040 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_6041 = torch.constant.int 0
    %true_6042 = torch.constant.bool true
    %result0_6043, %result1_6044 = torch.aten.var_mean.correction %5172, %5173, %int0_6041, %true_6042 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float9.999990e-07_6045 = torch.constant.float 9.9999999999999995E-7
    %int1_6046 = torch.constant.int 1
    %5174 = torch.aten.add.Scalar %result0_6043, %float9.999990e-07_6045, %int1_6046 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %5175 = torch.aten.rsqrt %5174 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_6047 = torch.constant.int 1
    %5176 = torch.aten.sub.Tensor %5171, %result1_6044, %int1_6047 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %5177 = torch.aten.mul.Tensor %5176, %5175 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_6048 = torch.constant.int 2
    %int1280_6049 = torch.constant.int 1280
    %int30_6050 = torch.constant.int 30
    %int32_6051 = torch.constant.int 32
    %5178 = torch.prim.ListConstruct %int2_6048, %int1280_6049, %int30_6050, %int32_6051 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5179 = torch.aten.view %5177, %5178 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.mid_block.attentions.0.norm.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.norm.bias : tensor<1280xf16>
    %5180 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.norm.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_6052 = torch.constant.int 0
    %5181 = torch.aten.unsqueeze %5180, %int0_6052 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_6053 = torch.constant.int 2
    %5182 = torch.aten.unsqueeze %5181, %int2_6053 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_6054 = torch.constant.int 3
    %5183 = torch.aten.unsqueeze %5182, %int3_6054 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.mid_block.attentions.0.norm.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.norm.weight : tensor<1280xf16>
    %5184 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.norm.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_6055 = torch.constant.int 0
    %5185 = torch.aten.unsqueeze %5184, %int0_6055 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_6056 = torch.constant.int 2
    %5186 = torch.aten.unsqueeze %5185, %int2_6056 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_6057 = torch.constant.int 3
    %5187 = torch.aten.unsqueeze %5186, %int3_6057 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %5188 = torch.aten.mul.Tensor %5179, %5187 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_6058 = torch.constant.int 1
    %5189 = torch.aten.add.Tensor %5188, %5183, %int1_6058 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_6059 = torch.constant.int 5
    %5190 = torch.prims.convert_element_type %5189, %int5_6059 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int0_6060 = torch.constant.int 0
    %int2_6061 = torch.constant.int 2
    %int3_6062 = torch.constant.int 3
    %int1_6063 = torch.constant.int 1
    %5191 = torch.prim.ListConstruct %int0_6060, %int2_6061, %int3_6062, %int1_6063 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5192 = torch.aten.permute %5190, %5191 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,30,32,1280],f16>
    %int2_6064 = torch.constant.int 2
    %int960_6065 = torch.constant.int 960
    %int1280_6066 = torch.constant.int 1280
    %5193 = torch.prim.ListConstruct %int2_6064, %int960_6065, %int1280_6066 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5194 = torch.aten.view %5192, %5193 : !torch.vtensor<[2,30,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.proj_in.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.proj_in.weight : tensor<1280x1280xf16>
    %5195 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.proj_in.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6067 = torch.constant.int 0
    %int1_6068 = torch.constant.int 1
    %5196 = torch.aten.transpose.int %5195, %int0_6067, %int1_6068 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int0_6069 = torch.constant.int 0
    %5197 = torch.aten.clone %5194, %int0_6069 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6070 = torch.constant.int 1920
    %int1280_6071 = torch.constant.int 1280
    %5198 = torch.prim.ListConstruct %int1920_6070, %int1280_6071 : (!torch.int, !torch.int) -> !torch.list<int>
    %5199 = torch.aten._unsafe_view %5197, %5198 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5200 = torch.aten.mm %5199, %5196 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6072 = torch.constant.int 2
    %int960_6073 = torch.constant.int 960
    %int1280_6074 = torch.constant.int 1280
    %5201 = torch.prim.ListConstruct %int2_6072, %int960_6073, %int1280_6074 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5202 = torch.aten.view %5200, %5201 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.proj_in.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.proj_in.bias : tensor<1280xf16>
    %5203 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.proj_in.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6075 = torch.constant.int 1
    %5204 = torch.aten.add.Tensor %5202, %5203, %int1_6075 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6076 = torch.constant.int 6
    %5205 = torch.prims.convert_element_type %5204, %int6_6076 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6077 = torch.constant.int 2
    %5206 = torch.prim.ListConstruct %int2_6077 : (!torch.int) -> !torch.list<int>
    %int0_6078 = torch.constant.int 0
    %true_6079 = torch.constant.bool true
    %result0_6080, %result1_6081 = torch.aten.var_mean.correction %5205, %5206, %int0_6078, %true_6079 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6082 = torch.constant.float 1.000000e-05
    %int1_6083 = torch.constant.int 1
    %5207 = torch.aten.add.Scalar %result0_6080, %float1.000000e-05_6082, %int1_6083 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5208 = torch.aten.rsqrt %5207 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6084 = torch.constant.int 1
    %5209 = torch.aten.sub.Tensor %5204, %result1_6081, %int1_6084 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5210 = torch.aten.mul.Tensor %5209, %5208 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16>
    %5211 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5212 = torch.aten.mul.Tensor %5210, %5211 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16>
    %5213 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6085 = torch.constant.int 1
    %5214 = torch.aten.add.Tensor %5212, %5213, %int1_6085 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6086 = torch.constant.int 5
    %5215 = torch.prims.convert_element_type %5214, %int5_6086 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16>
    %5216 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6087 = torch.constant.int 0
    %int1_6088 = torch.constant.int 1
    %5217 = torch.aten.transpose.int %5216, %int0_6087, %int1_6088 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6089 = torch.constant.int 1920
    %int1280_6090 = torch.constant.int 1280
    %5218 = torch.prim.ListConstruct %int1920_6089, %int1280_6090 : (!torch.int, !torch.int) -> !torch.list<int>
    %5219 = torch.aten.view %5215, %5218 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5220 = torch.aten.mm %5219, %5217 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6091 = torch.constant.int 2
    %int960_6092 = torch.constant.int 960
    %int1280_6093 = torch.constant.int 1280
    %5221 = torch.prim.ListConstruct %int2_6091, %int960_6092, %int1280_6093 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5222 = torch.aten.view %5220, %5221 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16>
    %5223 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6094 = torch.constant.int 0
    %int1_6095 = torch.constant.int 1
    %5224 = torch.aten.transpose.int %5223, %int0_6094, %int1_6095 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6096 = torch.constant.int 1920
    %int1280_6097 = torch.constant.int 1280
    %5225 = torch.prim.ListConstruct %int1920_6096, %int1280_6097 : (!torch.int, !torch.int) -> !torch.list<int>
    %5226 = torch.aten.view %5215, %5225 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5227 = torch.aten.mm %5226, %5224 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6098 = torch.constant.int 2
    %int960_6099 = torch.constant.int 960
    %int1280_6100 = torch.constant.int 1280
    %5228 = torch.prim.ListConstruct %int2_6098, %int960_6099, %int1280_6100 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5229 = torch.aten.view %5227, %5228 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16>
    %5230 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6101 = torch.constant.int 0
    %int1_6102 = torch.constant.int 1
    %5231 = torch.aten.transpose.int %5230, %int0_6101, %int1_6102 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6103 = torch.constant.int 1920
    %int1280_6104 = torch.constant.int 1280
    %5232 = torch.prim.ListConstruct %int1920_6103, %int1280_6104 : (!torch.int, !torch.int) -> !torch.list<int>
    %5233 = torch.aten.view %5215, %5232 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5234 = torch.aten.mm %5233, %5231 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6105 = torch.constant.int 2
    %int960_6106 = torch.constant.int 960
    %int1280_6107 = torch.constant.int 1280
    %5235 = torch.prim.ListConstruct %int2_6105, %int960_6106, %int1280_6107 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5236 = torch.aten.view %5234, %5235 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_6108 = torch.constant.int 2
    %int-1_6109 = torch.constant.int -1
    %int20_6110 = torch.constant.int 20
    %int64_6111 = torch.constant.int 64
    %5237 = torch.prim.ListConstruct %int2_6108, %int-1_6109, %int20_6110, %int64_6111 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5238 = torch.aten.view %5222, %5237 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6112 = torch.constant.int 1
    %int2_6113 = torch.constant.int 2
    %5239 = torch.aten.transpose.int %5238, %int1_6112, %int2_6113 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6114 = torch.constant.int 2
    %int-1_6115 = torch.constant.int -1
    %int20_6116 = torch.constant.int 20
    %int64_6117 = torch.constant.int 64
    %5240 = torch.prim.ListConstruct %int2_6114, %int-1_6115, %int20_6116, %int64_6117 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5241 = torch.aten.view %5229, %5240 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6118 = torch.constant.int 1
    %int2_6119 = torch.constant.int 2
    %5242 = torch.aten.transpose.int %5241, %int1_6118, %int2_6119 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6120 = torch.constant.int 2
    %int-1_6121 = torch.constant.int -1
    %int20_6122 = torch.constant.int 20
    %int64_6123 = torch.constant.int 64
    %5243 = torch.prim.ListConstruct %int2_6120, %int-1_6121, %int20_6122, %int64_6123 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5244 = torch.aten.view %5236, %5243 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6124 = torch.constant.int 1
    %int2_6125 = torch.constant.int 2
    %5245 = torch.aten.transpose.int %5244, %int1_6124, %int2_6125 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_6126 = torch.constant.float 0.000000e+00
    %false_6127 = torch.constant.bool false
    %none_6128 = torch.constant.none
    %none_6129 = torch.constant.none
    %5246:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5239, %5242, %5245, %float0.000000e00_6126, %false_6127, %none_6128, %none_6129) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6130 = torch.constant.int 1
    %int2_6131 = torch.constant.int 2
    %5247 = torch.aten.transpose.int %5246#0, %int1_6130, %int2_6131 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6132 = torch.constant.int 2
    %int-1_6133 = torch.constant.int -1
    %int1280_6134 = torch.constant.int 1280
    %5248 = torch.prim.ListConstruct %int2_6132, %int-1_6133, %int1280_6134 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5249 = torch.aten.view %5247, %5248 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6135 = torch.constant.int 5
    %5250 = torch.prims.convert_element_type %5249, %int5_6135 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6136 = torch.constant.int 1920
    %int1280_6137 = torch.constant.int 1280
    %5251 = torch.prim.ListConstruct %int1920_6136, %int1280_6137 : (!torch.int, !torch.int) -> !torch.list<int>
    %5252 = torch.aten.view %5250, %5251 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5253 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6138 = torch.constant.int 0
    %int1_6139 = torch.constant.int 1
    %5254 = torch.aten.transpose.int %5253, %int0_6138, %int1_6139 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16>
    %5255 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6140 = torch.constant.int 6
    %5256 = torch.prims.convert_element_type %5255, %int6_6140 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6141 = torch.constant.int 6
    %5257 = torch.prims.convert_element_type %5252, %int6_6141 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6142 = torch.constant.int 6
    %5258 = torch.prims.convert_element_type %5254, %int6_6142 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5259 = torch.aten.mm %5257, %5258 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6143 = torch.constant.int 1
    %5260 = torch.aten.mul.Scalar %5259, %int1_6143 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6144 = torch.constant.int 1
    %5261 = torch.aten.mul.Scalar %5256, %int1_6144 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6145 = torch.constant.int 1
    %5262 = torch.aten.add.Tensor %5260, %5261, %int1_6145 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6146 = torch.constant.int 5
    %5263 = torch.prims.convert_element_type %5262, %int5_6146 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6147 = torch.constant.int 2
    %int960_6148 = torch.constant.int 960
    %int1280_6149 = torch.constant.int 1280
    %5264 = torch.prim.ListConstruct %int2_6147, %int960_6148, %int1280_6149 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5265 = torch.aten.view %5263, %5264 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6150 = torch.constant.none
    %5266 = torch.aten.clone %5265, %none_6150 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6151 = torch.constant.float 1.000000e+00
    %5267 = torch.aten.div.Scalar %5266, %float1.000000e00_6151 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6152 = torch.constant.int 1
    %5268 = torch.aten.add.Tensor %5267, %5204, %int1_6152 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6153 = torch.constant.int 6
    %5269 = torch.prims.convert_element_type %5268, %int6_6153 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6154 = torch.constant.int 2
    %5270 = torch.prim.ListConstruct %int2_6154 : (!torch.int) -> !torch.list<int>
    %int0_6155 = torch.constant.int 0
    %true_6156 = torch.constant.bool true
    %result0_6157, %result1_6158 = torch.aten.var_mean.correction %5269, %5270, %int0_6155, %true_6156 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6159 = torch.constant.float 1.000000e-05
    %int1_6160 = torch.constant.int 1
    %5271 = torch.aten.add.Scalar %result0_6157, %float1.000000e-05_6159, %int1_6160 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5272 = torch.aten.rsqrt %5271 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6161 = torch.constant.int 1
    %5273 = torch.aten.sub.Tensor %5268, %result1_6158, %int1_6161 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5274 = torch.aten.mul.Tensor %5273, %5272 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16>
    %5275 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5276 = torch.aten.mul.Tensor %5274, %5275 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16>
    %5277 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6162 = torch.constant.int 1
    %5278 = torch.aten.add.Tensor %5276, %5277, %int1_6162 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6163 = torch.constant.int 5
    %5279 = torch.prims.convert_element_type %5278, %int5_6163 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16>
    %5280 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6164 = torch.constant.int 0
    %int1_6165 = torch.constant.int 1
    %5281 = torch.aten.transpose.int %5280, %int0_6164, %int1_6165 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6166 = torch.constant.int 1920
    %int1280_6167 = torch.constant.int 1280
    %5282 = torch.prim.ListConstruct %int1920_6166, %int1280_6167 : (!torch.int, !torch.int) -> !torch.list<int>
    %5283 = torch.aten.view %5279, %5282 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5284 = torch.aten.mm %5283, %5281 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6168 = torch.constant.int 2
    %int960_6169 = torch.constant.int 960
    %int1280_6170 = torch.constant.int 1280
    %5285 = torch.prim.ListConstruct %int2_6168, %int960_6169, %int1280_6170 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5286 = torch.aten.view %5284, %5285 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16>
    %5287 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6171 = torch.constant.int 0
    %int1_6172 = torch.constant.int 1
    %5288 = torch.aten.transpose.int %5287, %int0_6171, %int1_6172 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6173 = torch.constant.int 32
    %int2048_6174 = torch.constant.int 2048
    %5289 = torch.prim.ListConstruct %int32_6173, %int2048_6174 : (!torch.int, !torch.int) -> !torch.list<int>
    %5290 = torch.aten.view %arg6, %5289 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5291 = torch.aten.mm %5290, %5288 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6175 = torch.constant.int 2
    %int16_6176 = torch.constant.int 16
    %int1280_6177 = torch.constant.int 1280
    %5292 = torch.prim.ListConstruct %int2_6175, %int16_6176, %int1280_6177 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5293 = torch.aten.view %5291, %5292 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16>
    %5294 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6178 = torch.constant.int 0
    %int1_6179 = torch.constant.int 1
    %5295 = torch.aten.transpose.int %5294, %int0_6178, %int1_6179 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6180 = torch.constant.int 32
    %int2048_6181 = torch.constant.int 2048
    %5296 = torch.prim.ListConstruct %int32_6180, %int2048_6181 : (!torch.int, !torch.int) -> !torch.list<int>
    %5297 = torch.aten.view %arg6, %5296 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5298 = torch.aten.mm %5297, %5295 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6182 = torch.constant.int 2
    %int16_6183 = torch.constant.int 16
    %int1280_6184 = torch.constant.int 1280
    %5299 = torch.prim.ListConstruct %int2_6182, %int16_6183, %int1280_6184 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5300 = torch.aten.view %5298, %5299 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_6185 = torch.constant.int 2
    %int-1_6186 = torch.constant.int -1
    %int20_6187 = torch.constant.int 20
    %int64_6188 = torch.constant.int 64
    %5301 = torch.prim.ListConstruct %int2_6185, %int-1_6186, %int20_6187, %int64_6188 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5302 = torch.aten.view %5286, %5301 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6189 = torch.constant.int 1
    %int2_6190 = torch.constant.int 2
    %5303 = torch.aten.transpose.int %5302, %int1_6189, %int2_6190 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6191 = torch.constant.int 2
    %int-1_6192 = torch.constant.int -1
    %int20_6193 = torch.constant.int 20
    %int64_6194 = torch.constant.int 64
    %5304 = torch.prim.ListConstruct %int2_6191, %int-1_6192, %int20_6193, %int64_6194 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5305 = torch.aten.view %5293, %5304 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6195 = torch.constant.int 1
    %int2_6196 = torch.constant.int 2
    %5306 = torch.aten.transpose.int %5305, %int1_6195, %int2_6196 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_6197 = torch.constant.int 2
    %int-1_6198 = torch.constant.int -1
    %int20_6199 = torch.constant.int 20
    %int64_6200 = torch.constant.int 64
    %5307 = torch.prim.ListConstruct %int2_6197, %int-1_6198, %int20_6199, %int64_6200 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5308 = torch.aten.view %5300, %5307 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6201 = torch.constant.int 1
    %int2_6202 = torch.constant.int 2
    %5309 = torch.aten.transpose.int %5308, %int1_6201, %int2_6202 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_6203 = torch.constant.float 0.000000e+00
    %false_6204 = torch.constant.bool false
    %none_6205 = torch.constant.none
    %none_6206 = torch.constant.none
    %5310:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5303, %5306, %5309, %float0.000000e00_6203, %false_6204, %none_6205, %none_6206) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6207 = torch.constant.int 1
    %int2_6208 = torch.constant.int 2
    %5311 = torch.aten.transpose.int %5310#0, %int1_6207, %int2_6208 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6209 = torch.constant.int 2
    %int-1_6210 = torch.constant.int -1
    %int1280_6211 = torch.constant.int 1280
    %5312 = torch.prim.ListConstruct %int2_6209, %int-1_6210, %int1280_6211 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5313 = torch.aten.view %5311, %5312 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6212 = torch.constant.int 5
    %5314 = torch.prims.convert_element_type %5313, %int5_6212 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6213 = torch.constant.int 1920
    %int1280_6214 = torch.constant.int 1280
    %5315 = torch.prim.ListConstruct %int1920_6213, %int1280_6214 : (!torch.int, !torch.int) -> !torch.list<int>
    %5316 = torch.aten.view %5314, %5315 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5317 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6215 = torch.constant.int 0
    %int1_6216 = torch.constant.int 1
    %5318 = torch.aten.transpose.int %5317, %int0_6215, %int1_6216 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16>
    %5319 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6217 = torch.constant.int 6
    %5320 = torch.prims.convert_element_type %5319, %int6_6217 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6218 = torch.constant.int 6
    %5321 = torch.prims.convert_element_type %5316, %int6_6218 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6219 = torch.constant.int 6
    %5322 = torch.prims.convert_element_type %5318, %int6_6219 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5323 = torch.aten.mm %5321, %5322 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6220 = torch.constant.int 1
    %5324 = torch.aten.mul.Scalar %5323, %int1_6220 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6221 = torch.constant.int 1
    %5325 = torch.aten.mul.Scalar %5320, %int1_6221 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6222 = torch.constant.int 1
    %5326 = torch.aten.add.Tensor %5324, %5325, %int1_6222 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6223 = torch.constant.int 5
    %5327 = torch.prims.convert_element_type %5326, %int5_6223 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6224 = torch.constant.int 2
    %int960_6225 = torch.constant.int 960
    %int1280_6226 = torch.constant.int 1280
    %5328 = torch.prim.ListConstruct %int2_6224, %int960_6225, %int1280_6226 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5329 = torch.aten.view %5327, %5328 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6227 = torch.constant.none
    %5330 = torch.aten.clone %5329, %none_6227 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6228 = torch.constant.float 1.000000e+00
    %5331 = torch.aten.div.Scalar %5330, %float1.000000e00_6228 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6229 = torch.constant.int 1
    %5332 = torch.aten.add.Tensor %5331, %5268, %int1_6229 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6230 = torch.constant.int 6
    %5333 = torch.prims.convert_element_type %5332, %int6_6230 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6231 = torch.constant.int 2
    %5334 = torch.prim.ListConstruct %int2_6231 : (!torch.int) -> !torch.list<int>
    %int0_6232 = torch.constant.int 0
    %true_6233 = torch.constant.bool true
    %result0_6234, %result1_6235 = torch.aten.var_mean.correction %5333, %5334, %int0_6232, %true_6233 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6236 = torch.constant.float 1.000000e-05
    %int1_6237 = torch.constant.int 1
    %5335 = torch.aten.add.Scalar %result0_6234, %float1.000000e-05_6236, %int1_6237 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5336 = torch.aten.rsqrt %5335 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6238 = torch.constant.int 1
    %5337 = torch.aten.sub.Tensor %5332, %result1_6235, %int1_6238 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5338 = torch.aten.mul.Tensor %5337, %5336 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16>
    %5339 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5340 = torch.aten.mul.Tensor %5338, %5339 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16>
    %5341 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6239 = torch.constant.int 1
    %5342 = torch.aten.add.Tensor %5340, %5341, %int1_6239 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6240 = torch.constant.int 5
    %5343 = torch.prims.convert_element_type %5342, %int5_6240 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6241 = torch.constant.int 1920
    %int1280_6242 = torch.constant.int 1280
    %5344 = torch.prim.ListConstruct %int1920_6241, %int1280_6242 : (!torch.int, !torch.int) -> !torch.list<int>
    %5345 = torch.aten.view %5343, %5344 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5346 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_6243 = torch.constant.int 0
    %int1_6244 = torch.constant.int 1
    %5347 = torch.aten.transpose.int %5346, %int0_6243, %int1_6244 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16>
    %5348 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_6245 = torch.constant.int 6
    %5349 = torch.prims.convert_element_type %5348, %int6_6245 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_6246 = torch.constant.int 6
    %5350 = torch.prims.convert_element_type %5345, %int6_6246 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6247 = torch.constant.int 6
    %5351 = torch.prims.convert_element_type %5347, %int6_6247 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5352 = torch.aten.mm %5350, %5351 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_6248 = torch.constant.int 1
    %5353 = torch.aten.mul.Scalar %5352, %int1_6248 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_6249 = torch.constant.int 1
    %5354 = torch.aten.mul.Scalar %5349, %int1_6249 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_6250 = torch.constant.int 1
    %5355 = torch.aten.add.Tensor %5353, %5354, %int1_6250 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_6251 = torch.constant.int 5
    %5356 = torch.prims.convert_element_type %5355, %int5_6251 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_6252 = torch.constant.int 2
    %int960_6253 = torch.constant.int 960
    %int10240_6254 = torch.constant.int 10240
    %5357 = torch.prim.ListConstruct %int2_6252, %int960_6253, %int10240_6254 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5358 = torch.aten.view %5356, %5357 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_6255 = torch.constant.int -1
    %int0_6256 = torch.constant.int 0
    %int5120_6257 = torch.constant.int 5120
    %int1_6258 = torch.constant.int 1
    %5359 = torch.aten.slice.Tensor %5358, %int-1_6255, %int0_6256, %int5120_6257, %int1_6258 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_6259 = torch.constant.int -1
    %int5120_6260 = torch.constant.int 5120
    %int10240_6261 = torch.constant.int 10240
    %int1_6262 = torch.constant.int 1
    %5360 = torch.aten.slice.Tensor %5358, %int-1_6259, %int5120_6260, %int10240_6261, %int1_6262 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_6263 = torch.constant.str "none"
    %5361 = torch.aten.gelu %5360, %str_6263 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %5362 = torch.aten.mul.Tensor %5359, %5361 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_6264 = torch.constant.none
    %5363 = torch.aten.clone %5362, %none_6264 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_6265 = torch.constant.int 1920
    %int5120_6266 = torch.constant.int 5120
    %5364 = torch.prim.ListConstruct %int1920_6265, %int5120_6266 : (!torch.int, !torch.int) -> !torch.list<int>
    %5365 = torch.aten.view %5363, %5364 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16>
    %5366 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_6267 = torch.constant.int 0
    %int1_6268 = torch.constant.int 1
    %5367 = torch.aten.transpose.int %5366, %int0_6267, %int1_6268 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16>
    %5368 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.0.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6269 = torch.constant.int 6
    %5369 = torch.prims.convert_element_type %5368, %int6_6269 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6270 = torch.constant.int 6
    %5370 = torch.prims.convert_element_type %5365, %int6_6270 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_6271 = torch.constant.int 6
    %5371 = torch.prims.convert_element_type %5367, %int6_6271 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5372 = torch.aten.mm %5370, %5371 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6272 = torch.constant.int 1
    %5373 = torch.aten.mul.Scalar %5372, %int1_6272 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6273 = torch.constant.int 1
    %5374 = torch.aten.mul.Scalar %5369, %int1_6273 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6274 = torch.constant.int 1
    %5375 = torch.aten.add.Tensor %5373, %5374, %int1_6274 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6275 = torch.constant.int 5
    %5376 = torch.prims.convert_element_type %5375, %int5_6275 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6276 = torch.constant.int 2
    %int960_6277 = torch.constant.int 960
    %int1280_6278 = torch.constant.int 1280
    %5377 = torch.prim.ListConstruct %int2_6276, %int960_6277, %int1280_6278 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5378 = torch.aten.view %5376, %5377 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_6279 = torch.constant.int 1
    %5379 = torch.aten.add.Tensor %5378, %5332, %int1_6279 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6280 = torch.constant.int 6
    %5380 = torch.prims.convert_element_type %5379, %int6_6280 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6281 = torch.constant.int 2
    %5381 = torch.prim.ListConstruct %int2_6281 : (!torch.int) -> !torch.list<int>
    %int0_6282 = torch.constant.int 0
    %true_6283 = torch.constant.bool true
    %result0_6284, %result1_6285 = torch.aten.var_mean.correction %5380, %5381, %int0_6282, %true_6283 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6286 = torch.constant.float 1.000000e-05
    %int1_6287 = torch.constant.int 1
    %5382 = torch.aten.add.Scalar %result0_6284, %float1.000000e-05_6286, %int1_6287 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5383 = torch.aten.rsqrt %5382 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6288 = torch.constant.int 1
    %5384 = torch.aten.sub.Tensor %5379, %result1_6285, %int1_6288 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5385 = torch.aten.mul.Tensor %5384, %5383 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16>
    %5386 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5387 = torch.aten.mul.Tensor %5385, %5386 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16>
    %5388 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6289 = torch.constant.int 1
    %5389 = torch.aten.add.Tensor %5387, %5388, %int1_6289 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6290 = torch.constant.int 5
    %5390 = torch.prims.convert_element_type %5389, %int5_6290 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16>
    %5391 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6291 = torch.constant.int 0
    %int1_6292 = torch.constant.int 1
    %5392 = torch.aten.transpose.int %5391, %int0_6291, %int1_6292 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6293 = torch.constant.int 1920
    %int1280_6294 = torch.constant.int 1280
    %5393 = torch.prim.ListConstruct %int1920_6293, %int1280_6294 : (!torch.int, !torch.int) -> !torch.list<int>
    %5394 = torch.aten.view %5390, %5393 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5395 = torch.aten.mm %5394, %5392 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6295 = torch.constant.int 2
    %int960_6296 = torch.constant.int 960
    %int1280_6297 = torch.constant.int 1280
    %5396 = torch.prim.ListConstruct %int2_6295, %int960_6296, %int1280_6297 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5397 = torch.aten.view %5395, %5396 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16>
    %5398 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6298 = torch.constant.int 0
    %int1_6299 = torch.constant.int 1
    %5399 = torch.aten.transpose.int %5398, %int0_6298, %int1_6299 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6300 = torch.constant.int 1920
    %int1280_6301 = torch.constant.int 1280
    %5400 = torch.prim.ListConstruct %int1920_6300, %int1280_6301 : (!torch.int, !torch.int) -> !torch.list<int>
    %5401 = torch.aten.view %5390, %5400 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5402 = torch.aten.mm %5401, %5399 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6302 = torch.constant.int 2
    %int960_6303 = torch.constant.int 960
    %int1280_6304 = torch.constant.int 1280
    %5403 = torch.prim.ListConstruct %int2_6302, %int960_6303, %int1280_6304 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5404 = torch.aten.view %5402, %5403 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16>
    %5405 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6305 = torch.constant.int 0
    %int1_6306 = torch.constant.int 1
    %5406 = torch.aten.transpose.int %5405, %int0_6305, %int1_6306 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6307 = torch.constant.int 1920
    %int1280_6308 = torch.constant.int 1280
    %5407 = torch.prim.ListConstruct %int1920_6307, %int1280_6308 : (!torch.int, !torch.int) -> !torch.list<int>
    %5408 = torch.aten.view %5390, %5407 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5409 = torch.aten.mm %5408, %5406 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6309 = torch.constant.int 2
    %int960_6310 = torch.constant.int 960
    %int1280_6311 = torch.constant.int 1280
    %5410 = torch.prim.ListConstruct %int2_6309, %int960_6310, %int1280_6311 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5411 = torch.aten.view %5409, %5410 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_6312 = torch.constant.int 2
    %int-1_6313 = torch.constant.int -1
    %int20_6314 = torch.constant.int 20
    %int64_6315 = torch.constant.int 64
    %5412 = torch.prim.ListConstruct %int2_6312, %int-1_6313, %int20_6314, %int64_6315 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5413 = torch.aten.view %5397, %5412 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6316 = torch.constant.int 1
    %int2_6317 = torch.constant.int 2
    %5414 = torch.aten.transpose.int %5413, %int1_6316, %int2_6317 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6318 = torch.constant.int 2
    %int-1_6319 = torch.constant.int -1
    %int20_6320 = torch.constant.int 20
    %int64_6321 = torch.constant.int 64
    %5415 = torch.prim.ListConstruct %int2_6318, %int-1_6319, %int20_6320, %int64_6321 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5416 = torch.aten.view %5404, %5415 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6322 = torch.constant.int 1
    %int2_6323 = torch.constant.int 2
    %5417 = torch.aten.transpose.int %5416, %int1_6322, %int2_6323 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6324 = torch.constant.int 2
    %int-1_6325 = torch.constant.int -1
    %int20_6326 = torch.constant.int 20
    %int64_6327 = torch.constant.int 64
    %5418 = torch.prim.ListConstruct %int2_6324, %int-1_6325, %int20_6326, %int64_6327 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5419 = torch.aten.view %5411, %5418 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6328 = torch.constant.int 1
    %int2_6329 = torch.constant.int 2
    %5420 = torch.aten.transpose.int %5419, %int1_6328, %int2_6329 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_6330 = torch.constant.float 0.000000e+00
    %false_6331 = torch.constant.bool false
    %none_6332 = torch.constant.none
    %none_6333 = torch.constant.none
    %5421:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5414, %5417, %5420, %float0.000000e00_6330, %false_6331, %none_6332, %none_6333) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6334 = torch.constant.int 1
    %int2_6335 = torch.constant.int 2
    %5422 = torch.aten.transpose.int %5421#0, %int1_6334, %int2_6335 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6336 = torch.constant.int 2
    %int-1_6337 = torch.constant.int -1
    %int1280_6338 = torch.constant.int 1280
    %5423 = torch.prim.ListConstruct %int2_6336, %int-1_6337, %int1280_6338 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5424 = torch.aten.view %5422, %5423 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6339 = torch.constant.int 5
    %5425 = torch.prims.convert_element_type %5424, %int5_6339 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6340 = torch.constant.int 1920
    %int1280_6341 = torch.constant.int 1280
    %5426 = torch.prim.ListConstruct %int1920_6340, %int1280_6341 : (!torch.int, !torch.int) -> !torch.list<int>
    %5427 = torch.aten.view %5425, %5426 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5428 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6342 = torch.constant.int 0
    %int1_6343 = torch.constant.int 1
    %5429 = torch.aten.transpose.int %5428, %int0_6342, %int1_6343 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16>
    %5430 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6344 = torch.constant.int 6
    %5431 = torch.prims.convert_element_type %5430, %int6_6344 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6345 = torch.constant.int 6
    %5432 = torch.prims.convert_element_type %5427, %int6_6345 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6346 = torch.constant.int 6
    %5433 = torch.prims.convert_element_type %5429, %int6_6346 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5434 = torch.aten.mm %5432, %5433 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6347 = torch.constant.int 1
    %5435 = torch.aten.mul.Scalar %5434, %int1_6347 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6348 = torch.constant.int 1
    %5436 = torch.aten.mul.Scalar %5431, %int1_6348 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6349 = torch.constant.int 1
    %5437 = torch.aten.add.Tensor %5435, %5436, %int1_6349 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6350 = torch.constant.int 5
    %5438 = torch.prims.convert_element_type %5437, %int5_6350 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6351 = torch.constant.int 2
    %int960_6352 = torch.constant.int 960
    %int1280_6353 = torch.constant.int 1280
    %5439 = torch.prim.ListConstruct %int2_6351, %int960_6352, %int1280_6353 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5440 = torch.aten.view %5438, %5439 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6354 = torch.constant.none
    %5441 = torch.aten.clone %5440, %none_6354 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6355 = torch.constant.float 1.000000e+00
    %5442 = torch.aten.div.Scalar %5441, %float1.000000e00_6355 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6356 = torch.constant.int 1
    %5443 = torch.aten.add.Tensor %5442, %5379, %int1_6356 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6357 = torch.constant.int 6
    %5444 = torch.prims.convert_element_type %5443, %int6_6357 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6358 = torch.constant.int 2
    %5445 = torch.prim.ListConstruct %int2_6358 : (!torch.int) -> !torch.list<int>
    %int0_6359 = torch.constant.int 0
    %true_6360 = torch.constant.bool true
    %result0_6361, %result1_6362 = torch.aten.var_mean.correction %5444, %5445, %int0_6359, %true_6360 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6363 = torch.constant.float 1.000000e-05
    %int1_6364 = torch.constant.int 1
    %5446 = torch.aten.add.Scalar %result0_6361, %float1.000000e-05_6363, %int1_6364 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5447 = torch.aten.rsqrt %5446 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6365 = torch.constant.int 1
    %5448 = torch.aten.sub.Tensor %5443, %result1_6362, %int1_6365 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5449 = torch.aten.mul.Tensor %5448, %5447 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16>
    %5450 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5451 = torch.aten.mul.Tensor %5449, %5450 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16>
    %5452 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6366 = torch.constant.int 1
    %5453 = torch.aten.add.Tensor %5451, %5452, %int1_6366 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6367 = torch.constant.int 5
    %5454 = torch.prims.convert_element_type %5453, %int5_6367 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16>
    %5455 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6368 = torch.constant.int 0
    %int1_6369 = torch.constant.int 1
    %5456 = torch.aten.transpose.int %5455, %int0_6368, %int1_6369 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6370 = torch.constant.int 1920
    %int1280_6371 = torch.constant.int 1280
    %5457 = torch.prim.ListConstruct %int1920_6370, %int1280_6371 : (!torch.int, !torch.int) -> !torch.list<int>
    %5458 = torch.aten.view %5454, %5457 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5459 = torch.aten.mm %5458, %5456 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6372 = torch.constant.int 2
    %int960_6373 = torch.constant.int 960
    %int1280_6374 = torch.constant.int 1280
    %5460 = torch.prim.ListConstruct %int2_6372, %int960_6373, %int1280_6374 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5461 = torch.aten.view %5459, %5460 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16>
    %5462 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6375 = torch.constant.int 0
    %int1_6376 = torch.constant.int 1
    %5463 = torch.aten.transpose.int %5462, %int0_6375, %int1_6376 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6377 = torch.constant.int 32
    %int2048_6378 = torch.constant.int 2048
    %5464 = torch.prim.ListConstruct %int32_6377, %int2048_6378 : (!torch.int, !torch.int) -> !torch.list<int>
    %5465 = torch.aten.view %arg6, %5464 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5466 = torch.aten.mm %5465, %5463 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6379 = torch.constant.int 2
    %int16_6380 = torch.constant.int 16
    %int1280_6381 = torch.constant.int 1280
    %5467 = torch.prim.ListConstruct %int2_6379, %int16_6380, %int1280_6381 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5468 = torch.aten.view %5466, %5467 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16>
    %5469 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6382 = torch.constant.int 0
    %int1_6383 = torch.constant.int 1
    %5470 = torch.aten.transpose.int %5469, %int0_6382, %int1_6383 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6384 = torch.constant.int 32
    %int2048_6385 = torch.constant.int 2048
    %5471 = torch.prim.ListConstruct %int32_6384, %int2048_6385 : (!torch.int, !torch.int) -> !torch.list<int>
    %5472 = torch.aten.view %arg6, %5471 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5473 = torch.aten.mm %5472, %5470 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6386 = torch.constant.int 2
    %int16_6387 = torch.constant.int 16
    %int1280_6388 = torch.constant.int 1280
    %5474 = torch.prim.ListConstruct %int2_6386, %int16_6387, %int1280_6388 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5475 = torch.aten.view %5473, %5474 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_6389 = torch.constant.int 2
    %int-1_6390 = torch.constant.int -1
    %int20_6391 = torch.constant.int 20
    %int64_6392 = torch.constant.int 64
    %5476 = torch.prim.ListConstruct %int2_6389, %int-1_6390, %int20_6391, %int64_6392 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5477 = torch.aten.view %5461, %5476 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6393 = torch.constant.int 1
    %int2_6394 = torch.constant.int 2
    %5478 = torch.aten.transpose.int %5477, %int1_6393, %int2_6394 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6395 = torch.constant.int 2
    %int-1_6396 = torch.constant.int -1
    %int20_6397 = torch.constant.int 20
    %int64_6398 = torch.constant.int 64
    %5479 = torch.prim.ListConstruct %int2_6395, %int-1_6396, %int20_6397, %int64_6398 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5480 = torch.aten.view %5468, %5479 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6399 = torch.constant.int 1
    %int2_6400 = torch.constant.int 2
    %5481 = torch.aten.transpose.int %5480, %int1_6399, %int2_6400 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_6401 = torch.constant.int 2
    %int-1_6402 = torch.constant.int -1
    %int20_6403 = torch.constant.int 20
    %int64_6404 = torch.constant.int 64
    %5482 = torch.prim.ListConstruct %int2_6401, %int-1_6402, %int20_6403, %int64_6404 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5483 = torch.aten.view %5475, %5482 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6405 = torch.constant.int 1
    %int2_6406 = torch.constant.int 2
    %5484 = torch.aten.transpose.int %5483, %int1_6405, %int2_6406 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_6407 = torch.constant.float 0.000000e+00
    %false_6408 = torch.constant.bool false
    %none_6409 = torch.constant.none
    %none_6410 = torch.constant.none
    %5485:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5478, %5481, %5484, %float0.000000e00_6407, %false_6408, %none_6409, %none_6410) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6411 = torch.constant.int 1
    %int2_6412 = torch.constant.int 2
    %5486 = torch.aten.transpose.int %5485#0, %int1_6411, %int2_6412 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6413 = torch.constant.int 2
    %int-1_6414 = torch.constant.int -1
    %int1280_6415 = torch.constant.int 1280
    %5487 = torch.prim.ListConstruct %int2_6413, %int-1_6414, %int1280_6415 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5488 = torch.aten.view %5486, %5487 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6416 = torch.constant.int 5
    %5489 = torch.prims.convert_element_type %5488, %int5_6416 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6417 = torch.constant.int 1920
    %int1280_6418 = torch.constant.int 1280
    %5490 = torch.prim.ListConstruct %int1920_6417, %int1280_6418 : (!torch.int, !torch.int) -> !torch.list<int>
    %5491 = torch.aten.view %5489, %5490 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5492 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6419 = torch.constant.int 0
    %int1_6420 = torch.constant.int 1
    %5493 = torch.aten.transpose.int %5492, %int0_6419, %int1_6420 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16>
    %5494 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6421 = torch.constant.int 6
    %5495 = torch.prims.convert_element_type %5494, %int6_6421 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6422 = torch.constant.int 6
    %5496 = torch.prims.convert_element_type %5491, %int6_6422 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6423 = torch.constant.int 6
    %5497 = torch.prims.convert_element_type %5493, %int6_6423 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5498 = torch.aten.mm %5496, %5497 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6424 = torch.constant.int 1
    %5499 = torch.aten.mul.Scalar %5498, %int1_6424 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6425 = torch.constant.int 1
    %5500 = torch.aten.mul.Scalar %5495, %int1_6425 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6426 = torch.constant.int 1
    %5501 = torch.aten.add.Tensor %5499, %5500, %int1_6426 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6427 = torch.constant.int 5
    %5502 = torch.prims.convert_element_type %5501, %int5_6427 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6428 = torch.constant.int 2
    %int960_6429 = torch.constant.int 960
    %int1280_6430 = torch.constant.int 1280
    %5503 = torch.prim.ListConstruct %int2_6428, %int960_6429, %int1280_6430 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5504 = torch.aten.view %5502, %5503 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6431 = torch.constant.none
    %5505 = torch.aten.clone %5504, %none_6431 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6432 = torch.constant.float 1.000000e+00
    %5506 = torch.aten.div.Scalar %5505, %float1.000000e00_6432 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6433 = torch.constant.int 1
    %5507 = torch.aten.add.Tensor %5506, %5443, %int1_6433 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6434 = torch.constant.int 6
    %5508 = torch.prims.convert_element_type %5507, %int6_6434 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6435 = torch.constant.int 2
    %5509 = torch.prim.ListConstruct %int2_6435 : (!torch.int) -> !torch.list<int>
    %int0_6436 = torch.constant.int 0
    %true_6437 = torch.constant.bool true
    %result0_6438, %result1_6439 = torch.aten.var_mean.correction %5508, %5509, %int0_6436, %true_6437 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6440 = torch.constant.float 1.000000e-05
    %int1_6441 = torch.constant.int 1
    %5510 = torch.aten.add.Scalar %result0_6438, %float1.000000e-05_6440, %int1_6441 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5511 = torch.aten.rsqrt %5510 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6442 = torch.constant.int 1
    %5512 = torch.aten.sub.Tensor %5507, %result1_6439, %int1_6442 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5513 = torch.aten.mul.Tensor %5512, %5511 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16>
    %5514 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5515 = torch.aten.mul.Tensor %5513, %5514 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16>
    %5516 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6443 = torch.constant.int 1
    %5517 = torch.aten.add.Tensor %5515, %5516, %int1_6443 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6444 = torch.constant.int 5
    %5518 = torch.prims.convert_element_type %5517, %int5_6444 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6445 = torch.constant.int 1920
    %int1280_6446 = torch.constant.int 1280
    %5519 = torch.prim.ListConstruct %int1920_6445, %int1280_6446 : (!torch.int, !torch.int) -> !torch.list<int>
    %5520 = torch.aten.view %5518, %5519 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5521 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_6447 = torch.constant.int 0
    %int1_6448 = torch.constant.int 1
    %5522 = torch.aten.transpose.int %5521, %int0_6447, %int1_6448 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16>
    %5523 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_6449 = torch.constant.int 6
    %5524 = torch.prims.convert_element_type %5523, %int6_6449 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_6450 = torch.constant.int 6
    %5525 = torch.prims.convert_element_type %5520, %int6_6450 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6451 = torch.constant.int 6
    %5526 = torch.prims.convert_element_type %5522, %int6_6451 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5527 = torch.aten.mm %5525, %5526 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_6452 = torch.constant.int 1
    %5528 = torch.aten.mul.Scalar %5527, %int1_6452 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_6453 = torch.constant.int 1
    %5529 = torch.aten.mul.Scalar %5524, %int1_6453 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_6454 = torch.constant.int 1
    %5530 = torch.aten.add.Tensor %5528, %5529, %int1_6454 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_6455 = torch.constant.int 5
    %5531 = torch.prims.convert_element_type %5530, %int5_6455 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_6456 = torch.constant.int 2
    %int960_6457 = torch.constant.int 960
    %int10240_6458 = torch.constant.int 10240
    %5532 = torch.prim.ListConstruct %int2_6456, %int960_6457, %int10240_6458 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5533 = torch.aten.view %5531, %5532 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_6459 = torch.constant.int -1
    %int0_6460 = torch.constant.int 0
    %int5120_6461 = torch.constant.int 5120
    %int1_6462 = torch.constant.int 1
    %5534 = torch.aten.slice.Tensor %5533, %int-1_6459, %int0_6460, %int5120_6461, %int1_6462 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_6463 = torch.constant.int -1
    %int5120_6464 = torch.constant.int 5120
    %int10240_6465 = torch.constant.int 10240
    %int1_6466 = torch.constant.int 1
    %5535 = torch.aten.slice.Tensor %5533, %int-1_6463, %int5120_6464, %int10240_6465, %int1_6466 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_6467 = torch.constant.str "none"
    %5536 = torch.aten.gelu %5535, %str_6467 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %5537 = torch.aten.mul.Tensor %5534, %5536 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_6468 = torch.constant.none
    %5538 = torch.aten.clone %5537, %none_6468 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_6469 = torch.constant.int 1920
    %int5120_6470 = torch.constant.int 5120
    %5539 = torch.prim.ListConstruct %int1920_6469, %int5120_6470 : (!torch.int, !torch.int) -> !torch.list<int>
    %5540 = torch.aten.view %5538, %5539 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16>
    %5541 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_6471 = torch.constant.int 0
    %int1_6472 = torch.constant.int 1
    %5542 = torch.aten.transpose.int %5541, %int0_6471, %int1_6472 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16>
    %5543 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.1.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6473 = torch.constant.int 6
    %5544 = torch.prims.convert_element_type %5543, %int6_6473 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6474 = torch.constant.int 6
    %5545 = torch.prims.convert_element_type %5540, %int6_6474 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_6475 = torch.constant.int 6
    %5546 = torch.prims.convert_element_type %5542, %int6_6475 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5547 = torch.aten.mm %5545, %5546 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6476 = torch.constant.int 1
    %5548 = torch.aten.mul.Scalar %5547, %int1_6476 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6477 = torch.constant.int 1
    %5549 = torch.aten.mul.Scalar %5544, %int1_6477 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6478 = torch.constant.int 1
    %5550 = torch.aten.add.Tensor %5548, %5549, %int1_6478 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6479 = torch.constant.int 5
    %5551 = torch.prims.convert_element_type %5550, %int5_6479 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6480 = torch.constant.int 2
    %int960_6481 = torch.constant.int 960
    %int1280_6482 = torch.constant.int 1280
    %5552 = torch.prim.ListConstruct %int2_6480, %int960_6481, %int1280_6482 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5553 = torch.aten.view %5551, %5552 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_6483 = torch.constant.int 1
    %5554 = torch.aten.add.Tensor %5553, %5507, %int1_6483 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6484 = torch.constant.int 6
    %5555 = torch.prims.convert_element_type %5554, %int6_6484 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6485 = torch.constant.int 2
    %5556 = torch.prim.ListConstruct %int2_6485 : (!torch.int) -> !torch.list<int>
    %int0_6486 = torch.constant.int 0
    %true_6487 = torch.constant.bool true
    %result0_6488, %result1_6489 = torch.aten.var_mean.correction %5555, %5556, %int0_6486, %true_6487 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6490 = torch.constant.float 1.000000e-05
    %int1_6491 = torch.constant.int 1
    %5557 = torch.aten.add.Scalar %result0_6488, %float1.000000e-05_6490, %int1_6491 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5558 = torch.aten.rsqrt %5557 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6492 = torch.constant.int 1
    %5559 = torch.aten.sub.Tensor %5554, %result1_6489, %int1_6492 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5560 = torch.aten.mul.Tensor %5559, %5558 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16>
    %5561 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5562 = torch.aten.mul.Tensor %5560, %5561 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16>
    %5563 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6493 = torch.constant.int 1
    %5564 = torch.aten.add.Tensor %5562, %5563, %int1_6493 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6494 = torch.constant.int 5
    %5565 = torch.prims.convert_element_type %5564, %int5_6494 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16>
    %5566 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6495 = torch.constant.int 0
    %int1_6496 = torch.constant.int 1
    %5567 = torch.aten.transpose.int %5566, %int0_6495, %int1_6496 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6497 = torch.constant.int 1920
    %int1280_6498 = torch.constant.int 1280
    %5568 = torch.prim.ListConstruct %int1920_6497, %int1280_6498 : (!torch.int, !torch.int) -> !torch.list<int>
    %5569 = torch.aten.view %5565, %5568 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5570 = torch.aten.mm %5569, %5567 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6499 = torch.constant.int 2
    %int960_6500 = torch.constant.int 960
    %int1280_6501 = torch.constant.int 1280
    %5571 = torch.prim.ListConstruct %int2_6499, %int960_6500, %int1280_6501 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5572 = torch.aten.view %5570, %5571 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16>
    %5573 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6502 = torch.constant.int 0
    %int1_6503 = torch.constant.int 1
    %5574 = torch.aten.transpose.int %5573, %int0_6502, %int1_6503 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6504 = torch.constant.int 1920
    %int1280_6505 = torch.constant.int 1280
    %5575 = torch.prim.ListConstruct %int1920_6504, %int1280_6505 : (!torch.int, !torch.int) -> !torch.list<int>
    %5576 = torch.aten.view %5565, %5575 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5577 = torch.aten.mm %5576, %5574 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6506 = torch.constant.int 2
    %int960_6507 = torch.constant.int 960
    %int1280_6508 = torch.constant.int 1280
    %5578 = torch.prim.ListConstruct %int2_6506, %int960_6507, %int1280_6508 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5579 = torch.aten.view %5577, %5578 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16>
    %5580 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6509 = torch.constant.int 0
    %int1_6510 = torch.constant.int 1
    %5581 = torch.aten.transpose.int %5580, %int0_6509, %int1_6510 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6511 = torch.constant.int 1920
    %int1280_6512 = torch.constant.int 1280
    %5582 = torch.prim.ListConstruct %int1920_6511, %int1280_6512 : (!torch.int, !torch.int) -> !torch.list<int>
    %5583 = torch.aten.view %5565, %5582 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5584 = torch.aten.mm %5583, %5581 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6513 = torch.constant.int 2
    %int960_6514 = torch.constant.int 960
    %int1280_6515 = torch.constant.int 1280
    %5585 = torch.prim.ListConstruct %int2_6513, %int960_6514, %int1280_6515 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5586 = torch.aten.view %5584, %5585 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_6516 = torch.constant.int 2
    %int-1_6517 = torch.constant.int -1
    %int20_6518 = torch.constant.int 20
    %int64_6519 = torch.constant.int 64
    %5587 = torch.prim.ListConstruct %int2_6516, %int-1_6517, %int20_6518, %int64_6519 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5588 = torch.aten.view %5572, %5587 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6520 = torch.constant.int 1
    %int2_6521 = torch.constant.int 2
    %5589 = torch.aten.transpose.int %5588, %int1_6520, %int2_6521 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6522 = torch.constant.int 2
    %int-1_6523 = torch.constant.int -1
    %int20_6524 = torch.constant.int 20
    %int64_6525 = torch.constant.int 64
    %5590 = torch.prim.ListConstruct %int2_6522, %int-1_6523, %int20_6524, %int64_6525 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5591 = torch.aten.view %5579, %5590 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6526 = torch.constant.int 1
    %int2_6527 = torch.constant.int 2
    %5592 = torch.aten.transpose.int %5591, %int1_6526, %int2_6527 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6528 = torch.constant.int 2
    %int-1_6529 = torch.constant.int -1
    %int20_6530 = torch.constant.int 20
    %int64_6531 = torch.constant.int 64
    %5593 = torch.prim.ListConstruct %int2_6528, %int-1_6529, %int20_6530, %int64_6531 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5594 = torch.aten.view %5586, %5593 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6532 = torch.constant.int 1
    %int2_6533 = torch.constant.int 2
    %5595 = torch.aten.transpose.int %5594, %int1_6532, %int2_6533 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_6534 = torch.constant.float 0.000000e+00
    %false_6535 = torch.constant.bool false
    %none_6536 = torch.constant.none
    %none_6537 = torch.constant.none
    %5596:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5589, %5592, %5595, %float0.000000e00_6534, %false_6535, %none_6536, %none_6537) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6538 = torch.constant.int 1
    %int2_6539 = torch.constant.int 2
    %5597 = torch.aten.transpose.int %5596#0, %int1_6538, %int2_6539 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6540 = torch.constant.int 2
    %int-1_6541 = torch.constant.int -1
    %int1280_6542 = torch.constant.int 1280
    %5598 = torch.prim.ListConstruct %int2_6540, %int-1_6541, %int1280_6542 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5599 = torch.aten.view %5597, %5598 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6543 = torch.constant.int 5
    %5600 = torch.prims.convert_element_type %5599, %int5_6543 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6544 = torch.constant.int 1920
    %int1280_6545 = torch.constant.int 1280
    %5601 = torch.prim.ListConstruct %int1920_6544, %int1280_6545 : (!torch.int, !torch.int) -> !torch.list<int>
    %5602 = torch.aten.view %5600, %5601 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5603 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6546 = torch.constant.int 0
    %int1_6547 = torch.constant.int 1
    %5604 = torch.aten.transpose.int %5603, %int0_6546, %int1_6547 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16>
    %5605 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6548 = torch.constant.int 6
    %5606 = torch.prims.convert_element_type %5605, %int6_6548 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6549 = torch.constant.int 6
    %5607 = torch.prims.convert_element_type %5602, %int6_6549 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6550 = torch.constant.int 6
    %5608 = torch.prims.convert_element_type %5604, %int6_6550 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5609 = torch.aten.mm %5607, %5608 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6551 = torch.constant.int 1
    %5610 = torch.aten.mul.Scalar %5609, %int1_6551 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6552 = torch.constant.int 1
    %5611 = torch.aten.mul.Scalar %5606, %int1_6552 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6553 = torch.constant.int 1
    %5612 = torch.aten.add.Tensor %5610, %5611, %int1_6553 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6554 = torch.constant.int 5
    %5613 = torch.prims.convert_element_type %5612, %int5_6554 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6555 = torch.constant.int 2
    %int960_6556 = torch.constant.int 960
    %int1280_6557 = torch.constant.int 1280
    %5614 = torch.prim.ListConstruct %int2_6555, %int960_6556, %int1280_6557 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5615 = torch.aten.view %5613, %5614 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6558 = torch.constant.none
    %5616 = torch.aten.clone %5615, %none_6558 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6559 = torch.constant.float 1.000000e+00
    %5617 = torch.aten.div.Scalar %5616, %float1.000000e00_6559 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6560 = torch.constant.int 1
    %5618 = torch.aten.add.Tensor %5617, %5554, %int1_6560 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6561 = torch.constant.int 6
    %5619 = torch.prims.convert_element_type %5618, %int6_6561 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6562 = torch.constant.int 2
    %5620 = torch.prim.ListConstruct %int2_6562 : (!torch.int) -> !torch.list<int>
    %int0_6563 = torch.constant.int 0
    %true_6564 = torch.constant.bool true
    %result0_6565, %result1_6566 = torch.aten.var_mean.correction %5619, %5620, %int0_6563, %true_6564 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6567 = torch.constant.float 1.000000e-05
    %int1_6568 = torch.constant.int 1
    %5621 = torch.aten.add.Scalar %result0_6565, %float1.000000e-05_6567, %int1_6568 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5622 = torch.aten.rsqrt %5621 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6569 = torch.constant.int 1
    %5623 = torch.aten.sub.Tensor %5618, %result1_6566, %int1_6569 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5624 = torch.aten.mul.Tensor %5623, %5622 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16>
    %5625 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5626 = torch.aten.mul.Tensor %5624, %5625 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16>
    %5627 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6570 = torch.constant.int 1
    %5628 = torch.aten.add.Tensor %5626, %5627, %int1_6570 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6571 = torch.constant.int 5
    %5629 = torch.prims.convert_element_type %5628, %int5_6571 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16>
    %5630 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6572 = torch.constant.int 0
    %int1_6573 = torch.constant.int 1
    %5631 = torch.aten.transpose.int %5630, %int0_6572, %int1_6573 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6574 = torch.constant.int 1920
    %int1280_6575 = torch.constant.int 1280
    %5632 = torch.prim.ListConstruct %int1920_6574, %int1280_6575 : (!torch.int, !torch.int) -> !torch.list<int>
    %5633 = torch.aten.view %5629, %5632 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5634 = torch.aten.mm %5633, %5631 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6576 = torch.constant.int 2
    %int960_6577 = torch.constant.int 960
    %int1280_6578 = torch.constant.int 1280
    %5635 = torch.prim.ListConstruct %int2_6576, %int960_6577, %int1280_6578 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5636 = torch.aten.view %5634, %5635 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16>
    %5637 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6579 = torch.constant.int 0
    %int1_6580 = torch.constant.int 1
    %5638 = torch.aten.transpose.int %5637, %int0_6579, %int1_6580 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6581 = torch.constant.int 32
    %int2048_6582 = torch.constant.int 2048
    %5639 = torch.prim.ListConstruct %int32_6581, %int2048_6582 : (!torch.int, !torch.int) -> !torch.list<int>
    %5640 = torch.aten.view %arg6, %5639 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5641 = torch.aten.mm %5640, %5638 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6583 = torch.constant.int 2
    %int16_6584 = torch.constant.int 16
    %int1280_6585 = torch.constant.int 1280
    %5642 = torch.prim.ListConstruct %int2_6583, %int16_6584, %int1280_6585 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5643 = torch.aten.view %5641, %5642 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16>
    %5644 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6586 = torch.constant.int 0
    %int1_6587 = torch.constant.int 1
    %5645 = torch.aten.transpose.int %5644, %int0_6586, %int1_6587 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6588 = torch.constant.int 32
    %int2048_6589 = torch.constant.int 2048
    %5646 = torch.prim.ListConstruct %int32_6588, %int2048_6589 : (!torch.int, !torch.int) -> !torch.list<int>
    %5647 = torch.aten.view %arg6, %5646 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5648 = torch.aten.mm %5647, %5645 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6590 = torch.constant.int 2
    %int16_6591 = torch.constant.int 16
    %int1280_6592 = torch.constant.int 1280
    %5649 = torch.prim.ListConstruct %int2_6590, %int16_6591, %int1280_6592 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5650 = torch.aten.view %5648, %5649 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_6593 = torch.constant.int 2
    %int-1_6594 = torch.constant.int -1
    %int20_6595 = torch.constant.int 20
    %int64_6596 = torch.constant.int 64
    %5651 = torch.prim.ListConstruct %int2_6593, %int-1_6594, %int20_6595, %int64_6596 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5652 = torch.aten.view %5636, %5651 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6597 = torch.constant.int 1
    %int2_6598 = torch.constant.int 2
    %5653 = torch.aten.transpose.int %5652, %int1_6597, %int2_6598 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6599 = torch.constant.int 2
    %int-1_6600 = torch.constant.int -1
    %int20_6601 = torch.constant.int 20
    %int64_6602 = torch.constant.int 64
    %5654 = torch.prim.ListConstruct %int2_6599, %int-1_6600, %int20_6601, %int64_6602 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5655 = torch.aten.view %5643, %5654 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6603 = torch.constant.int 1
    %int2_6604 = torch.constant.int 2
    %5656 = torch.aten.transpose.int %5655, %int1_6603, %int2_6604 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_6605 = torch.constant.int 2
    %int-1_6606 = torch.constant.int -1
    %int20_6607 = torch.constant.int 20
    %int64_6608 = torch.constant.int 64
    %5657 = torch.prim.ListConstruct %int2_6605, %int-1_6606, %int20_6607, %int64_6608 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5658 = torch.aten.view %5650, %5657 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6609 = torch.constant.int 1
    %int2_6610 = torch.constant.int 2
    %5659 = torch.aten.transpose.int %5658, %int1_6609, %int2_6610 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_6611 = torch.constant.float 0.000000e+00
    %false_6612 = torch.constant.bool false
    %none_6613 = torch.constant.none
    %none_6614 = torch.constant.none
    %5660:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5653, %5656, %5659, %float0.000000e00_6611, %false_6612, %none_6613, %none_6614) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6615 = torch.constant.int 1
    %int2_6616 = torch.constant.int 2
    %5661 = torch.aten.transpose.int %5660#0, %int1_6615, %int2_6616 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6617 = torch.constant.int 2
    %int-1_6618 = torch.constant.int -1
    %int1280_6619 = torch.constant.int 1280
    %5662 = torch.prim.ListConstruct %int2_6617, %int-1_6618, %int1280_6619 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5663 = torch.aten.view %5661, %5662 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6620 = torch.constant.int 5
    %5664 = torch.prims.convert_element_type %5663, %int5_6620 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6621 = torch.constant.int 1920
    %int1280_6622 = torch.constant.int 1280
    %5665 = torch.prim.ListConstruct %int1920_6621, %int1280_6622 : (!torch.int, !torch.int) -> !torch.list<int>
    %5666 = torch.aten.view %5664, %5665 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5667 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6623 = torch.constant.int 0
    %int1_6624 = torch.constant.int 1
    %5668 = torch.aten.transpose.int %5667, %int0_6623, %int1_6624 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16>
    %5669 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6625 = torch.constant.int 6
    %5670 = torch.prims.convert_element_type %5669, %int6_6625 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6626 = torch.constant.int 6
    %5671 = torch.prims.convert_element_type %5666, %int6_6626 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6627 = torch.constant.int 6
    %5672 = torch.prims.convert_element_type %5668, %int6_6627 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5673 = torch.aten.mm %5671, %5672 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6628 = torch.constant.int 1
    %5674 = torch.aten.mul.Scalar %5673, %int1_6628 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6629 = torch.constant.int 1
    %5675 = torch.aten.mul.Scalar %5670, %int1_6629 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6630 = torch.constant.int 1
    %5676 = torch.aten.add.Tensor %5674, %5675, %int1_6630 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6631 = torch.constant.int 5
    %5677 = torch.prims.convert_element_type %5676, %int5_6631 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6632 = torch.constant.int 2
    %int960_6633 = torch.constant.int 960
    %int1280_6634 = torch.constant.int 1280
    %5678 = torch.prim.ListConstruct %int2_6632, %int960_6633, %int1280_6634 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5679 = torch.aten.view %5677, %5678 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6635 = torch.constant.none
    %5680 = torch.aten.clone %5679, %none_6635 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6636 = torch.constant.float 1.000000e+00
    %5681 = torch.aten.div.Scalar %5680, %float1.000000e00_6636 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6637 = torch.constant.int 1
    %5682 = torch.aten.add.Tensor %5681, %5618, %int1_6637 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6638 = torch.constant.int 6
    %5683 = torch.prims.convert_element_type %5682, %int6_6638 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6639 = torch.constant.int 2
    %5684 = torch.prim.ListConstruct %int2_6639 : (!torch.int) -> !torch.list<int>
    %int0_6640 = torch.constant.int 0
    %true_6641 = torch.constant.bool true
    %result0_6642, %result1_6643 = torch.aten.var_mean.correction %5683, %5684, %int0_6640, %true_6641 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6644 = torch.constant.float 1.000000e-05
    %int1_6645 = torch.constant.int 1
    %5685 = torch.aten.add.Scalar %result0_6642, %float1.000000e-05_6644, %int1_6645 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5686 = torch.aten.rsqrt %5685 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6646 = torch.constant.int 1
    %5687 = torch.aten.sub.Tensor %5682, %result1_6643, %int1_6646 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5688 = torch.aten.mul.Tensor %5687, %5686 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16>
    %5689 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5690 = torch.aten.mul.Tensor %5688, %5689 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16>
    %5691 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6647 = torch.constant.int 1
    %5692 = torch.aten.add.Tensor %5690, %5691, %int1_6647 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6648 = torch.constant.int 5
    %5693 = torch.prims.convert_element_type %5692, %int5_6648 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6649 = torch.constant.int 1920
    %int1280_6650 = torch.constant.int 1280
    %5694 = torch.prim.ListConstruct %int1920_6649, %int1280_6650 : (!torch.int, !torch.int) -> !torch.list<int>
    %5695 = torch.aten.view %5693, %5694 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5696 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_6651 = torch.constant.int 0
    %int1_6652 = torch.constant.int 1
    %5697 = torch.aten.transpose.int %5696, %int0_6651, %int1_6652 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16>
    %5698 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_6653 = torch.constant.int 6
    %5699 = torch.prims.convert_element_type %5698, %int6_6653 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_6654 = torch.constant.int 6
    %5700 = torch.prims.convert_element_type %5695, %int6_6654 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6655 = torch.constant.int 6
    %5701 = torch.prims.convert_element_type %5697, %int6_6655 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5702 = torch.aten.mm %5700, %5701 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_6656 = torch.constant.int 1
    %5703 = torch.aten.mul.Scalar %5702, %int1_6656 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_6657 = torch.constant.int 1
    %5704 = torch.aten.mul.Scalar %5699, %int1_6657 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_6658 = torch.constant.int 1
    %5705 = torch.aten.add.Tensor %5703, %5704, %int1_6658 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_6659 = torch.constant.int 5
    %5706 = torch.prims.convert_element_type %5705, %int5_6659 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_6660 = torch.constant.int 2
    %int960_6661 = torch.constant.int 960
    %int10240_6662 = torch.constant.int 10240
    %5707 = torch.prim.ListConstruct %int2_6660, %int960_6661, %int10240_6662 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5708 = torch.aten.view %5706, %5707 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_6663 = torch.constant.int -1
    %int0_6664 = torch.constant.int 0
    %int5120_6665 = torch.constant.int 5120
    %int1_6666 = torch.constant.int 1
    %5709 = torch.aten.slice.Tensor %5708, %int-1_6663, %int0_6664, %int5120_6665, %int1_6666 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_6667 = torch.constant.int -1
    %int5120_6668 = torch.constant.int 5120
    %int10240_6669 = torch.constant.int 10240
    %int1_6670 = torch.constant.int 1
    %5710 = torch.aten.slice.Tensor %5708, %int-1_6667, %int5120_6668, %int10240_6669, %int1_6670 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_6671 = torch.constant.str "none"
    %5711 = torch.aten.gelu %5710, %str_6671 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %5712 = torch.aten.mul.Tensor %5709, %5711 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_6672 = torch.constant.none
    %5713 = torch.aten.clone %5712, %none_6672 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_6673 = torch.constant.int 1920
    %int5120_6674 = torch.constant.int 5120
    %5714 = torch.prim.ListConstruct %int1920_6673, %int5120_6674 : (!torch.int, !torch.int) -> !torch.list<int>
    %5715 = torch.aten.view %5713, %5714 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16>
    %5716 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_6675 = torch.constant.int 0
    %int1_6676 = torch.constant.int 1
    %5717 = torch.aten.transpose.int %5716, %int0_6675, %int1_6676 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16>
    %5718 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.2.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6677 = torch.constant.int 6
    %5719 = torch.prims.convert_element_type %5718, %int6_6677 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6678 = torch.constant.int 6
    %5720 = torch.prims.convert_element_type %5715, %int6_6678 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_6679 = torch.constant.int 6
    %5721 = torch.prims.convert_element_type %5717, %int6_6679 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5722 = torch.aten.mm %5720, %5721 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6680 = torch.constant.int 1
    %5723 = torch.aten.mul.Scalar %5722, %int1_6680 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6681 = torch.constant.int 1
    %5724 = torch.aten.mul.Scalar %5719, %int1_6681 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6682 = torch.constant.int 1
    %5725 = torch.aten.add.Tensor %5723, %5724, %int1_6682 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6683 = torch.constant.int 5
    %5726 = torch.prims.convert_element_type %5725, %int5_6683 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6684 = torch.constant.int 2
    %int960_6685 = torch.constant.int 960
    %int1280_6686 = torch.constant.int 1280
    %5727 = torch.prim.ListConstruct %int2_6684, %int960_6685, %int1280_6686 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5728 = torch.aten.view %5726, %5727 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_6687 = torch.constant.int 1
    %5729 = torch.aten.add.Tensor %5728, %5682, %int1_6687 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6688 = torch.constant.int 6
    %5730 = torch.prims.convert_element_type %5729, %int6_6688 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6689 = torch.constant.int 2
    %5731 = torch.prim.ListConstruct %int2_6689 : (!torch.int) -> !torch.list<int>
    %int0_6690 = torch.constant.int 0
    %true_6691 = torch.constant.bool true
    %result0_6692, %result1_6693 = torch.aten.var_mean.correction %5730, %5731, %int0_6690, %true_6691 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6694 = torch.constant.float 1.000000e-05
    %int1_6695 = torch.constant.int 1
    %5732 = torch.aten.add.Scalar %result0_6692, %float1.000000e-05_6694, %int1_6695 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5733 = torch.aten.rsqrt %5732 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6696 = torch.constant.int 1
    %5734 = torch.aten.sub.Tensor %5729, %result1_6693, %int1_6696 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5735 = torch.aten.mul.Tensor %5734, %5733 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16>
    %5736 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5737 = torch.aten.mul.Tensor %5735, %5736 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16>
    %5738 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6697 = torch.constant.int 1
    %5739 = torch.aten.add.Tensor %5737, %5738, %int1_6697 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6698 = torch.constant.int 5
    %5740 = torch.prims.convert_element_type %5739, %int5_6698 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16>
    %5741 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6699 = torch.constant.int 0
    %int1_6700 = torch.constant.int 1
    %5742 = torch.aten.transpose.int %5741, %int0_6699, %int1_6700 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6701 = torch.constant.int 1920
    %int1280_6702 = torch.constant.int 1280
    %5743 = torch.prim.ListConstruct %int1920_6701, %int1280_6702 : (!torch.int, !torch.int) -> !torch.list<int>
    %5744 = torch.aten.view %5740, %5743 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5745 = torch.aten.mm %5744, %5742 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6703 = torch.constant.int 2
    %int960_6704 = torch.constant.int 960
    %int1280_6705 = torch.constant.int 1280
    %5746 = torch.prim.ListConstruct %int2_6703, %int960_6704, %int1280_6705 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5747 = torch.aten.view %5745, %5746 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16>
    %5748 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6706 = torch.constant.int 0
    %int1_6707 = torch.constant.int 1
    %5749 = torch.aten.transpose.int %5748, %int0_6706, %int1_6707 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6708 = torch.constant.int 1920
    %int1280_6709 = torch.constant.int 1280
    %5750 = torch.prim.ListConstruct %int1920_6708, %int1280_6709 : (!torch.int, !torch.int) -> !torch.list<int>
    %5751 = torch.aten.view %5740, %5750 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5752 = torch.aten.mm %5751, %5749 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6710 = torch.constant.int 2
    %int960_6711 = torch.constant.int 960
    %int1280_6712 = torch.constant.int 1280
    %5753 = torch.prim.ListConstruct %int2_6710, %int960_6711, %int1280_6712 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5754 = torch.aten.view %5752, %5753 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16>
    %5755 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6713 = torch.constant.int 0
    %int1_6714 = torch.constant.int 1
    %5756 = torch.aten.transpose.int %5755, %int0_6713, %int1_6714 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6715 = torch.constant.int 1920
    %int1280_6716 = torch.constant.int 1280
    %5757 = torch.prim.ListConstruct %int1920_6715, %int1280_6716 : (!torch.int, !torch.int) -> !torch.list<int>
    %5758 = torch.aten.view %5740, %5757 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5759 = torch.aten.mm %5758, %5756 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6717 = torch.constant.int 2
    %int960_6718 = torch.constant.int 960
    %int1280_6719 = torch.constant.int 1280
    %5760 = torch.prim.ListConstruct %int2_6717, %int960_6718, %int1280_6719 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5761 = torch.aten.view %5759, %5760 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_6720 = torch.constant.int 2
    %int-1_6721 = torch.constant.int -1
    %int20_6722 = torch.constant.int 20
    %int64_6723 = torch.constant.int 64
    %5762 = torch.prim.ListConstruct %int2_6720, %int-1_6721, %int20_6722, %int64_6723 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5763 = torch.aten.view %5747, %5762 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6724 = torch.constant.int 1
    %int2_6725 = torch.constant.int 2
    %5764 = torch.aten.transpose.int %5763, %int1_6724, %int2_6725 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6726 = torch.constant.int 2
    %int-1_6727 = torch.constant.int -1
    %int20_6728 = torch.constant.int 20
    %int64_6729 = torch.constant.int 64
    %5765 = torch.prim.ListConstruct %int2_6726, %int-1_6727, %int20_6728, %int64_6729 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5766 = torch.aten.view %5754, %5765 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6730 = torch.constant.int 1
    %int2_6731 = torch.constant.int 2
    %5767 = torch.aten.transpose.int %5766, %int1_6730, %int2_6731 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6732 = torch.constant.int 2
    %int-1_6733 = torch.constant.int -1
    %int20_6734 = torch.constant.int 20
    %int64_6735 = torch.constant.int 64
    %5768 = torch.prim.ListConstruct %int2_6732, %int-1_6733, %int20_6734, %int64_6735 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5769 = torch.aten.view %5761, %5768 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6736 = torch.constant.int 1
    %int2_6737 = torch.constant.int 2
    %5770 = torch.aten.transpose.int %5769, %int1_6736, %int2_6737 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_6738 = torch.constant.float 0.000000e+00
    %false_6739 = torch.constant.bool false
    %none_6740 = torch.constant.none
    %none_6741 = torch.constant.none
    %5771:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5764, %5767, %5770, %float0.000000e00_6738, %false_6739, %none_6740, %none_6741) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6742 = torch.constant.int 1
    %int2_6743 = torch.constant.int 2
    %5772 = torch.aten.transpose.int %5771#0, %int1_6742, %int2_6743 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6744 = torch.constant.int 2
    %int-1_6745 = torch.constant.int -1
    %int1280_6746 = torch.constant.int 1280
    %5773 = torch.prim.ListConstruct %int2_6744, %int-1_6745, %int1280_6746 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5774 = torch.aten.view %5772, %5773 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6747 = torch.constant.int 5
    %5775 = torch.prims.convert_element_type %5774, %int5_6747 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6748 = torch.constant.int 1920
    %int1280_6749 = torch.constant.int 1280
    %5776 = torch.prim.ListConstruct %int1920_6748, %int1280_6749 : (!torch.int, !torch.int) -> !torch.list<int>
    %5777 = torch.aten.view %5775, %5776 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5778 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6750 = torch.constant.int 0
    %int1_6751 = torch.constant.int 1
    %5779 = torch.aten.transpose.int %5778, %int0_6750, %int1_6751 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16>
    %5780 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6752 = torch.constant.int 6
    %5781 = torch.prims.convert_element_type %5780, %int6_6752 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6753 = torch.constant.int 6
    %5782 = torch.prims.convert_element_type %5777, %int6_6753 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6754 = torch.constant.int 6
    %5783 = torch.prims.convert_element_type %5779, %int6_6754 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5784 = torch.aten.mm %5782, %5783 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6755 = torch.constant.int 1
    %5785 = torch.aten.mul.Scalar %5784, %int1_6755 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6756 = torch.constant.int 1
    %5786 = torch.aten.mul.Scalar %5781, %int1_6756 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6757 = torch.constant.int 1
    %5787 = torch.aten.add.Tensor %5785, %5786, %int1_6757 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6758 = torch.constant.int 5
    %5788 = torch.prims.convert_element_type %5787, %int5_6758 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6759 = torch.constant.int 2
    %int960_6760 = torch.constant.int 960
    %int1280_6761 = torch.constant.int 1280
    %5789 = torch.prim.ListConstruct %int2_6759, %int960_6760, %int1280_6761 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5790 = torch.aten.view %5788, %5789 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6762 = torch.constant.none
    %5791 = torch.aten.clone %5790, %none_6762 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6763 = torch.constant.float 1.000000e+00
    %5792 = torch.aten.div.Scalar %5791, %float1.000000e00_6763 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6764 = torch.constant.int 1
    %5793 = torch.aten.add.Tensor %5792, %5729, %int1_6764 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6765 = torch.constant.int 6
    %5794 = torch.prims.convert_element_type %5793, %int6_6765 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6766 = torch.constant.int 2
    %5795 = torch.prim.ListConstruct %int2_6766 : (!torch.int) -> !torch.list<int>
    %int0_6767 = torch.constant.int 0
    %true_6768 = torch.constant.bool true
    %result0_6769, %result1_6770 = torch.aten.var_mean.correction %5794, %5795, %int0_6767, %true_6768 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6771 = torch.constant.float 1.000000e-05
    %int1_6772 = torch.constant.int 1
    %5796 = torch.aten.add.Scalar %result0_6769, %float1.000000e-05_6771, %int1_6772 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5797 = torch.aten.rsqrt %5796 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6773 = torch.constant.int 1
    %5798 = torch.aten.sub.Tensor %5793, %result1_6770, %int1_6773 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5799 = torch.aten.mul.Tensor %5798, %5797 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16>
    %5800 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5801 = torch.aten.mul.Tensor %5799, %5800 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16>
    %5802 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6774 = torch.constant.int 1
    %5803 = torch.aten.add.Tensor %5801, %5802, %int1_6774 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6775 = torch.constant.int 5
    %5804 = torch.prims.convert_element_type %5803, %int5_6775 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16>
    %5805 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6776 = torch.constant.int 0
    %int1_6777 = torch.constant.int 1
    %5806 = torch.aten.transpose.int %5805, %int0_6776, %int1_6777 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6778 = torch.constant.int 1920
    %int1280_6779 = torch.constant.int 1280
    %5807 = torch.prim.ListConstruct %int1920_6778, %int1280_6779 : (!torch.int, !torch.int) -> !torch.list<int>
    %5808 = torch.aten.view %5804, %5807 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5809 = torch.aten.mm %5808, %5806 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6780 = torch.constant.int 2
    %int960_6781 = torch.constant.int 960
    %int1280_6782 = torch.constant.int 1280
    %5810 = torch.prim.ListConstruct %int2_6780, %int960_6781, %int1280_6782 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5811 = torch.aten.view %5809, %5810 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16>
    %5812 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6783 = torch.constant.int 0
    %int1_6784 = torch.constant.int 1
    %5813 = torch.aten.transpose.int %5812, %int0_6783, %int1_6784 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6785 = torch.constant.int 32
    %int2048_6786 = torch.constant.int 2048
    %5814 = torch.prim.ListConstruct %int32_6785, %int2048_6786 : (!torch.int, !torch.int) -> !torch.list<int>
    %5815 = torch.aten.view %arg6, %5814 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5816 = torch.aten.mm %5815, %5813 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6787 = torch.constant.int 2
    %int16_6788 = torch.constant.int 16
    %int1280_6789 = torch.constant.int 1280
    %5817 = torch.prim.ListConstruct %int2_6787, %int16_6788, %int1280_6789 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5818 = torch.aten.view %5816, %5817 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16>
    %5819 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6790 = torch.constant.int 0
    %int1_6791 = torch.constant.int 1
    %5820 = torch.aten.transpose.int %5819, %int0_6790, %int1_6791 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6792 = torch.constant.int 32
    %int2048_6793 = torch.constant.int 2048
    %5821 = torch.prim.ListConstruct %int32_6792, %int2048_6793 : (!torch.int, !torch.int) -> !torch.list<int>
    %5822 = torch.aten.view %arg6, %5821 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5823 = torch.aten.mm %5822, %5820 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6794 = torch.constant.int 2
    %int16_6795 = torch.constant.int 16
    %int1280_6796 = torch.constant.int 1280
    %5824 = torch.prim.ListConstruct %int2_6794, %int16_6795, %int1280_6796 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5825 = torch.aten.view %5823, %5824 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_6797 = torch.constant.int 2
    %int-1_6798 = torch.constant.int -1
    %int20_6799 = torch.constant.int 20
    %int64_6800 = torch.constant.int 64
    %5826 = torch.prim.ListConstruct %int2_6797, %int-1_6798, %int20_6799, %int64_6800 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5827 = torch.aten.view %5811, %5826 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6801 = torch.constant.int 1
    %int2_6802 = torch.constant.int 2
    %5828 = torch.aten.transpose.int %5827, %int1_6801, %int2_6802 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6803 = torch.constant.int 2
    %int-1_6804 = torch.constant.int -1
    %int20_6805 = torch.constant.int 20
    %int64_6806 = torch.constant.int 64
    %5829 = torch.prim.ListConstruct %int2_6803, %int-1_6804, %int20_6805, %int64_6806 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5830 = torch.aten.view %5818, %5829 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6807 = torch.constant.int 1
    %int2_6808 = torch.constant.int 2
    %5831 = torch.aten.transpose.int %5830, %int1_6807, %int2_6808 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_6809 = torch.constant.int 2
    %int-1_6810 = torch.constant.int -1
    %int20_6811 = torch.constant.int 20
    %int64_6812 = torch.constant.int 64
    %5832 = torch.prim.ListConstruct %int2_6809, %int-1_6810, %int20_6811, %int64_6812 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5833 = torch.aten.view %5825, %5832 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_6813 = torch.constant.int 1
    %int2_6814 = torch.constant.int 2
    %5834 = torch.aten.transpose.int %5833, %int1_6813, %int2_6814 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_6815 = torch.constant.float 0.000000e+00
    %false_6816 = torch.constant.bool false
    %none_6817 = torch.constant.none
    %none_6818 = torch.constant.none
    %5835:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5828, %5831, %5834, %float0.000000e00_6815, %false_6816, %none_6817, %none_6818) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6819 = torch.constant.int 1
    %int2_6820 = torch.constant.int 2
    %5836 = torch.aten.transpose.int %5835#0, %int1_6819, %int2_6820 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6821 = torch.constant.int 2
    %int-1_6822 = torch.constant.int -1
    %int1280_6823 = torch.constant.int 1280
    %5837 = torch.prim.ListConstruct %int2_6821, %int-1_6822, %int1280_6823 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5838 = torch.aten.view %5836, %5837 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6824 = torch.constant.int 5
    %5839 = torch.prims.convert_element_type %5838, %int5_6824 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6825 = torch.constant.int 1920
    %int1280_6826 = torch.constant.int 1280
    %5840 = torch.prim.ListConstruct %int1920_6825, %int1280_6826 : (!torch.int, !torch.int) -> !torch.list<int>
    %5841 = torch.aten.view %5839, %5840 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %5842 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6827 = torch.constant.int 0
    %int1_6828 = torch.constant.int 1
    %5843 = torch.aten.transpose.int %5842, %int0_6827, %int1_6828 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16>
    %5844 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6829 = torch.constant.int 6
    %5845 = torch.prims.convert_element_type %5844, %int6_6829 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6830 = torch.constant.int 6
    %5846 = torch.prims.convert_element_type %5841, %int6_6830 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6831 = torch.constant.int 6
    %5847 = torch.prims.convert_element_type %5843, %int6_6831 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5848 = torch.aten.mm %5846, %5847 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6832 = torch.constant.int 1
    %5849 = torch.aten.mul.Scalar %5848, %int1_6832 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6833 = torch.constant.int 1
    %5850 = torch.aten.mul.Scalar %5845, %int1_6833 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6834 = torch.constant.int 1
    %5851 = torch.aten.add.Tensor %5849, %5850, %int1_6834 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6835 = torch.constant.int 5
    %5852 = torch.prims.convert_element_type %5851, %int5_6835 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6836 = torch.constant.int 2
    %int960_6837 = torch.constant.int 960
    %int1280_6838 = torch.constant.int 1280
    %5853 = torch.prim.ListConstruct %int2_6836, %int960_6837, %int1280_6838 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5854 = torch.aten.view %5852, %5853 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6839 = torch.constant.none
    %5855 = torch.aten.clone %5854, %none_6839 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6840 = torch.constant.float 1.000000e+00
    %5856 = torch.aten.div.Scalar %5855, %float1.000000e00_6840 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6841 = torch.constant.int 1
    %5857 = torch.aten.add.Tensor %5856, %5793, %int1_6841 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6842 = torch.constant.int 6
    %5858 = torch.prims.convert_element_type %5857, %int6_6842 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6843 = torch.constant.int 2
    %5859 = torch.prim.ListConstruct %int2_6843 : (!torch.int) -> !torch.list<int>
    %int0_6844 = torch.constant.int 0
    %true_6845 = torch.constant.bool true
    %result0_6846, %result1_6847 = torch.aten.var_mean.correction %5858, %5859, %int0_6844, %true_6845 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6848 = torch.constant.float 1.000000e-05
    %int1_6849 = torch.constant.int 1
    %5860 = torch.aten.add.Scalar %result0_6846, %float1.000000e-05_6848, %int1_6849 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5861 = torch.aten.rsqrt %5860 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6850 = torch.constant.int 1
    %5862 = torch.aten.sub.Tensor %5857, %result1_6847, %int1_6850 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5863 = torch.aten.mul.Tensor %5862, %5861 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16>
    %5864 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5865 = torch.aten.mul.Tensor %5863, %5864 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16>
    %5866 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6851 = torch.constant.int 1
    %5867 = torch.aten.add.Tensor %5865, %5866, %int1_6851 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6852 = torch.constant.int 5
    %5868 = torch.prims.convert_element_type %5867, %int5_6852 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6853 = torch.constant.int 1920
    %int1280_6854 = torch.constant.int 1280
    %5869 = torch.prim.ListConstruct %int1920_6853, %int1280_6854 : (!torch.int, !torch.int) -> !torch.list<int>
    %5870 = torch.aten.view %5868, %5869 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %5871 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_6855 = torch.constant.int 0
    %int1_6856 = torch.constant.int 1
    %5872 = torch.aten.transpose.int %5871, %int0_6855, %int1_6856 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16>
    %5873 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_6857 = torch.constant.int 6
    %5874 = torch.prims.convert_element_type %5873, %int6_6857 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_6858 = torch.constant.int 6
    %5875 = torch.prims.convert_element_type %5870, %int6_6858 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6859 = torch.constant.int 6
    %5876 = torch.prims.convert_element_type %5872, %int6_6859 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %5877 = torch.aten.mm %5875, %5876 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_6860 = torch.constant.int 1
    %5878 = torch.aten.mul.Scalar %5877, %int1_6860 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_6861 = torch.constant.int 1
    %5879 = torch.aten.mul.Scalar %5874, %int1_6861 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_6862 = torch.constant.int 1
    %5880 = torch.aten.add.Tensor %5878, %5879, %int1_6862 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_6863 = torch.constant.int 5
    %5881 = torch.prims.convert_element_type %5880, %int5_6863 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_6864 = torch.constant.int 2
    %int960_6865 = torch.constant.int 960
    %int10240_6866 = torch.constant.int 10240
    %5882 = torch.prim.ListConstruct %int2_6864, %int960_6865, %int10240_6866 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5883 = torch.aten.view %5881, %5882 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_6867 = torch.constant.int -1
    %int0_6868 = torch.constant.int 0
    %int5120_6869 = torch.constant.int 5120
    %int1_6870 = torch.constant.int 1
    %5884 = torch.aten.slice.Tensor %5883, %int-1_6867, %int0_6868, %int5120_6869, %int1_6870 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_6871 = torch.constant.int -1
    %int5120_6872 = torch.constant.int 5120
    %int10240_6873 = torch.constant.int 10240
    %int1_6874 = torch.constant.int 1
    %5885 = torch.aten.slice.Tensor %5883, %int-1_6871, %int5120_6872, %int10240_6873, %int1_6874 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_6875 = torch.constant.str "none"
    %5886 = torch.aten.gelu %5885, %str_6875 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %5887 = torch.aten.mul.Tensor %5884, %5886 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_6876 = torch.constant.none
    %5888 = torch.aten.clone %5887, %none_6876 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_6877 = torch.constant.int 1920
    %int5120_6878 = torch.constant.int 5120
    %5889 = torch.prim.ListConstruct %int1920_6877, %int5120_6878 : (!torch.int, !torch.int) -> !torch.list<int>
    %5890 = torch.aten.view %5888, %5889 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16>
    %5891 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_6879 = torch.constant.int 0
    %int1_6880 = torch.constant.int 1
    %5892 = torch.aten.transpose.int %5891, %int0_6879, %int1_6880 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16>
    %5893 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.3.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6881 = torch.constant.int 6
    %5894 = torch.prims.convert_element_type %5893, %int6_6881 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6882 = torch.constant.int 6
    %5895 = torch.prims.convert_element_type %5890, %int6_6882 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_6883 = torch.constant.int 6
    %5896 = torch.prims.convert_element_type %5892, %int6_6883 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %5897 = torch.aten.mm %5895, %5896 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6884 = torch.constant.int 1
    %5898 = torch.aten.mul.Scalar %5897, %int1_6884 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6885 = torch.constant.int 1
    %5899 = torch.aten.mul.Scalar %5894, %int1_6885 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6886 = torch.constant.int 1
    %5900 = torch.aten.add.Tensor %5898, %5899, %int1_6886 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6887 = torch.constant.int 5
    %5901 = torch.prims.convert_element_type %5900, %int5_6887 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6888 = torch.constant.int 2
    %int960_6889 = torch.constant.int 960
    %int1280_6890 = torch.constant.int 1280
    %5902 = torch.prim.ListConstruct %int2_6888, %int960_6889, %int1280_6890 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5903 = torch.aten.view %5901, %5902 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_6891 = torch.constant.int 1
    %5904 = torch.aten.add.Tensor %5903, %5857, %int1_6891 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6892 = torch.constant.int 6
    %5905 = torch.prims.convert_element_type %5904, %int6_6892 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6893 = torch.constant.int 2
    %5906 = torch.prim.ListConstruct %int2_6893 : (!torch.int) -> !torch.list<int>
    %int0_6894 = torch.constant.int 0
    %true_6895 = torch.constant.bool true
    %result0_6896, %result1_6897 = torch.aten.var_mean.correction %5905, %5906, %int0_6894, %true_6895 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6898 = torch.constant.float 1.000000e-05
    %int1_6899 = torch.constant.int 1
    %5907 = torch.aten.add.Scalar %result0_6896, %float1.000000e-05_6898, %int1_6899 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5908 = torch.aten.rsqrt %5907 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6900 = torch.constant.int 1
    %5909 = torch.aten.sub.Tensor %5904, %result1_6897, %int1_6900 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5910 = torch.aten.mul.Tensor %5909, %5908 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16>
    %5911 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5912 = torch.aten.mul.Tensor %5910, %5911 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16>
    %5913 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6901 = torch.constant.int 1
    %5914 = torch.aten.add.Tensor %5912, %5913, %int1_6901 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6902 = torch.constant.int 5
    %5915 = torch.prims.convert_element_type %5914, %int5_6902 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16>
    %5916 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6903 = torch.constant.int 0
    %int1_6904 = torch.constant.int 1
    %5917 = torch.aten.transpose.int %5916, %int0_6903, %int1_6904 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6905 = torch.constant.int 1920
    %int1280_6906 = torch.constant.int 1280
    %5918 = torch.prim.ListConstruct %int1920_6905, %int1280_6906 : (!torch.int, !torch.int) -> !torch.list<int>
    %5919 = torch.aten.view %5915, %5918 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5920 = torch.aten.mm %5919, %5917 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6907 = torch.constant.int 2
    %int960_6908 = torch.constant.int 960
    %int1280_6909 = torch.constant.int 1280
    %5921 = torch.prim.ListConstruct %int2_6907, %int960_6908, %int1280_6909 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5922 = torch.aten.view %5920, %5921 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16>
    %5923 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6910 = torch.constant.int 0
    %int1_6911 = torch.constant.int 1
    %5924 = torch.aten.transpose.int %5923, %int0_6910, %int1_6911 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6912 = torch.constant.int 1920
    %int1280_6913 = torch.constant.int 1280
    %5925 = torch.prim.ListConstruct %int1920_6912, %int1280_6913 : (!torch.int, !torch.int) -> !torch.list<int>
    %5926 = torch.aten.view %5915, %5925 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5927 = torch.aten.mm %5926, %5924 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6914 = torch.constant.int 2
    %int960_6915 = torch.constant.int 960
    %int1280_6916 = torch.constant.int 1280
    %5928 = torch.prim.ListConstruct %int2_6914, %int960_6915, %int1280_6916 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5929 = torch.aten.view %5927, %5928 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16>
    %5930 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6917 = torch.constant.int 0
    %int1_6918 = torch.constant.int 1
    %5931 = torch.aten.transpose.int %5930, %int0_6917, %int1_6918 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6919 = torch.constant.int 1920
    %int1280_6920 = torch.constant.int 1280
    %5932 = torch.prim.ListConstruct %int1920_6919, %int1280_6920 : (!torch.int, !torch.int) -> !torch.list<int>
    %5933 = torch.aten.view %5915, %5932 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5934 = torch.aten.mm %5933, %5931 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6921 = torch.constant.int 2
    %int960_6922 = torch.constant.int 960
    %int1280_6923 = torch.constant.int 1280
    %5935 = torch.prim.ListConstruct %int2_6921, %int960_6922, %int1280_6923 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5936 = torch.aten.view %5934, %5935 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_6924 = torch.constant.int 2
    %int-1_6925 = torch.constant.int -1
    %int20_6926 = torch.constant.int 20
    %int64_6927 = torch.constant.int 64
    %5937 = torch.prim.ListConstruct %int2_6924, %int-1_6925, %int20_6926, %int64_6927 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5938 = torch.aten.view %5922, %5937 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6928 = torch.constant.int 1
    %int2_6929 = torch.constant.int 2
    %5939 = torch.aten.transpose.int %5938, %int1_6928, %int2_6929 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6930 = torch.constant.int 2
    %int-1_6931 = torch.constant.int -1
    %int20_6932 = torch.constant.int 20
    %int64_6933 = torch.constant.int 64
    %5940 = torch.prim.ListConstruct %int2_6930, %int-1_6931, %int20_6932, %int64_6933 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5941 = torch.aten.view %5929, %5940 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6934 = torch.constant.int 1
    %int2_6935 = torch.constant.int 2
    %5942 = torch.aten.transpose.int %5941, %int1_6934, %int2_6935 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_6936 = torch.constant.int 2
    %int-1_6937 = torch.constant.int -1
    %int20_6938 = torch.constant.int 20
    %int64_6939 = torch.constant.int 64
    %5943 = torch.prim.ListConstruct %int2_6936, %int-1_6937, %int20_6938, %int64_6939 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5944 = torch.aten.view %5936, %5943 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_6940 = torch.constant.int 1
    %int2_6941 = torch.constant.int 2
    %5945 = torch.aten.transpose.int %5944, %int1_6940, %int2_6941 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_6942 = torch.constant.float 0.000000e+00
    %false_6943 = torch.constant.bool false
    %none_6944 = torch.constant.none
    %none_6945 = torch.constant.none
    %5946:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%5939, %5942, %5945, %float0.000000e00_6942, %false_6943, %none_6944, %none_6945) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_6946 = torch.constant.int 1
    %int2_6947 = torch.constant.int 2
    %5947 = torch.aten.transpose.int %5946#0, %int1_6946, %int2_6947 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_6948 = torch.constant.int 2
    %int-1_6949 = torch.constant.int -1
    %int1280_6950 = torch.constant.int 1280
    %5948 = torch.prim.ListConstruct %int2_6948, %int-1_6949, %int1280_6950 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5949 = torch.aten.view %5947, %5948 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_6951 = torch.constant.int 5
    %5950 = torch.prims.convert_element_type %5949, %int5_6951 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_6952 = torch.constant.int 1920
    %int1280_6953 = torch.constant.int 1280
    %5951 = torch.prim.ListConstruct %int1920_6952, %int1280_6953 : (!torch.int, !torch.int) -> !torch.list<int>
    %5952 = torch.aten.view %5950, %5951 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %5953 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6954 = torch.constant.int 0
    %int1_6955 = torch.constant.int 1
    %5954 = torch.aten.transpose.int %5953, %int0_6954, %int1_6955 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16>
    %5955 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_6956 = torch.constant.int 6
    %5956 = torch.prims.convert_element_type %5955, %int6_6956 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_6957 = torch.constant.int 6
    %5957 = torch.prims.convert_element_type %5952, %int6_6957 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_6958 = torch.constant.int 6
    %5958 = torch.prims.convert_element_type %5954, %int6_6958 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %5959 = torch.aten.mm %5957, %5958 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_6959 = torch.constant.int 1
    %5960 = torch.aten.mul.Scalar %5959, %int1_6959 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_6960 = torch.constant.int 1
    %5961 = torch.aten.mul.Scalar %5956, %int1_6960 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_6961 = torch.constant.int 1
    %5962 = torch.aten.add.Tensor %5960, %5961, %int1_6961 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_6962 = torch.constant.int 5
    %5963 = torch.prims.convert_element_type %5962, %int5_6962 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_6963 = torch.constant.int 2
    %int960_6964 = torch.constant.int 960
    %int1280_6965 = torch.constant.int 1280
    %5964 = torch.prim.ListConstruct %int2_6963, %int960_6964, %int1280_6965 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5965 = torch.aten.view %5963, %5964 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_6966 = torch.constant.none
    %5966 = torch.aten.clone %5965, %none_6966 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_6967 = torch.constant.float 1.000000e+00
    %5967 = torch.aten.div.Scalar %5966, %float1.000000e00_6967 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_6968 = torch.constant.int 1
    %5968 = torch.aten.add.Tensor %5967, %5904, %int1_6968 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_6969 = torch.constant.int 6
    %5969 = torch.prims.convert_element_type %5968, %int6_6969 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_6970 = torch.constant.int 2
    %5970 = torch.prim.ListConstruct %int2_6970 : (!torch.int) -> !torch.list<int>
    %int0_6971 = torch.constant.int 0
    %true_6972 = torch.constant.bool true
    %result0_6973, %result1_6974 = torch.aten.var_mean.correction %5969, %5970, %int0_6971, %true_6972 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_6975 = torch.constant.float 1.000000e-05
    %int1_6976 = torch.constant.int 1
    %5971 = torch.aten.add.Scalar %result0_6973, %float1.000000e-05_6975, %int1_6976 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %5972 = torch.aten.rsqrt %5971 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_6977 = torch.constant.int 1
    %5973 = torch.aten.sub.Tensor %5968, %result1_6974, %int1_6977 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %5974 = torch.aten.mul.Tensor %5973, %5972 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16>
    %5975 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %5976 = torch.aten.mul.Tensor %5974, %5975 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16>
    %5977 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_6978 = torch.constant.int 1
    %5978 = torch.aten.add.Tensor %5976, %5977, %int1_6978 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_6979 = torch.constant.int 5
    %5979 = torch.prims.convert_element_type %5978, %int5_6979 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16>
    %5980 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_6980 = torch.constant.int 0
    %int1_6981 = torch.constant.int 1
    %5981 = torch.aten.transpose.int %5980, %int0_6980, %int1_6981 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_6982 = torch.constant.int 1920
    %int1280_6983 = torch.constant.int 1280
    %5982 = torch.prim.ListConstruct %int1920_6982, %int1280_6983 : (!torch.int, !torch.int) -> !torch.list<int>
    %5983 = torch.aten.view %5979, %5982 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %5984 = torch.aten.mm %5983, %5981 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_6984 = torch.constant.int 2
    %int960_6985 = torch.constant.int 960
    %int1280_6986 = torch.constant.int 1280
    %5985 = torch.prim.ListConstruct %int2_6984, %int960_6985, %int1280_6986 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5986 = torch.aten.view %5984, %5985 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16>
    %5987 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6987 = torch.constant.int 0
    %int1_6988 = torch.constant.int 1
    %5988 = torch.aten.transpose.int %5987, %int0_6987, %int1_6988 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6989 = torch.constant.int 32
    %int2048_6990 = torch.constant.int 2048
    %5989 = torch.prim.ListConstruct %int32_6989, %int2048_6990 : (!torch.int, !torch.int) -> !torch.list<int>
    %5990 = torch.aten.view %arg6, %5989 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5991 = torch.aten.mm %5990, %5988 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6991 = torch.constant.int 2
    %int16_6992 = torch.constant.int 16
    %int1280_6993 = torch.constant.int 1280
    %5992 = torch.prim.ListConstruct %int2_6991, %int16_6992, %int1280_6993 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %5993 = torch.aten.view %5991, %5992 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16>
    %5994 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_6994 = torch.constant.int 0
    %int1_6995 = torch.constant.int 1
    %5995 = torch.aten.transpose.int %5994, %int0_6994, %int1_6995 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_6996 = torch.constant.int 32
    %int2048_6997 = torch.constant.int 2048
    %5996 = torch.prim.ListConstruct %int32_6996, %int2048_6997 : (!torch.int, !torch.int) -> !torch.list<int>
    %5997 = torch.aten.view %arg6, %5996 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %5998 = torch.aten.mm %5997, %5995 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_6998 = torch.constant.int 2
    %int16_6999 = torch.constant.int 16
    %int1280_7000 = torch.constant.int 1280
    %5999 = torch.prim.ListConstruct %int2_6998, %int16_6999, %int1280_7000 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6000 = torch.aten.view %5998, %5999 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_7001 = torch.constant.int 2
    %int-1_7002 = torch.constant.int -1
    %int20_7003 = torch.constant.int 20
    %int64_7004 = torch.constant.int 64
    %6001 = torch.prim.ListConstruct %int2_7001, %int-1_7002, %int20_7003, %int64_7004 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6002 = torch.aten.view %5986, %6001 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7005 = torch.constant.int 1
    %int2_7006 = torch.constant.int 2
    %6003 = torch.aten.transpose.int %6002, %int1_7005, %int2_7006 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7007 = torch.constant.int 2
    %int-1_7008 = torch.constant.int -1
    %int20_7009 = torch.constant.int 20
    %int64_7010 = torch.constant.int 64
    %6004 = torch.prim.ListConstruct %int2_7007, %int-1_7008, %int20_7009, %int64_7010 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6005 = torch.aten.view %5993, %6004 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7011 = torch.constant.int 1
    %int2_7012 = torch.constant.int 2
    %6006 = torch.aten.transpose.int %6005, %int1_7011, %int2_7012 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_7013 = torch.constant.int 2
    %int-1_7014 = torch.constant.int -1
    %int20_7015 = torch.constant.int 20
    %int64_7016 = torch.constant.int 64
    %6007 = torch.prim.ListConstruct %int2_7013, %int-1_7014, %int20_7015, %int64_7016 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6008 = torch.aten.view %6000, %6007 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7017 = torch.constant.int 1
    %int2_7018 = torch.constant.int 2
    %6009 = torch.aten.transpose.int %6008, %int1_7017, %int2_7018 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_7019 = torch.constant.float 0.000000e+00
    %false_7020 = torch.constant.bool false
    %none_7021 = torch.constant.none
    %none_7022 = torch.constant.none
    %6010:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6003, %6006, %6009, %float0.000000e00_7019, %false_7020, %none_7021, %none_7022) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7023 = torch.constant.int 1
    %int2_7024 = torch.constant.int 2
    %6011 = torch.aten.transpose.int %6010#0, %int1_7023, %int2_7024 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7025 = torch.constant.int 2
    %int-1_7026 = torch.constant.int -1
    %int1280_7027 = torch.constant.int 1280
    %6012 = torch.prim.ListConstruct %int2_7025, %int-1_7026, %int1280_7027 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6013 = torch.aten.view %6011, %6012 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7028 = torch.constant.int 5
    %6014 = torch.prims.convert_element_type %6013, %int5_7028 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7029 = torch.constant.int 1920
    %int1280_7030 = torch.constant.int 1280
    %6015 = torch.prim.ListConstruct %int1920_7029, %int1280_7030 : (!torch.int, !torch.int) -> !torch.list<int>
    %6016 = torch.aten.view %6014, %6015 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6017 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7031 = torch.constant.int 0
    %int1_7032 = torch.constant.int 1
    %6018 = torch.aten.transpose.int %6017, %int0_7031, %int1_7032 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16>
    %6019 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7033 = torch.constant.int 6
    %6020 = torch.prims.convert_element_type %6019, %int6_7033 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7034 = torch.constant.int 6
    %6021 = torch.prims.convert_element_type %6016, %int6_7034 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7035 = torch.constant.int 6
    %6022 = torch.prims.convert_element_type %6018, %int6_7035 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6023 = torch.aten.mm %6021, %6022 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7036 = torch.constant.int 1
    %6024 = torch.aten.mul.Scalar %6023, %int1_7036 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7037 = torch.constant.int 1
    %6025 = torch.aten.mul.Scalar %6020, %int1_7037 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7038 = torch.constant.int 1
    %6026 = torch.aten.add.Tensor %6024, %6025, %int1_7038 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7039 = torch.constant.int 5
    %6027 = torch.prims.convert_element_type %6026, %int5_7039 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7040 = torch.constant.int 2
    %int960_7041 = torch.constant.int 960
    %int1280_7042 = torch.constant.int 1280
    %6028 = torch.prim.ListConstruct %int2_7040, %int960_7041, %int1280_7042 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6029 = torch.aten.view %6027, %6028 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7043 = torch.constant.none
    %6030 = torch.aten.clone %6029, %none_7043 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7044 = torch.constant.float 1.000000e+00
    %6031 = torch.aten.div.Scalar %6030, %float1.000000e00_7044 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7045 = torch.constant.int 1
    %6032 = torch.aten.add.Tensor %6031, %5968, %int1_7045 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7046 = torch.constant.int 6
    %6033 = torch.prims.convert_element_type %6032, %int6_7046 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7047 = torch.constant.int 2
    %6034 = torch.prim.ListConstruct %int2_7047 : (!torch.int) -> !torch.list<int>
    %int0_7048 = torch.constant.int 0
    %true_7049 = torch.constant.bool true
    %result0_7050, %result1_7051 = torch.aten.var_mean.correction %6033, %6034, %int0_7048, %true_7049 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7052 = torch.constant.float 1.000000e-05
    %int1_7053 = torch.constant.int 1
    %6035 = torch.aten.add.Scalar %result0_7050, %float1.000000e-05_7052, %int1_7053 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6036 = torch.aten.rsqrt %6035 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7054 = torch.constant.int 1
    %6037 = torch.aten.sub.Tensor %6032, %result1_7051, %int1_7054 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6038 = torch.aten.mul.Tensor %6037, %6036 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16>
    %6039 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6040 = torch.aten.mul.Tensor %6038, %6039 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16>
    %6041 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7055 = torch.constant.int 1
    %6042 = torch.aten.add.Tensor %6040, %6041, %int1_7055 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7056 = torch.constant.int 5
    %6043 = torch.prims.convert_element_type %6042, %int5_7056 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7057 = torch.constant.int 1920
    %int1280_7058 = torch.constant.int 1280
    %6044 = torch.prim.ListConstruct %int1920_7057, %int1280_7058 : (!torch.int, !torch.int) -> !torch.list<int>
    %6045 = torch.aten.view %6043, %6044 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6046 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_7059 = torch.constant.int 0
    %int1_7060 = torch.constant.int 1
    %6047 = torch.aten.transpose.int %6046, %int0_7059, %int1_7060 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16>
    %6048 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_7061 = torch.constant.int 6
    %6049 = torch.prims.convert_element_type %6048, %int6_7061 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_7062 = torch.constant.int 6
    %6050 = torch.prims.convert_element_type %6045, %int6_7062 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7063 = torch.constant.int 6
    %6051 = torch.prims.convert_element_type %6047, %int6_7063 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6052 = torch.aten.mm %6050, %6051 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_7064 = torch.constant.int 1
    %6053 = torch.aten.mul.Scalar %6052, %int1_7064 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_7065 = torch.constant.int 1
    %6054 = torch.aten.mul.Scalar %6049, %int1_7065 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_7066 = torch.constant.int 1
    %6055 = torch.aten.add.Tensor %6053, %6054, %int1_7066 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_7067 = torch.constant.int 5
    %6056 = torch.prims.convert_element_type %6055, %int5_7067 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_7068 = torch.constant.int 2
    %int960_7069 = torch.constant.int 960
    %int10240_7070 = torch.constant.int 10240
    %6057 = torch.prim.ListConstruct %int2_7068, %int960_7069, %int10240_7070 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6058 = torch.aten.view %6056, %6057 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_7071 = torch.constant.int -1
    %int0_7072 = torch.constant.int 0
    %int5120_7073 = torch.constant.int 5120
    %int1_7074 = torch.constant.int 1
    %6059 = torch.aten.slice.Tensor %6058, %int-1_7071, %int0_7072, %int5120_7073, %int1_7074 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_7075 = torch.constant.int -1
    %int5120_7076 = torch.constant.int 5120
    %int10240_7077 = torch.constant.int 10240
    %int1_7078 = torch.constant.int 1
    %6060 = torch.aten.slice.Tensor %6058, %int-1_7075, %int5120_7076, %int10240_7077, %int1_7078 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_7079 = torch.constant.str "none"
    %6061 = torch.aten.gelu %6060, %str_7079 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %6062 = torch.aten.mul.Tensor %6059, %6061 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_7080 = torch.constant.none
    %6063 = torch.aten.clone %6062, %none_7080 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_7081 = torch.constant.int 1920
    %int5120_7082 = torch.constant.int 5120
    %6064 = torch.prim.ListConstruct %int1920_7081, %int5120_7082 : (!torch.int, !torch.int) -> !torch.list<int>
    %6065 = torch.aten.view %6063, %6064 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16>
    %6066 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_7083 = torch.constant.int 0
    %int1_7084 = torch.constant.int 1
    %6067 = torch.aten.transpose.int %6066, %int0_7083, %int1_7084 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16>
    %6068 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.4.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7085 = torch.constant.int 6
    %6069 = torch.prims.convert_element_type %6068, %int6_7085 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7086 = torch.constant.int 6
    %6070 = torch.prims.convert_element_type %6065, %int6_7086 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_7087 = torch.constant.int 6
    %6071 = torch.prims.convert_element_type %6067, %int6_7087 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6072 = torch.aten.mm %6070, %6071 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7088 = torch.constant.int 1
    %6073 = torch.aten.mul.Scalar %6072, %int1_7088 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7089 = torch.constant.int 1
    %6074 = torch.aten.mul.Scalar %6069, %int1_7089 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7090 = torch.constant.int 1
    %6075 = torch.aten.add.Tensor %6073, %6074, %int1_7090 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7091 = torch.constant.int 5
    %6076 = torch.prims.convert_element_type %6075, %int5_7091 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7092 = torch.constant.int 2
    %int960_7093 = torch.constant.int 960
    %int1280_7094 = torch.constant.int 1280
    %6077 = torch.prim.ListConstruct %int2_7092, %int960_7093, %int1280_7094 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6078 = torch.aten.view %6076, %6077 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_7095 = torch.constant.int 1
    %6079 = torch.aten.add.Tensor %6078, %6032, %int1_7095 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7096 = torch.constant.int 6
    %6080 = torch.prims.convert_element_type %6079, %int6_7096 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7097 = torch.constant.int 2
    %6081 = torch.prim.ListConstruct %int2_7097 : (!torch.int) -> !torch.list<int>
    %int0_7098 = torch.constant.int 0
    %true_7099 = torch.constant.bool true
    %result0_7100, %result1_7101 = torch.aten.var_mean.correction %6080, %6081, %int0_7098, %true_7099 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7102 = torch.constant.float 1.000000e-05
    %int1_7103 = torch.constant.int 1
    %6082 = torch.aten.add.Scalar %result0_7100, %float1.000000e-05_7102, %int1_7103 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6083 = torch.aten.rsqrt %6082 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7104 = torch.constant.int 1
    %6084 = torch.aten.sub.Tensor %6079, %result1_7101, %int1_7104 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6085 = torch.aten.mul.Tensor %6084, %6083 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16>
    %6086 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6087 = torch.aten.mul.Tensor %6085, %6086 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16>
    %6088 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7105 = torch.constant.int 1
    %6089 = torch.aten.add.Tensor %6087, %6088, %int1_7105 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7106 = torch.constant.int 5
    %6090 = torch.prims.convert_element_type %6089, %int5_7106 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16>
    %6091 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7107 = torch.constant.int 0
    %int1_7108 = torch.constant.int 1
    %6092 = torch.aten.transpose.int %6091, %int0_7107, %int1_7108 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7109 = torch.constant.int 1920
    %int1280_7110 = torch.constant.int 1280
    %6093 = torch.prim.ListConstruct %int1920_7109, %int1280_7110 : (!torch.int, !torch.int) -> !torch.list<int>
    %6094 = torch.aten.view %6090, %6093 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6095 = torch.aten.mm %6094, %6092 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7111 = torch.constant.int 2
    %int960_7112 = torch.constant.int 960
    %int1280_7113 = torch.constant.int 1280
    %6096 = torch.prim.ListConstruct %int2_7111, %int960_7112, %int1280_7113 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6097 = torch.aten.view %6095, %6096 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16>
    %6098 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7114 = torch.constant.int 0
    %int1_7115 = torch.constant.int 1
    %6099 = torch.aten.transpose.int %6098, %int0_7114, %int1_7115 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7116 = torch.constant.int 1920
    %int1280_7117 = torch.constant.int 1280
    %6100 = torch.prim.ListConstruct %int1920_7116, %int1280_7117 : (!torch.int, !torch.int) -> !torch.list<int>
    %6101 = torch.aten.view %6090, %6100 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6102 = torch.aten.mm %6101, %6099 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7118 = torch.constant.int 2
    %int960_7119 = torch.constant.int 960
    %int1280_7120 = torch.constant.int 1280
    %6103 = torch.prim.ListConstruct %int2_7118, %int960_7119, %int1280_7120 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6104 = torch.aten.view %6102, %6103 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16>
    %6105 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7121 = torch.constant.int 0
    %int1_7122 = torch.constant.int 1
    %6106 = torch.aten.transpose.int %6105, %int0_7121, %int1_7122 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7123 = torch.constant.int 1920
    %int1280_7124 = torch.constant.int 1280
    %6107 = torch.prim.ListConstruct %int1920_7123, %int1280_7124 : (!torch.int, !torch.int) -> !torch.list<int>
    %6108 = torch.aten.view %6090, %6107 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6109 = torch.aten.mm %6108, %6106 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7125 = torch.constant.int 2
    %int960_7126 = torch.constant.int 960
    %int1280_7127 = torch.constant.int 1280
    %6110 = torch.prim.ListConstruct %int2_7125, %int960_7126, %int1280_7127 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6111 = torch.aten.view %6109, %6110 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_7128 = torch.constant.int 2
    %int-1_7129 = torch.constant.int -1
    %int20_7130 = torch.constant.int 20
    %int64_7131 = torch.constant.int 64
    %6112 = torch.prim.ListConstruct %int2_7128, %int-1_7129, %int20_7130, %int64_7131 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6113 = torch.aten.view %6097, %6112 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7132 = torch.constant.int 1
    %int2_7133 = torch.constant.int 2
    %6114 = torch.aten.transpose.int %6113, %int1_7132, %int2_7133 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7134 = torch.constant.int 2
    %int-1_7135 = torch.constant.int -1
    %int20_7136 = torch.constant.int 20
    %int64_7137 = torch.constant.int 64
    %6115 = torch.prim.ListConstruct %int2_7134, %int-1_7135, %int20_7136, %int64_7137 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6116 = torch.aten.view %6104, %6115 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7138 = torch.constant.int 1
    %int2_7139 = torch.constant.int 2
    %6117 = torch.aten.transpose.int %6116, %int1_7138, %int2_7139 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7140 = torch.constant.int 2
    %int-1_7141 = torch.constant.int -1
    %int20_7142 = torch.constant.int 20
    %int64_7143 = torch.constant.int 64
    %6118 = torch.prim.ListConstruct %int2_7140, %int-1_7141, %int20_7142, %int64_7143 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6119 = torch.aten.view %6111, %6118 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7144 = torch.constant.int 1
    %int2_7145 = torch.constant.int 2
    %6120 = torch.aten.transpose.int %6119, %int1_7144, %int2_7145 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_7146 = torch.constant.float 0.000000e+00
    %false_7147 = torch.constant.bool false
    %none_7148 = torch.constant.none
    %none_7149 = torch.constant.none
    %6121:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6114, %6117, %6120, %float0.000000e00_7146, %false_7147, %none_7148, %none_7149) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7150 = torch.constant.int 1
    %int2_7151 = torch.constant.int 2
    %6122 = torch.aten.transpose.int %6121#0, %int1_7150, %int2_7151 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7152 = torch.constant.int 2
    %int-1_7153 = torch.constant.int -1
    %int1280_7154 = torch.constant.int 1280
    %6123 = torch.prim.ListConstruct %int2_7152, %int-1_7153, %int1280_7154 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6124 = torch.aten.view %6122, %6123 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7155 = torch.constant.int 5
    %6125 = torch.prims.convert_element_type %6124, %int5_7155 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7156 = torch.constant.int 1920
    %int1280_7157 = torch.constant.int 1280
    %6126 = torch.prim.ListConstruct %int1920_7156, %int1280_7157 : (!torch.int, !torch.int) -> !torch.list<int>
    %6127 = torch.aten.view %6125, %6126 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6128 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7158 = torch.constant.int 0
    %int1_7159 = torch.constant.int 1
    %6129 = torch.aten.transpose.int %6128, %int0_7158, %int1_7159 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16>
    %6130 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7160 = torch.constant.int 6
    %6131 = torch.prims.convert_element_type %6130, %int6_7160 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7161 = torch.constant.int 6
    %6132 = torch.prims.convert_element_type %6127, %int6_7161 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7162 = torch.constant.int 6
    %6133 = torch.prims.convert_element_type %6129, %int6_7162 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6134 = torch.aten.mm %6132, %6133 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7163 = torch.constant.int 1
    %6135 = torch.aten.mul.Scalar %6134, %int1_7163 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7164 = torch.constant.int 1
    %6136 = torch.aten.mul.Scalar %6131, %int1_7164 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7165 = torch.constant.int 1
    %6137 = torch.aten.add.Tensor %6135, %6136, %int1_7165 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7166 = torch.constant.int 5
    %6138 = torch.prims.convert_element_type %6137, %int5_7166 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7167 = torch.constant.int 2
    %int960_7168 = torch.constant.int 960
    %int1280_7169 = torch.constant.int 1280
    %6139 = torch.prim.ListConstruct %int2_7167, %int960_7168, %int1280_7169 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6140 = torch.aten.view %6138, %6139 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7170 = torch.constant.none
    %6141 = torch.aten.clone %6140, %none_7170 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7171 = torch.constant.float 1.000000e+00
    %6142 = torch.aten.div.Scalar %6141, %float1.000000e00_7171 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7172 = torch.constant.int 1
    %6143 = torch.aten.add.Tensor %6142, %6079, %int1_7172 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7173 = torch.constant.int 6
    %6144 = torch.prims.convert_element_type %6143, %int6_7173 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7174 = torch.constant.int 2
    %6145 = torch.prim.ListConstruct %int2_7174 : (!torch.int) -> !torch.list<int>
    %int0_7175 = torch.constant.int 0
    %true_7176 = torch.constant.bool true
    %result0_7177, %result1_7178 = torch.aten.var_mean.correction %6144, %6145, %int0_7175, %true_7176 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7179 = torch.constant.float 1.000000e-05
    %int1_7180 = torch.constant.int 1
    %6146 = torch.aten.add.Scalar %result0_7177, %float1.000000e-05_7179, %int1_7180 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6147 = torch.aten.rsqrt %6146 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7181 = torch.constant.int 1
    %6148 = torch.aten.sub.Tensor %6143, %result1_7178, %int1_7181 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6149 = torch.aten.mul.Tensor %6148, %6147 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16>
    %6150 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6151 = torch.aten.mul.Tensor %6149, %6150 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16>
    %6152 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7182 = torch.constant.int 1
    %6153 = torch.aten.add.Tensor %6151, %6152, %int1_7182 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7183 = torch.constant.int 5
    %6154 = torch.prims.convert_element_type %6153, %int5_7183 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16>
    %6155 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7184 = torch.constant.int 0
    %int1_7185 = torch.constant.int 1
    %6156 = torch.aten.transpose.int %6155, %int0_7184, %int1_7185 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7186 = torch.constant.int 1920
    %int1280_7187 = torch.constant.int 1280
    %6157 = torch.prim.ListConstruct %int1920_7186, %int1280_7187 : (!torch.int, !torch.int) -> !torch.list<int>
    %6158 = torch.aten.view %6154, %6157 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6159 = torch.aten.mm %6158, %6156 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7188 = torch.constant.int 2
    %int960_7189 = torch.constant.int 960
    %int1280_7190 = torch.constant.int 1280
    %6160 = torch.prim.ListConstruct %int2_7188, %int960_7189, %int1280_7190 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6161 = torch.aten.view %6159, %6160 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16>
    %6162 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7191 = torch.constant.int 0
    %int1_7192 = torch.constant.int 1
    %6163 = torch.aten.transpose.int %6162, %int0_7191, %int1_7192 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7193 = torch.constant.int 32
    %int2048_7194 = torch.constant.int 2048
    %6164 = torch.prim.ListConstruct %int32_7193, %int2048_7194 : (!torch.int, !torch.int) -> !torch.list<int>
    %6165 = torch.aten.view %arg6, %6164 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6166 = torch.aten.mm %6165, %6163 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7195 = torch.constant.int 2
    %int16_7196 = torch.constant.int 16
    %int1280_7197 = torch.constant.int 1280
    %6167 = torch.prim.ListConstruct %int2_7195, %int16_7196, %int1280_7197 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6168 = torch.aten.view %6166, %6167 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16>
    %6169 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7198 = torch.constant.int 0
    %int1_7199 = torch.constant.int 1
    %6170 = torch.aten.transpose.int %6169, %int0_7198, %int1_7199 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7200 = torch.constant.int 32
    %int2048_7201 = torch.constant.int 2048
    %6171 = torch.prim.ListConstruct %int32_7200, %int2048_7201 : (!torch.int, !torch.int) -> !torch.list<int>
    %6172 = torch.aten.view %arg6, %6171 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6173 = torch.aten.mm %6172, %6170 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7202 = torch.constant.int 2
    %int16_7203 = torch.constant.int 16
    %int1280_7204 = torch.constant.int 1280
    %6174 = torch.prim.ListConstruct %int2_7202, %int16_7203, %int1280_7204 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6175 = torch.aten.view %6173, %6174 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_7205 = torch.constant.int 2
    %int-1_7206 = torch.constant.int -1
    %int20_7207 = torch.constant.int 20
    %int64_7208 = torch.constant.int 64
    %6176 = torch.prim.ListConstruct %int2_7205, %int-1_7206, %int20_7207, %int64_7208 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6177 = torch.aten.view %6161, %6176 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7209 = torch.constant.int 1
    %int2_7210 = torch.constant.int 2
    %6178 = torch.aten.transpose.int %6177, %int1_7209, %int2_7210 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7211 = torch.constant.int 2
    %int-1_7212 = torch.constant.int -1
    %int20_7213 = torch.constant.int 20
    %int64_7214 = torch.constant.int 64
    %6179 = torch.prim.ListConstruct %int2_7211, %int-1_7212, %int20_7213, %int64_7214 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6180 = torch.aten.view %6168, %6179 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7215 = torch.constant.int 1
    %int2_7216 = torch.constant.int 2
    %6181 = torch.aten.transpose.int %6180, %int1_7215, %int2_7216 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_7217 = torch.constant.int 2
    %int-1_7218 = torch.constant.int -1
    %int20_7219 = torch.constant.int 20
    %int64_7220 = torch.constant.int 64
    %6182 = torch.prim.ListConstruct %int2_7217, %int-1_7218, %int20_7219, %int64_7220 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6183 = torch.aten.view %6175, %6182 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7221 = torch.constant.int 1
    %int2_7222 = torch.constant.int 2
    %6184 = torch.aten.transpose.int %6183, %int1_7221, %int2_7222 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_7223 = torch.constant.float 0.000000e+00
    %false_7224 = torch.constant.bool false
    %none_7225 = torch.constant.none
    %none_7226 = torch.constant.none
    %6185:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6178, %6181, %6184, %float0.000000e00_7223, %false_7224, %none_7225, %none_7226) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7227 = torch.constant.int 1
    %int2_7228 = torch.constant.int 2
    %6186 = torch.aten.transpose.int %6185#0, %int1_7227, %int2_7228 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7229 = torch.constant.int 2
    %int-1_7230 = torch.constant.int -1
    %int1280_7231 = torch.constant.int 1280
    %6187 = torch.prim.ListConstruct %int2_7229, %int-1_7230, %int1280_7231 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6188 = torch.aten.view %6186, %6187 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7232 = torch.constant.int 5
    %6189 = torch.prims.convert_element_type %6188, %int5_7232 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7233 = torch.constant.int 1920
    %int1280_7234 = torch.constant.int 1280
    %6190 = torch.prim.ListConstruct %int1920_7233, %int1280_7234 : (!torch.int, !torch.int) -> !torch.list<int>
    %6191 = torch.aten.view %6189, %6190 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6192 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7235 = torch.constant.int 0
    %int1_7236 = torch.constant.int 1
    %6193 = torch.aten.transpose.int %6192, %int0_7235, %int1_7236 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16>
    %6194 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7237 = torch.constant.int 6
    %6195 = torch.prims.convert_element_type %6194, %int6_7237 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7238 = torch.constant.int 6
    %6196 = torch.prims.convert_element_type %6191, %int6_7238 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7239 = torch.constant.int 6
    %6197 = torch.prims.convert_element_type %6193, %int6_7239 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6198 = torch.aten.mm %6196, %6197 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7240 = torch.constant.int 1
    %6199 = torch.aten.mul.Scalar %6198, %int1_7240 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7241 = torch.constant.int 1
    %6200 = torch.aten.mul.Scalar %6195, %int1_7241 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7242 = torch.constant.int 1
    %6201 = torch.aten.add.Tensor %6199, %6200, %int1_7242 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7243 = torch.constant.int 5
    %6202 = torch.prims.convert_element_type %6201, %int5_7243 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7244 = torch.constant.int 2
    %int960_7245 = torch.constant.int 960
    %int1280_7246 = torch.constant.int 1280
    %6203 = torch.prim.ListConstruct %int2_7244, %int960_7245, %int1280_7246 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6204 = torch.aten.view %6202, %6203 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7247 = torch.constant.none
    %6205 = torch.aten.clone %6204, %none_7247 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7248 = torch.constant.float 1.000000e+00
    %6206 = torch.aten.div.Scalar %6205, %float1.000000e00_7248 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7249 = torch.constant.int 1
    %6207 = torch.aten.add.Tensor %6206, %6143, %int1_7249 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7250 = torch.constant.int 6
    %6208 = torch.prims.convert_element_type %6207, %int6_7250 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7251 = torch.constant.int 2
    %6209 = torch.prim.ListConstruct %int2_7251 : (!torch.int) -> !torch.list<int>
    %int0_7252 = torch.constant.int 0
    %true_7253 = torch.constant.bool true
    %result0_7254, %result1_7255 = torch.aten.var_mean.correction %6208, %6209, %int0_7252, %true_7253 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7256 = torch.constant.float 1.000000e-05
    %int1_7257 = torch.constant.int 1
    %6210 = torch.aten.add.Scalar %result0_7254, %float1.000000e-05_7256, %int1_7257 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6211 = torch.aten.rsqrt %6210 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7258 = torch.constant.int 1
    %6212 = torch.aten.sub.Tensor %6207, %result1_7255, %int1_7258 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6213 = torch.aten.mul.Tensor %6212, %6211 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16>
    %6214 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6215 = torch.aten.mul.Tensor %6213, %6214 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16>
    %6216 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7259 = torch.constant.int 1
    %6217 = torch.aten.add.Tensor %6215, %6216, %int1_7259 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7260 = torch.constant.int 5
    %6218 = torch.prims.convert_element_type %6217, %int5_7260 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7261 = torch.constant.int 1920
    %int1280_7262 = torch.constant.int 1280
    %6219 = torch.prim.ListConstruct %int1920_7261, %int1280_7262 : (!torch.int, !torch.int) -> !torch.list<int>
    %6220 = torch.aten.view %6218, %6219 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6221 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_7263 = torch.constant.int 0
    %int1_7264 = torch.constant.int 1
    %6222 = torch.aten.transpose.int %6221, %int0_7263, %int1_7264 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16>
    %6223 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_7265 = torch.constant.int 6
    %6224 = torch.prims.convert_element_type %6223, %int6_7265 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_7266 = torch.constant.int 6
    %6225 = torch.prims.convert_element_type %6220, %int6_7266 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7267 = torch.constant.int 6
    %6226 = torch.prims.convert_element_type %6222, %int6_7267 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6227 = torch.aten.mm %6225, %6226 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_7268 = torch.constant.int 1
    %6228 = torch.aten.mul.Scalar %6227, %int1_7268 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_7269 = torch.constant.int 1
    %6229 = torch.aten.mul.Scalar %6224, %int1_7269 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_7270 = torch.constant.int 1
    %6230 = torch.aten.add.Tensor %6228, %6229, %int1_7270 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_7271 = torch.constant.int 5
    %6231 = torch.prims.convert_element_type %6230, %int5_7271 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_7272 = torch.constant.int 2
    %int960_7273 = torch.constant.int 960
    %int10240_7274 = torch.constant.int 10240
    %6232 = torch.prim.ListConstruct %int2_7272, %int960_7273, %int10240_7274 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6233 = torch.aten.view %6231, %6232 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_7275 = torch.constant.int -1
    %int0_7276 = torch.constant.int 0
    %int5120_7277 = torch.constant.int 5120
    %int1_7278 = torch.constant.int 1
    %6234 = torch.aten.slice.Tensor %6233, %int-1_7275, %int0_7276, %int5120_7277, %int1_7278 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_7279 = torch.constant.int -1
    %int5120_7280 = torch.constant.int 5120
    %int10240_7281 = torch.constant.int 10240
    %int1_7282 = torch.constant.int 1
    %6235 = torch.aten.slice.Tensor %6233, %int-1_7279, %int5120_7280, %int10240_7281, %int1_7282 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_7283 = torch.constant.str "none"
    %6236 = torch.aten.gelu %6235, %str_7283 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %6237 = torch.aten.mul.Tensor %6234, %6236 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_7284 = torch.constant.none
    %6238 = torch.aten.clone %6237, %none_7284 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_7285 = torch.constant.int 1920
    %int5120_7286 = torch.constant.int 5120
    %6239 = torch.prim.ListConstruct %int1920_7285, %int5120_7286 : (!torch.int, !torch.int) -> !torch.list<int>
    %6240 = torch.aten.view %6238, %6239 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16>
    %6241 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_7287 = torch.constant.int 0
    %int1_7288 = torch.constant.int 1
    %6242 = torch.aten.transpose.int %6241, %int0_7287, %int1_7288 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16>
    %6243 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.5.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7289 = torch.constant.int 6
    %6244 = torch.prims.convert_element_type %6243, %int6_7289 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7290 = torch.constant.int 6
    %6245 = torch.prims.convert_element_type %6240, %int6_7290 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_7291 = torch.constant.int 6
    %6246 = torch.prims.convert_element_type %6242, %int6_7291 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6247 = torch.aten.mm %6245, %6246 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7292 = torch.constant.int 1
    %6248 = torch.aten.mul.Scalar %6247, %int1_7292 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7293 = torch.constant.int 1
    %6249 = torch.aten.mul.Scalar %6244, %int1_7293 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7294 = torch.constant.int 1
    %6250 = torch.aten.add.Tensor %6248, %6249, %int1_7294 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7295 = torch.constant.int 5
    %6251 = torch.prims.convert_element_type %6250, %int5_7295 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7296 = torch.constant.int 2
    %int960_7297 = torch.constant.int 960
    %int1280_7298 = torch.constant.int 1280
    %6252 = torch.prim.ListConstruct %int2_7296, %int960_7297, %int1280_7298 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6253 = torch.aten.view %6251, %6252 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_7299 = torch.constant.int 1
    %6254 = torch.aten.add.Tensor %6253, %6207, %int1_7299 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7300 = torch.constant.int 6
    %6255 = torch.prims.convert_element_type %6254, %int6_7300 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7301 = torch.constant.int 2
    %6256 = torch.prim.ListConstruct %int2_7301 : (!torch.int) -> !torch.list<int>
    %int0_7302 = torch.constant.int 0
    %true_7303 = torch.constant.bool true
    %result0_7304, %result1_7305 = torch.aten.var_mean.correction %6255, %6256, %int0_7302, %true_7303 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7306 = torch.constant.float 1.000000e-05
    %int1_7307 = torch.constant.int 1
    %6257 = torch.aten.add.Scalar %result0_7304, %float1.000000e-05_7306, %int1_7307 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6258 = torch.aten.rsqrt %6257 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7308 = torch.constant.int 1
    %6259 = torch.aten.sub.Tensor %6254, %result1_7305, %int1_7308 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6260 = torch.aten.mul.Tensor %6259, %6258 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16>
    %6261 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6262 = torch.aten.mul.Tensor %6260, %6261 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16>
    %6263 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7309 = torch.constant.int 1
    %6264 = torch.aten.add.Tensor %6262, %6263, %int1_7309 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7310 = torch.constant.int 5
    %6265 = torch.prims.convert_element_type %6264, %int5_7310 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16>
    %6266 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7311 = torch.constant.int 0
    %int1_7312 = torch.constant.int 1
    %6267 = torch.aten.transpose.int %6266, %int0_7311, %int1_7312 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7313 = torch.constant.int 1920
    %int1280_7314 = torch.constant.int 1280
    %6268 = torch.prim.ListConstruct %int1920_7313, %int1280_7314 : (!torch.int, !torch.int) -> !torch.list<int>
    %6269 = torch.aten.view %6265, %6268 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6270 = torch.aten.mm %6269, %6267 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7315 = torch.constant.int 2
    %int960_7316 = torch.constant.int 960
    %int1280_7317 = torch.constant.int 1280
    %6271 = torch.prim.ListConstruct %int2_7315, %int960_7316, %int1280_7317 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6272 = torch.aten.view %6270, %6271 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16>
    %6273 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7318 = torch.constant.int 0
    %int1_7319 = torch.constant.int 1
    %6274 = torch.aten.transpose.int %6273, %int0_7318, %int1_7319 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7320 = torch.constant.int 1920
    %int1280_7321 = torch.constant.int 1280
    %6275 = torch.prim.ListConstruct %int1920_7320, %int1280_7321 : (!torch.int, !torch.int) -> !torch.list<int>
    %6276 = torch.aten.view %6265, %6275 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6277 = torch.aten.mm %6276, %6274 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7322 = torch.constant.int 2
    %int960_7323 = torch.constant.int 960
    %int1280_7324 = torch.constant.int 1280
    %6278 = torch.prim.ListConstruct %int2_7322, %int960_7323, %int1280_7324 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6279 = torch.aten.view %6277, %6278 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16>
    %6280 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7325 = torch.constant.int 0
    %int1_7326 = torch.constant.int 1
    %6281 = torch.aten.transpose.int %6280, %int0_7325, %int1_7326 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7327 = torch.constant.int 1920
    %int1280_7328 = torch.constant.int 1280
    %6282 = torch.prim.ListConstruct %int1920_7327, %int1280_7328 : (!torch.int, !torch.int) -> !torch.list<int>
    %6283 = torch.aten.view %6265, %6282 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6284 = torch.aten.mm %6283, %6281 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7329 = torch.constant.int 2
    %int960_7330 = torch.constant.int 960
    %int1280_7331 = torch.constant.int 1280
    %6285 = torch.prim.ListConstruct %int2_7329, %int960_7330, %int1280_7331 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6286 = torch.aten.view %6284, %6285 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_7332 = torch.constant.int 2
    %int-1_7333 = torch.constant.int -1
    %int20_7334 = torch.constant.int 20
    %int64_7335 = torch.constant.int 64
    %6287 = torch.prim.ListConstruct %int2_7332, %int-1_7333, %int20_7334, %int64_7335 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6288 = torch.aten.view %6272, %6287 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7336 = torch.constant.int 1
    %int2_7337 = torch.constant.int 2
    %6289 = torch.aten.transpose.int %6288, %int1_7336, %int2_7337 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7338 = torch.constant.int 2
    %int-1_7339 = torch.constant.int -1
    %int20_7340 = torch.constant.int 20
    %int64_7341 = torch.constant.int 64
    %6290 = torch.prim.ListConstruct %int2_7338, %int-1_7339, %int20_7340, %int64_7341 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6291 = torch.aten.view %6279, %6290 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7342 = torch.constant.int 1
    %int2_7343 = torch.constant.int 2
    %6292 = torch.aten.transpose.int %6291, %int1_7342, %int2_7343 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7344 = torch.constant.int 2
    %int-1_7345 = torch.constant.int -1
    %int20_7346 = torch.constant.int 20
    %int64_7347 = torch.constant.int 64
    %6293 = torch.prim.ListConstruct %int2_7344, %int-1_7345, %int20_7346, %int64_7347 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6294 = torch.aten.view %6286, %6293 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7348 = torch.constant.int 1
    %int2_7349 = torch.constant.int 2
    %6295 = torch.aten.transpose.int %6294, %int1_7348, %int2_7349 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_7350 = torch.constant.float 0.000000e+00
    %false_7351 = torch.constant.bool false
    %none_7352 = torch.constant.none
    %none_7353 = torch.constant.none
    %6296:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6289, %6292, %6295, %float0.000000e00_7350, %false_7351, %none_7352, %none_7353) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7354 = torch.constant.int 1
    %int2_7355 = torch.constant.int 2
    %6297 = torch.aten.transpose.int %6296#0, %int1_7354, %int2_7355 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7356 = torch.constant.int 2
    %int-1_7357 = torch.constant.int -1
    %int1280_7358 = torch.constant.int 1280
    %6298 = torch.prim.ListConstruct %int2_7356, %int-1_7357, %int1280_7358 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6299 = torch.aten.view %6297, %6298 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7359 = torch.constant.int 5
    %6300 = torch.prims.convert_element_type %6299, %int5_7359 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7360 = torch.constant.int 1920
    %int1280_7361 = torch.constant.int 1280
    %6301 = torch.prim.ListConstruct %int1920_7360, %int1280_7361 : (!torch.int, !torch.int) -> !torch.list<int>
    %6302 = torch.aten.view %6300, %6301 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6303 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7362 = torch.constant.int 0
    %int1_7363 = torch.constant.int 1
    %6304 = torch.aten.transpose.int %6303, %int0_7362, %int1_7363 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16>
    %6305 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7364 = torch.constant.int 6
    %6306 = torch.prims.convert_element_type %6305, %int6_7364 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7365 = torch.constant.int 6
    %6307 = torch.prims.convert_element_type %6302, %int6_7365 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7366 = torch.constant.int 6
    %6308 = torch.prims.convert_element_type %6304, %int6_7366 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6309 = torch.aten.mm %6307, %6308 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7367 = torch.constant.int 1
    %6310 = torch.aten.mul.Scalar %6309, %int1_7367 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7368 = torch.constant.int 1
    %6311 = torch.aten.mul.Scalar %6306, %int1_7368 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7369 = torch.constant.int 1
    %6312 = torch.aten.add.Tensor %6310, %6311, %int1_7369 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7370 = torch.constant.int 5
    %6313 = torch.prims.convert_element_type %6312, %int5_7370 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7371 = torch.constant.int 2
    %int960_7372 = torch.constant.int 960
    %int1280_7373 = torch.constant.int 1280
    %6314 = torch.prim.ListConstruct %int2_7371, %int960_7372, %int1280_7373 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6315 = torch.aten.view %6313, %6314 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7374 = torch.constant.none
    %6316 = torch.aten.clone %6315, %none_7374 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7375 = torch.constant.float 1.000000e+00
    %6317 = torch.aten.div.Scalar %6316, %float1.000000e00_7375 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7376 = torch.constant.int 1
    %6318 = torch.aten.add.Tensor %6317, %6254, %int1_7376 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7377 = torch.constant.int 6
    %6319 = torch.prims.convert_element_type %6318, %int6_7377 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7378 = torch.constant.int 2
    %6320 = torch.prim.ListConstruct %int2_7378 : (!torch.int) -> !torch.list<int>
    %int0_7379 = torch.constant.int 0
    %true_7380 = torch.constant.bool true
    %result0_7381, %result1_7382 = torch.aten.var_mean.correction %6319, %6320, %int0_7379, %true_7380 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7383 = torch.constant.float 1.000000e-05
    %int1_7384 = torch.constant.int 1
    %6321 = torch.aten.add.Scalar %result0_7381, %float1.000000e-05_7383, %int1_7384 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6322 = torch.aten.rsqrt %6321 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7385 = torch.constant.int 1
    %6323 = torch.aten.sub.Tensor %6318, %result1_7382, %int1_7385 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6324 = torch.aten.mul.Tensor %6323, %6322 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16>
    %6325 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6326 = torch.aten.mul.Tensor %6324, %6325 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16>
    %6327 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7386 = torch.constant.int 1
    %6328 = torch.aten.add.Tensor %6326, %6327, %int1_7386 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7387 = torch.constant.int 5
    %6329 = torch.prims.convert_element_type %6328, %int5_7387 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16>
    %6330 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7388 = torch.constant.int 0
    %int1_7389 = torch.constant.int 1
    %6331 = torch.aten.transpose.int %6330, %int0_7388, %int1_7389 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7390 = torch.constant.int 1920
    %int1280_7391 = torch.constant.int 1280
    %6332 = torch.prim.ListConstruct %int1920_7390, %int1280_7391 : (!torch.int, !torch.int) -> !torch.list<int>
    %6333 = torch.aten.view %6329, %6332 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6334 = torch.aten.mm %6333, %6331 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7392 = torch.constant.int 2
    %int960_7393 = torch.constant.int 960
    %int1280_7394 = torch.constant.int 1280
    %6335 = torch.prim.ListConstruct %int2_7392, %int960_7393, %int1280_7394 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6336 = torch.aten.view %6334, %6335 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16>
    %6337 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7395 = torch.constant.int 0
    %int1_7396 = torch.constant.int 1
    %6338 = torch.aten.transpose.int %6337, %int0_7395, %int1_7396 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7397 = torch.constant.int 32
    %int2048_7398 = torch.constant.int 2048
    %6339 = torch.prim.ListConstruct %int32_7397, %int2048_7398 : (!torch.int, !torch.int) -> !torch.list<int>
    %6340 = torch.aten.view %arg6, %6339 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6341 = torch.aten.mm %6340, %6338 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7399 = torch.constant.int 2
    %int16_7400 = torch.constant.int 16
    %int1280_7401 = torch.constant.int 1280
    %6342 = torch.prim.ListConstruct %int2_7399, %int16_7400, %int1280_7401 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6343 = torch.aten.view %6341, %6342 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16>
    %6344 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7402 = torch.constant.int 0
    %int1_7403 = torch.constant.int 1
    %6345 = torch.aten.transpose.int %6344, %int0_7402, %int1_7403 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7404 = torch.constant.int 32
    %int2048_7405 = torch.constant.int 2048
    %6346 = torch.prim.ListConstruct %int32_7404, %int2048_7405 : (!torch.int, !torch.int) -> !torch.list<int>
    %6347 = torch.aten.view %arg6, %6346 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6348 = torch.aten.mm %6347, %6345 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7406 = torch.constant.int 2
    %int16_7407 = torch.constant.int 16
    %int1280_7408 = torch.constant.int 1280
    %6349 = torch.prim.ListConstruct %int2_7406, %int16_7407, %int1280_7408 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6350 = torch.aten.view %6348, %6349 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_7409 = torch.constant.int 2
    %int-1_7410 = torch.constant.int -1
    %int20_7411 = torch.constant.int 20
    %int64_7412 = torch.constant.int 64
    %6351 = torch.prim.ListConstruct %int2_7409, %int-1_7410, %int20_7411, %int64_7412 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6352 = torch.aten.view %6336, %6351 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7413 = torch.constant.int 1
    %int2_7414 = torch.constant.int 2
    %6353 = torch.aten.transpose.int %6352, %int1_7413, %int2_7414 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7415 = torch.constant.int 2
    %int-1_7416 = torch.constant.int -1
    %int20_7417 = torch.constant.int 20
    %int64_7418 = torch.constant.int 64
    %6354 = torch.prim.ListConstruct %int2_7415, %int-1_7416, %int20_7417, %int64_7418 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6355 = torch.aten.view %6343, %6354 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7419 = torch.constant.int 1
    %int2_7420 = torch.constant.int 2
    %6356 = torch.aten.transpose.int %6355, %int1_7419, %int2_7420 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_7421 = torch.constant.int 2
    %int-1_7422 = torch.constant.int -1
    %int20_7423 = torch.constant.int 20
    %int64_7424 = torch.constant.int 64
    %6357 = torch.prim.ListConstruct %int2_7421, %int-1_7422, %int20_7423, %int64_7424 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6358 = torch.aten.view %6350, %6357 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7425 = torch.constant.int 1
    %int2_7426 = torch.constant.int 2
    %6359 = torch.aten.transpose.int %6358, %int1_7425, %int2_7426 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_7427 = torch.constant.float 0.000000e+00
    %false_7428 = torch.constant.bool false
    %none_7429 = torch.constant.none
    %none_7430 = torch.constant.none
    %6360:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6353, %6356, %6359, %float0.000000e00_7427, %false_7428, %none_7429, %none_7430) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7431 = torch.constant.int 1
    %int2_7432 = torch.constant.int 2
    %6361 = torch.aten.transpose.int %6360#0, %int1_7431, %int2_7432 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7433 = torch.constant.int 2
    %int-1_7434 = torch.constant.int -1
    %int1280_7435 = torch.constant.int 1280
    %6362 = torch.prim.ListConstruct %int2_7433, %int-1_7434, %int1280_7435 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6363 = torch.aten.view %6361, %6362 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7436 = torch.constant.int 5
    %6364 = torch.prims.convert_element_type %6363, %int5_7436 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7437 = torch.constant.int 1920
    %int1280_7438 = torch.constant.int 1280
    %6365 = torch.prim.ListConstruct %int1920_7437, %int1280_7438 : (!torch.int, !torch.int) -> !torch.list<int>
    %6366 = torch.aten.view %6364, %6365 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6367 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7439 = torch.constant.int 0
    %int1_7440 = torch.constant.int 1
    %6368 = torch.aten.transpose.int %6367, %int0_7439, %int1_7440 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16>
    %6369 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7441 = torch.constant.int 6
    %6370 = torch.prims.convert_element_type %6369, %int6_7441 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7442 = torch.constant.int 6
    %6371 = torch.prims.convert_element_type %6366, %int6_7442 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7443 = torch.constant.int 6
    %6372 = torch.prims.convert_element_type %6368, %int6_7443 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6373 = torch.aten.mm %6371, %6372 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7444 = torch.constant.int 1
    %6374 = torch.aten.mul.Scalar %6373, %int1_7444 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7445 = torch.constant.int 1
    %6375 = torch.aten.mul.Scalar %6370, %int1_7445 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7446 = torch.constant.int 1
    %6376 = torch.aten.add.Tensor %6374, %6375, %int1_7446 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7447 = torch.constant.int 5
    %6377 = torch.prims.convert_element_type %6376, %int5_7447 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7448 = torch.constant.int 2
    %int960_7449 = torch.constant.int 960
    %int1280_7450 = torch.constant.int 1280
    %6378 = torch.prim.ListConstruct %int2_7448, %int960_7449, %int1280_7450 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6379 = torch.aten.view %6377, %6378 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7451 = torch.constant.none
    %6380 = torch.aten.clone %6379, %none_7451 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7452 = torch.constant.float 1.000000e+00
    %6381 = torch.aten.div.Scalar %6380, %float1.000000e00_7452 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7453 = torch.constant.int 1
    %6382 = torch.aten.add.Tensor %6381, %6318, %int1_7453 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7454 = torch.constant.int 6
    %6383 = torch.prims.convert_element_type %6382, %int6_7454 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7455 = torch.constant.int 2
    %6384 = torch.prim.ListConstruct %int2_7455 : (!torch.int) -> !torch.list<int>
    %int0_7456 = torch.constant.int 0
    %true_7457 = torch.constant.bool true
    %result0_7458, %result1_7459 = torch.aten.var_mean.correction %6383, %6384, %int0_7456, %true_7457 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7460 = torch.constant.float 1.000000e-05
    %int1_7461 = torch.constant.int 1
    %6385 = torch.aten.add.Scalar %result0_7458, %float1.000000e-05_7460, %int1_7461 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6386 = torch.aten.rsqrt %6385 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7462 = torch.constant.int 1
    %6387 = torch.aten.sub.Tensor %6382, %result1_7459, %int1_7462 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6388 = torch.aten.mul.Tensor %6387, %6386 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16>
    %6389 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6390 = torch.aten.mul.Tensor %6388, %6389 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16>
    %6391 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7463 = torch.constant.int 1
    %6392 = torch.aten.add.Tensor %6390, %6391, %int1_7463 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7464 = torch.constant.int 5
    %6393 = torch.prims.convert_element_type %6392, %int5_7464 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7465 = torch.constant.int 1920
    %int1280_7466 = torch.constant.int 1280
    %6394 = torch.prim.ListConstruct %int1920_7465, %int1280_7466 : (!torch.int, !torch.int) -> !torch.list<int>
    %6395 = torch.aten.view %6393, %6394 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6396 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_7467 = torch.constant.int 0
    %int1_7468 = torch.constant.int 1
    %6397 = torch.aten.transpose.int %6396, %int0_7467, %int1_7468 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16>
    %6398 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_7469 = torch.constant.int 6
    %6399 = torch.prims.convert_element_type %6398, %int6_7469 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_7470 = torch.constant.int 6
    %6400 = torch.prims.convert_element_type %6395, %int6_7470 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7471 = torch.constant.int 6
    %6401 = torch.prims.convert_element_type %6397, %int6_7471 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6402 = torch.aten.mm %6400, %6401 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_7472 = torch.constant.int 1
    %6403 = torch.aten.mul.Scalar %6402, %int1_7472 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_7473 = torch.constant.int 1
    %6404 = torch.aten.mul.Scalar %6399, %int1_7473 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_7474 = torch.constant.int 1
    %6405 = torch.aten.add.Tensor %6403, %6404, %int1_7474 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_7475 = torch.constant.int 5
    %6406 = torch.prims.convert_element_type %6405, %int5_7475 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_7476 = torch.constant.int 2
    %int960_7477 = torch.constant.int 960
    %int10240_7478 = torch.constant.int 10240
    %6407 = torch.prim.ListConstruct %int2_7476, %int960_7477, %int10240_7478 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6408 = torch.aten.view %6406, %6407 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_7479 = torch.constant.int -1
    %int0_7480 = torch.constant.int 0
    %int5120_7481 = torch.constant.int 5120
    %int1_7482 = torch.constant.int 1
    %6409 = torch.aten.slice.Tensor %6408, %int-1_7479, %int0_7480, %int5120_7481, %int1_7482 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_7483 = torch.constant.int -1
    %int5120_7484 = torch.constant.int 5120
    %int10240_7485 = torch.constant.int 10240
    %int1_7486 = torch.constant.int 1
    %6410 = torch.aten.slice.Tensor %6408, %int-1_7483, %int5120_7484, %int10240_7485, %int1_7486 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_7487 = torch.constant.str "none"
    %6411 = torch.aten.gelu %6410, %str_7487 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %6412 = torch.aten.mul.Tensor %6409, %6411 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_7488 = torch.constant.none
    %6413 = torch.aten.clone %6412, %none_7488 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_7489 = torch.constant.int 1920
    %int5120_7490 = torch.constant.int 5120
    %6414 = torch.prim.ListConstruct %int1920_7489, %int5120_7490 : (!torch.int, !torch.int) -> !torch.list<int>
    %6415 = torch.aten.view %6413, %6414 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16>
    %6416 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_7491 = torch.constant.int 0
    %int1_7492 = torch.constant.int 1
    %6417 = torch.aten.transpose.int %6416, %int0_7491, %int1_7492 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16>
    %6418 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.6.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7493 = torch.constant.int 6
    %6419 = torch.prims.convert_element_type %6418, %int6_7493 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7494 = torch.constant.int 6
    %6420 = torch.prims.convert_element_type %6415, %int6_7494 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_7495 = torch.constant.int 6
    %6421 = torch.prims.convert_element_type %6417, %int6_7495 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6422 = torch.aten.mm %6420, %6421 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7496 = torch.constant.int 1
    %6423 = torch.aten.mul.Scalar %6422, %int1_7496 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7497 = torch.constant.int 1
    %6424 = torch.aten.mul.Scalar %6419, %int1_7497 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7498 = torch.constant.int 1
    %6425 = torch.aten.add.Tensor %6423, %6424, %int1_7498 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7499 = torch.constant.int 5
    %6426 = torch.prims.convert_element_type %6425, %int5_7499 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7500 = torch.constant.int 2
    %int960_7501 = torch.constant.int 960
    %int1280_7502 = torch.constant.int 1280
    %6427 = torch.prim.ListConstruct %int2_7500, %int960_7501, %int1280_7502 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6428 = torch.aten.view %6426, %6427 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_7503 = torch.constant.int 1
    %6429 = torch.aten.add.Tensor %6428, %6382, %int1_7503 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7504 = torch.constant.int 6
    %6430 = torch.prims.convert_element_type %6429, %int6_7504 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7505 = torch.constant.int 2
    %6431 = torch.prim.ListConstruct %int2_7505 : (!torch.int) -> !torch.list<int>
    %int0_7506 = torch.constant.int 0
    %true_7507 = torch.constant.bool true
    %result0_7508, %result1_7509 = torch.aten.var_mean.correction %6430, %6431, %int0_7506, %true_7507 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7510 = torch.constant.float 1.000000e-05
    %int1_7511 = torch.constant.int 1
    %6432 = torch.aten.add.Scalar %result0_7508, %float1.000000e-05_7510, %int1_7511 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6433 = torch.aten.rsqrt %6432 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7512 = torch.constant.int 1
    %6434 = torch.aten.sub.Tensor %6429, %result1_7509, %int1_7512 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6435 = torch.aten.mul.Tensor %6434, %6433 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16>
    %6436 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6437 = torch.aten.mul.Tensor %6435, %6436 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16>
    %6438 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7513 = torch.constant.int 1
    %6439 = torch.aten.add.Tensor %6437, %6438, %int1_7513 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7514 = torch.constant.int 5
    %6440 = torch.prims.convert_element_type %6439, %int5_7514 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16>
    %6441 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7515 = torch.constant.int 0
    %int1_7516 = torch.constant.int 1
    %6442 = torch.aten.transpose.int %6441, %int0_7515, %int1_7516 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7517 = torch.constant.int 1920
    %int1280_7518 = torch.constant.int 1280
    %6443 = torch.prim.ListConstruct %int1920_7517, %int1280_7518 : (!torch.int, !torch.int) -> !torch.list<int>
    %6444 = torch.aten.view %6440, %6443 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6445 = torch.aten.mm %6444, %6442 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7519 = torch.constant.int 2
    %int960_7520 = torch.constant.int 960
    %int1280_7521 = torch.constant.int 1280
    %6446 = torch.prim.ListConstruct %int2_7519, %int960_7520, %int1280_7521 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6447 = torch.aten.view %6445, %6446 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16>
    %6448 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7522 = torch.constant.int 0
    %int1_7523 = torch.constant.int 1
    %6449 = torch.aten.transpose.int %6448, %int0_7522, %int1_7523 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7524 = torch.constant.int 1920
    %int1280_7525 = torch.constant.int 1280
    %6450 = torch.prim.ListConstruct %int1920_7524, %int1280_7525 : (!torch.int, !torch.int) -> !torch.list<int>
    %6451 = torch.aten.view %6440, %6450 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6452 = torch.aten.mm %6451, %6449 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7526 = torch.constant.int 2
    %int960_7527 = torch.constant.int 960
    %int1280_7528 = torch.constant.int 1280
    %6453 = torch.prim.ListConstruct %int2_7526, %int960_7527, %int1280_7528 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6454 = torch.aten.view %6452, %6453 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16>
    %6455 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7529 = torch.constant.int 0
    %int1_7530 = torch.constant.int 1
    %6456 = torch.aten.transpose.int %6455, %int0_7529, %int1_7530 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7531 = torch.constant.int 1920
    %int1280_7532 = torch.constant.int 1280
    %6457 = torch.prim.ListConstruct %int1920_7531, %int1280_7532 : (!torch.int, !torch.int) -> !torch.list<int>
    %6458 = torch.aten.view %6440, %6457 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6459 = torch.aten.mm %6458, %6456 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7533 = torch.constant.int 2
    %int960_7534 = torch.constant.int 960
    %int1280_7535 = torch.constant.int 1280
    %6460 = torch.prim.ListConstruct %int2_7533, %int960_7534, %int1280_7535 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6461 = torch.aten.view %6459, %6460 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_7536 = torch.constant.int 2
    %int-1_7537 = torch.constant.int -1
    %int20_7538 = torch.constant.int 20
    %int64_7539 = torch.constant.int 64
    %6462 = torch.prim.ListConstruct %int2_7536, %int-1_7537, %int20_7538, %int64_7539 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6463 = torch.aten.view %6447, %6462 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7540 = torch.constant.int 1
    %int2_7541 = torch.constant.int 2
    %6464 = torch.aten.transpose.int %6463, %int1_7540, %int2_7541 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7542 = torch.constant.int 2
    %int-1_7543 = torch.constant.int -1
    %int20_7544 = torch.constant.int 20
    %int64_7545 = torch.constant.int 64
    %6465 = torch.prim.ListConstruct %int2_7542, %int-1_7543, %int20_7544, %int64_7545 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6466 = torch.aten.view %6454, %6465 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7546 = torch.constant.int 1
    %int2_7547 = torch.constant.int 2
    %6467 = torch.aten.transpose.int %6466, %int1_7546, %int2_7547 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7548 = torch.constant.int 2
    %int-1_7549 = torch.constant.int -1
    %int20_7550 = torch.constant.int 20
    %int64_7551 = torch.constant.int 64
    %6468 = torch.prim.ListConstruct %int2_7548, %int-1_7549, %int20_7550, %int64_7551 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6469 = torch.aten.view %6461, %6468 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7552 = torch.constant.int 1
    %int2_7553 = torch.constant.int 2
    %6470 = torch.aten.transpose.int %6469, %int1_7552, %int2_7553 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_7554 = torch.constant.float 0.000000e+00
    %false_7555 = torch.constant.bool false
    %none_7556 = torch.constant.none
    %none_7557 = torch.constant.none
    %6471:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6464, %6467, %6470, %float0.000000e00_7554, %false_7555, %none_7556, %none_7557) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7558 = torch.constant.int 1
    %int2_7559 = torch.constant.int 2
    %6472 = torch.aten.transpose.int %6471#0, %int1_7558, %int2_7559 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7560 = torch.constant.int 2
    %int-1_7561 = torch.constant.int -1
    %int1280_7562 = torch.constant.int 1280
    %6473 = torch.prim.ListConstruct %int2_7560, %int-1_7561, %int1280_7562 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6474 = torch.aten.view %6472, %6473 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7563 = torch.constant.int 5
    %6475 = torch.prims.convert_element_type %6474, %int5_7563 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7564 = torch.constant.int 1920
    %int1280_7565 = torch.constant.int 1280
    %6476 = torch.prim.ListConstruct %int1920_7564, %int1280_7565 : (!torch.int, !torch.int) -> !torch.list<int>
    %6477 = torch.aten.view %6475, %6476 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6478 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7566 = torch.constant.int 0
    %int1_7567 = torch.constant.int 1
    %6479 = torch.aten.transpose.int %6478, %int0_7566, %int1_7567 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16>
    %6480 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7568 = torch.constant.int 6
    %6481 = torch.prims.convert_element_type %6480, %int6_7568 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7569 = torch.constant.int 6
    %6482 = torch.prims.convert_element_type %6477, %int6_7569 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7570 = torch.constant.int 6
    %6483 = torch.prims.convert_element_type %6479, %int6_7570 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6484 = torch.aten.mm %6482, %6483 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7571 = torch.constant.int 1
    %6485 = torch.aten.mul.Scalar %6484, %int1_7571 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7572 = torch.constant.int 1
    %6486 = torch.aten.mul.Scalar %6481, %int1_7572 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7573 = torch.constant.int 1
    %6487 = torch.aten.add.Tensor %6485, %6486, %int1_7573 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7574 = torch.constant.int 5
    %6488 = torch.prims.convert_element_type %6487, %int5_7574 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7575 = torch.constant.int 2
    %int960_7576 = torch.constant.int 960
    %int1280_7577 = torch.constant.int 1280
    %6489 = torch.prim.ListConstruct %int2_7575, %int960_7576, %int1280_7577 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6490 = torch.aten.view %6488, %6489 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7578 = torch.constant.none
    %6491 = torch.aten.clone %6490, %none_7578 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7579 = torch.constant.float 1.000000e+00
    %6492 = torch.aten.div.Scalar %6491, %float1.000000e00_7579 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7580 = torch.constant.int 1
    %6493 = torch.aten.add.Tensor %6492, %6429, %int1_7580 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7581 = torch.constant.int 6
    %6494 = torch.prims.convert_element_type %6493, %int6_7581 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7582 = torch.constant.int 2
    %6495 = torch.prim.ListConstruct %int2_7582 : (!torch.int) -> !torch.list<int>
    %int0_7583 = torch.constant.int 0
    %true_7584 = torch.constant.bool true
    %result0_7585, %result1_7586 = torch.aten.var_mean.correction %6494, %6495, %int0_7583, %true_7584 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7587 = torch.constant.float 1.000000e-05
    %int1_7588 = torch.constant.int 1
    %6496 = torch.aten.add.Scalar %result0_7585, %float1.000000e-05_7587, %int1_7588 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6497 = torch.aten.rsqrt %6496 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7589 = torch.constant.int 1
    %6498 = torch.aten.sub.Tensor %6493, %result1_7586, %int1_7589 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6499 = torch.aten.mul.Tensor %6498, %6497 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16>
    %6500 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6501 = torch.aten.mul.Tensor %6499, %6500 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16>
    %6502 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7590 = torch.constant.int 1
    %6503 = torch.aten.add.Tensor %6501, %6502, %int1_7590 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7591 = torch.constant.int 5
    %6504 = torch.prims.convert_element_type %6503, %int5_7591 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16>
    %6505 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7592 = torch.constant.int 0
    %int1_7593 = torch.constant.int 1
    %6506 = torch.aten.transpose.int %6505, %int0_7592, %int1_7593 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7594 = torch.constant.int 1920
    %int1280_7595 = torch.constant.int 1280
    %6507 = torch.prim.ListConstruct %int1920_7594, %int1280_7595 : (!torch.int, !torch.int) -> !torch.list<int>
    %6508 = torch.aten.view %6504, %6507 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6509 = torch.aten.mm %6508, %6506 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7596 = torch.constant.int 2
    %int960_7597 = torch.constant.int 960
    %int1280_7598 = torch.constant.int 1280
    %6510 = torch.prim.ListConstruct %int2_7596, %int960_7597, %int1280_7598 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6511 = torch.aten.view %6509, %6510 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16>
    %6512 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7599 = torch.constant.int 0
    %int1_7600 = torch.constant.int 1
    %6513 = torch.aten.transpose.int %6512, %int0_7599, %int1_7600 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7601 = torch.constant.int 32
    %int2048_7602 = torch.constant.int 2048
    %6514 = torch.prim.ListConstruct %int32_7601, %int2048_7602 : (!torch.int, !torch.int) -> !torch.list<int>
    %6515 = torch.aten.view %arg6, %6514 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6516 = torch.aten.mm %6515, %6513 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7603 = torch.constant.int 2
    %int16_7604 = torch.constant.int 16
    %int1280_7605 = torch.constant.int 1280
    %6517 = torch.prim.ListConstruct %int2_7603, %int16_7604, %int1280_7605 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6518 = torch.aten.view %6516, %6517 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16>
    %6519 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7606 = torch.constant.int 0
    %int1_7607 = torch.constant.int 1
    %6520 = torch.aten.transpose.int %6519, %int0_7606, %int1_7607 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7608 = torch.constant.int 32
    %int2048_7609 = torch.constant.int 2048
    %6521 = torch.prim.ListConstruct %int32_7608, %int2048_7609 : (!torch.int, !torch.int) -> !torch.list<int>
    %6522 = torch.aten.view %arg6, %6521 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6523 = torch.aten.mm %6522, %6520 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7610 = torch.constant.int 2
    %int16_7611 = torch.constant.int 16
    %int1280_7612 = torch.constant.int 1280
    %6524 = torch.prim.ListConstruct %int2_7610, %int16_7611, %int1280_7612 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6525 = torch.aten.view %6523, %6524 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_7613 = torch.constant.int 2
    %int-1_7614 = torch.constant.int -1
    %int20_7615 = torch.constant.int 20
    %int64_7616 = torch.constant.int 64
    %6526 = torch.prim.ListConstruct %int2_7613, %int-1_7614, %int20_7615, %int64_7616 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6527 = torch.aten.view %6511, %6526 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7617 = torch.constant.int 1
    %int2_7618 = torch.constant.int 2
    %6528 = torch.aten.transpose.int %6527, %int1_7617, %int2_7618 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7619 = torch.constant.int 2
    %int-1_7620 = torch.constant.int -1
    %int20_7621 = torch.constant.int 20
    %int64_7622 = torch.constant.int 64
    %6529 = torch.prim.ListConstruct %int2_7619, %int-1_7620, %int20_7621, %int64_7622 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6530 = torch.aten.view %6518, %6529 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7623 = torch.constant.int 1
    %int2_7624 = torch.constant.int 2
    %6531 = torch.aten.transpose.int %6530, %int1_7623, %int2_7624 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_7625 = torch.constant.int 2
    %int-1_7626 = torch.constant.int -1
    %int20_7627 = torch.constant.int 20
    %int64_7628 = torch.constant.int 64
    %6532 = torch.prim.ListConstruct %int2_7625, %int-1_7626, %int20_7627, %int64_7628 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6533 = torch.aten.view %6525, %6532 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7629 = torch.constant.int 1
    %int2_7630 = torch.constant.int 2
    %6534 = torch.aten.transpose.int %6533, %int1_7629, %int2_7630 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_7631 = torch.constant.float 0.000000e+00
    %false_7632 = torch.constant.bool false
    %none_7633 = torch.constant.none
    %none_7634 = torch.constant.none
    %6535:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6528, %6531, %6534, %float0.000000e00_7631, %false_7632, %none_7633, %none_7634) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7635 = torch.constant.int 1
    %int2_7636 = torch.constant.int 2
    %6536 = torch.aten.transpose.int %6535#0, %int1_7635, %int2_7636 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7637 = torch.constant.int 2
    %int-1_7638 = torch.constant.int -1
    %int1280_7639 = torch.constant.int 1280
    %6537 = torch.prim.ListConstruct %int2_7637, %int-1_7638, %int1280_7639 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6538 = torch.aten.view %6536, %6537 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7640 = torch.constant.int 5
    %6539 = torch.prims.convert_element_type %6538, %int5_7640 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7641 = torch.constant.int 1920
    %int1280_7642 = torch.constant.int 1280
    %6540 = torch.prim.ListConstruct %int1920_7641, %int1280_7642 : (!torch.int, !torch.int) -> !torch.list<int>
    %6541 = torch.aten.view %6539, %6540 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6542 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7643 = torch.constant.int 0
    %int1_7644 = torch.constant.int 1
    %6543 = torch.aten.transpose.int %6542, %int0_7643, %int1_7644 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16>
    %6544 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7645 = torch.constant.int 6
    %6545 = torch.prims.convert_element_type %6544, %int6_7645 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7646 = torch.constant.int 6
    %6546 = torch.prims.convert_element_type %6541, %int6_7646 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7647 = torch.constant.int 6
    %6547 = torch.prims.convert_element_type %6543, %int6_7647 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6548 = torch.aten.mm %6546, %6547 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7648 = torch.constant.int 1
    %6549 = torch.aten.mul.Scalar %6548, %int1_7648 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7649 = torch.constant.int 1
    %6550 = torch.aten.mul.Scalar %6545, %int1_7649 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7650 = torch.constant.int 1
    %6551 = torch.aten.add.Tensor %6549, %6550, %int1_7650 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7651 = torch.constant.int 5
    %6552 = torch.prims.convert_element_type %6551, %int5_7651 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7652 = torch.constant.int 2
    %int960_7653 = torch.constant.int 960
    %int1280_7654 = torch.constant.int 1280
    %6553 = torch.prim.ListConstruct %int2_7652, %int960_7653, %int1280_7654 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6554 = torch.aten.view %6552, %6553 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7655 = torch.constant.none
    %6555 = torch.aten.clone %6554, %none_7655 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7656 = torch.constant.float 1.000000e+00
    %6556 = torch.aten.div.Scalar %6555, %float1.000000e00_7656 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7657 = torch.constant.int 1
    %6557 = torch.aten.add.Tensor %6556, %6493, %int1_7657 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7658 = torch.constant.int 6
    %6558 = torch.prims.convert_element_type %6557, %int6_7658 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7659 = torch.constant.int 2
    %6559 = torch.prim.ListConstruct %int2_7659 : (!torch.int) -> !torch.list<int>
    %int0_7660 = torch.constant.int 0
    %true_7661 = torch.constant.bool true
    %result0_7662, %result1_7663 = torch.aten.var_mean.correction %6558, %6559, %int0_7660, %true_7661 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7664 = torch.constant.float 1.000000e-05
    %int1_7665 = torch.constant.int 1
    %6560 = torch.aten.add.Scalar %result0_7662, %float1.000000e-05_7664, %int1_7665 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6561 = torch.aten.rsqrt %6560 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7666 = torch.constant.int 1
    %6562 = torch.aten.sub.Tensor %6557, %result1_7663, %int1_7666 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6563 = torch.aten.mul.Tensor %6562, %6561 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16>
    %6564 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6565 = torch.aten.mul.Tensor %6563, %6564 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16>
    %6566 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7667 = torch.constant.int 1
    %6567 = torch.aten.add.Tensor %6565, %6566, %int1_7667 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7668 = torch.constant.int 5
    %6568 = torch.prims.convert_element_type %6567, %int5_7668 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7669 = torch.constant.int 1920
    %int1280_7670 = torch.constant.int 1280
    %6569 = torch.prim.ListConstruct %int1920_7669, %int1280_7670 : (!torch.int, !torch.int) -> !torch.list<int>
    %6570 = torch.aten.view %6568, %6569 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6571 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_7671 = torch.constant.int 0
    %int1_7672 = torch.constant.int 1
    %6572 = torch.aten.transpose.int %6571, %int0_7671, %int1_7672 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16>
    %6573 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_7673 = torch.constant.int 6
    %6574 = torch.prims.convert_element_type %6573, %int6_7673 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_7674 = torch.constant.int 6
    %6575 = torch.prims.convert_element_type %6570, %int6_7674 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7675 = torch.constant.int 6
    %6576 = torch.prims.convert_element_type %6572, %int6_7675 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6577 = torch.aten.mm %6575, %6576 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_7676 = torch.constant.int 1
    %6578 = torch.aten.mul.Scalar %6577, %int1_7676 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_7677 = torch.constant.int 1
    %6579 = torch.aten.mul.Scalar %6574, %int1_7677 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_7678 = torch.constant.int 1
    %6580 = torch.aten.add.Tensor %6578, %6579, %int1_7678 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_7679 = torch.constant.int 5
    %6581 = torch.prims.convert_element_type %6580, %int5_7679 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_7680 = torch.constant.int 2
    %int960_7681 = torch.constant.int 960
    %int10240_7682 = torch.constant.int 10240
    %6582 = torch.prim.ListConstruct %int2_7680, %int960_7681, %int10240_7682 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6583 = torch.aten.view %6581, %6582 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_7683 = torch.constant.int -1
    %int0_7684 = torch.constant.int 0
    %int5120_7685 = torch.constant.int 5120
    %int1_7686 = torch.constant.int 1
    %6584 = torch.aten.slice.Tensor %6583, %int-1_7683, %int0_7684, %int5120_7685, %int1_7686 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_7687 = torch.constant.int -1
    %int5120_7688 = torch.constant.int 5120
    %int10240_7689 = torch.constant.int 10240
    %int1_7690 = torch.constant.int 1
    %6585 = torch.aten.slice.Tensor %6583, %int-1_7687, %int5120_7688, %int10240_7689, %int1_7690 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_7691 = torch.constant.str "none"
    %6586 = torch.aten.gelu %6585, %str_7691 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %6587 = torch.aten.mul.Tensor %6584, %6586 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_7692 = torch.constant.none
    %6588 = torch.aten.clone %6587, %none_7692 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_7693 = torch.constant.int 1920
    %int5120_7694 = torch.constant.int 5120
    %6589 = torch.prim.ListConstruct %int1920_7693, %int5120_7694 : (!torch.int, !torch.int) -> !torch.list<int>
    %6590 = torch.aten.view %6588, %6589 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16>
    %6591 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_7695 = torch.constant.int 0
    %int1_7696 = torch.constant.int 1
    %6592 = torch.aten.transpose.int %6591, %int0_7695, %int1_7696 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16>
    %6593 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.7.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7697 = torch.constant.int 6
    %6594 = torch.prims.convert_element_type %6593, %int6_7697 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7698 = torch.constant.int 6
    %6595 = torch.prims.convert_element_type %6590, %int6_7698 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_7699 = torch.constant.int 6
    %6596 = torch.prims.convert_element_type %6592, %int6_7699 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6597 = torch.aten.mm %6595, %6596 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7700 = torch.constant.int 1
    %6598 = torch.aten.mul.Scalar %6597, %int1_7700 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7701 = torch.constant.int 1
    %6599 = torch.aten.mul.Scalar %6594, %int1_7701 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7702 = torch.constant.int 1
    %6600 = torch.aten.add.Tensor %6598, %6599, %int1_7702 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7703 = torch.constant.int 5
    %6601 = torch.prims.convert_element_type %6600, %int5_7703 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7704 = torch.constant.int 2
    %int960_7705 = torch.constant.int 960
    %int1280_7706 = torch.constant.int 1280
    %6602 = torch.prim.ListConstruct %int2_7704, %int960_7705, %int1280_7706 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6603 = torch.aten.view %6601, %6602 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_7707 = torch.constant.int 1
    %6604 = torch.aten.add.Tensor %6603, %6557, %int1_7707 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7708 = torch.constant.int 6
    %6605 = torch.prims.convert_element_type %6604, %int6_7708 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7709 = torch.constant.int 2
    %6606 = torch.prim.ListConstruct %int2_7709 : (!torch.int) -> !torch.list<int>
    %int0_7710 = torch.constant.int 0
    %true_7711 = torch.constant.bool true
    %result0_7712, %result1_7713 = torch.aten.var_mean.correction %6605, %6606, %int0_7710, %true_7711 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7714 = torch.constant.float 1.000000e-05
    %int1_7715 = torch.constant.int 1
    %6607 = torch.aten.add.Scalar %result0_7712, %float1.000000e-05_7714, %int1_7715 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6608 = torch.aten.rsqrt %6607 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7716 = torch.constant.int 1
    %6609 = torch.aten.sub.Tensor %6604, %result1_7713, %int1_7716 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6610 = torch.aten.mul.Tensor %6609, %6608 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16>
    %6611 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6612 = torch.aten.mul.Tensor %6610, %6611 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16>
    %6613 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7717 = torch.constant.int 1
    %6614 = torch.aten.add.Tensor %6612, %6613, %int1_7717 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7718 = torch.constant.int 5
    %6615 = torch.prims.convert_element_type %6614, %int5_7718 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16>
    %6616 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7719 = torch.constant.int 0
    %int1_7720 = torch.constant.int 1
    %6617 = torch.aten.transpose.int %6616, %int0_7719, %int1_7720 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7721 = torch.constant.int 1920
    %int1280_7722 = torch.constant.int 1280
    %6618 = torch.prim.ListConstruct %int1920_7721, %int1280_7722 : (!torch.int, !torch.int) -> !torch.list<int>
    %6619 = torch.aten.view %6615, %6618 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6620 = torch.aten.mm %6619, %6617 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7723 = torch.constant.int 2
    %int960_7724 = torch.constant.int 960
    %int1280_7725 = torch.constant.int 1280
    %6621 = torch.prim.ListConstruct %int2_7723, %int960_7724, %int1280_7725 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6622 = torch.aten.view %6620, %6621 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16>
    %6623 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7726 = torch.constant.int 0
    %int1_7727 = torch.constant.int 1
    %6624 = torch.aten.transpose.int %6623, %int0_7726, %int1_7727 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7728 = torch.constant.int 1920
    %int1280_7729 = torch.constant.int 1280
    %6625 = torch.prim.ListConstruct %int1920_7728, %int1280_7729 : (!torch.int, !torch.int) -> !torch.list<int>
    %6626 = torch.aten.view %6615, %6625 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6627 = torch.aten.mm %6626, %6624 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7730 = torch.constant.int 2
    %int960_7731 = torch.constant.int 960
    %int1280_7732 = torch.constant.int 1280
    %6628 = torch.prim.ListConstruct %int2_7730, %int960_7731, %int1280_7732 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6629 = torch.aten.view %6627, %6628 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16>
    %6630 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7733 = torch.constant.int 0
    %int1_7734 = torch.constant.int 1
    %6631 = torch.aten.transpose.int %6630, %int0_7733, %int1_7734 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7735 = torch.constant.int 1920
    %int1280_7736 = torch.constant.int 1280
    %6632 = torch.prim.ListConstruct %int1920_7735, %int1280_7736 : (!torch.int, !torch.int) -> !torch.list<int>
    %6633 = torch.aten.view %6615, %6632 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6634 = torch.aten.mm %6633, %6631 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7737 = torch.constant.int 2
    %int960_7738 = torch.constant.int 960
    %int1280_7739 = torch.constant.int 1280
    %6635 = torch.prim.ListConstruct %int2_7737, %int960_7738, %int1280_7739 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6636 = torch.aten.view %6634, %6635 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_7740 = torch.constant.int 2
    %int-1_7741 = torch.constant.int -1
    %int20_7742 = torch.constant.int 20
    %int64_7743 = torch.constant.int 64
    %6637 = torch.prim.ListConstruct %int2_7740, %int-1_7741, %int20_7742, %int64_7743 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6638 = torch.aten.view %6622, %6637 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7744 = torch.constant.int 1
    %int2_7745 = torch.constant.int 2
    %6639 = torch.aten.transpose.int %6638, %int1_7744, %int2_7745 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7746 = torch.constant.int 2
    %int-1_7747 = torch.constant.int -1
    %int20_7748 = torch.constant.int 20
    %int64_7749 = torch.constant.int 64
    %6640 = torch.prim.ListConstruct %int2_7746, %int-1_7747, %int20_7748, %int64_7749 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6641 = torch.aten.view %6629, %6640 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7750 = torch.constant.int 1
    %int2_7751 = torch.constant.int 2
    %6642 = torch.aten.transpose.int %6641, %int1_7750, %int2_7751 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7752 = torch.constant.int 2
    %int-1_7753 = torch.constant.int -1
    %int20_7754 = torch.constant.int 20
    %int64_7755 = torch.constant.int 64
    %6643 = torch.prim.ListConstruct %int2_7752, %int-1_7753, %int20_7754, %int64_7755 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6644 = torch.aten.view %6636, %6643 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7756 = torch.constant.int 1
    %int2_7757 = torch.constant.int 2
    %6645 = torch.aten.transpose.int %6644, %int1_7756, %int2_7757 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_7758 = torch.constant.float 0.000000e+00
    %false_7759 = torch.constant.bool false
    %none_7760 = torch.constant.none
    %none_7761 = torch.constant.none
    %6646:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6639, %6642, %6645, %float0.000000e00_7758, %false_7759, %none_7760, %none_7761) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7762 = torch.constant.int 1
    %int2_7763 = torch.constant.int 2
    %6647 = torch.aten.transpose.int %6646#0, %int1_7762, %int2_7763 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7764 = torch.constant.int 2
    %int-1_7765 = torch.constant.int -1
    %int1280_7766 = torch.constant.int 1280
    %6648 = torch.prim.ListConstruct %int2_7764, %int-1_7765, %int1280_7766 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6649 = torch.aten.view %6647, %6648 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7767 = torch.constant.int 5
    %6650 = torch.prims.convert_element_type %6649, %int5_7767 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7768 = torch.constant.int 1920
    %int1280_7769 = torch.constant.int 1280
    %6651 = torch.prim.ListConstruct %int1920_7768, %int1280_7769 : (!torch.int, !torch.int) -> !torch.list<int>
    %6652 = torch.aten.view %6650, %6651 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6653 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7770 = torch.constant.int 0
    %int1_7771 = torch.constant.int 1
    %6654 = torch.aten.transpose.int %6653, %int0_7770, %int1_7771 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16>
    %6655 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7772 = torch.constant.int 6
    %6656 = torch.prims.convert_element_type %6655, %int6_7772 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7773 = torch.constant.int 6
    %6657 = torch.prims.convert_element_type %6652, %int6_7773 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7774 = torch.constant.int 6
    %6658 = torch.prims.convert_element_type %6654, %int6_7774 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6659 = torch.aten.mm %6657, %6658 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7775 = torch.constant.int 1
    %6660 = torch.aten.mul.Scalar %6659, %int1_7775 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7776 = torch.constant.int 1
    %6661 = torch.aten.mul.Scalar %6656, %int1_7776 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7777 = torch.constant.int 1
    %6662 = torch.aten.add.Tensor %6660, %6661, %int1_7777 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7778 = torch.constant.int 5
    %6663 = torch.prims.convert_element_type %6662, %int5_7778 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7779 = torch.constant.int 2
    %int960_7780 = torch.constant.int 960
    %int1280_7781 = torch.constant.int 1280
    %6664 = torch.prim.ListConstruct %int2_7779, %int960_7780, %int1280_7781 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6665 = torch.aten.view %6663, %6664 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7782 = torch.constant.none
    %6666 = torch.aten.clone %6665, %none_7782 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7783 = torch.constant.float 1.000000e+00
    %6667 = torch.aten.div.Scalar %6666, %float1.000000e00_7783 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7784 = torch.constant.int 1
    %6668 = torch.aten.add.Tensor %6667, %6604, %int1_7784 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7785 = torch.constant.int 6
    %6669 = torch.prims.convert_element_type %6668, %int6_7785 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7786 = torch.constant.int 2
    %6670 = torch.prim.ListConstruct %int2_7786 : (!torch.int) -> !torch.list<int>
    %int0_7787 = torch.constant.int 0
    %true_7788 = torch.constant.bool true
    %result0_7789, %result1_7790 = torch.aten.var_mean.correction %6669, %6670, %int0_7787, %true_7788 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7791 = torch.constant.float 1.000000e-05
    %int1_7792 = torch.constant.int 1
    %6671 = torch.aten.add.Scalar %result0_7789, %float1.000000e-05_7791, %int1_7792 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6672 = torch.aten.rsqrt %6671 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7793 = torch.constant.int 1
    %6673 = torch.aten.sub.Tensor %6668, %result1_7790, %int1_7793 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6674 = torch.aten.mul.Tensor %6673, %6672 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16>
    %6675 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6676 = torch.aten.mul.Tensor %6674, %6675 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16>
    %6677 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7794 = torch.constant.int 1
    %6678 = torch.aten.add.Tensor %6676, %6677, %int1_7794 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7795 = torch.constant.int 5
    %6679 = torch.prims.convert_element_type %6678, %int5_7795 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16>
    %6680 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7796 = torch.constant.int 0
    %int1_7797 = torch.constant.int 1
    %6681 = torch.aten.transpose.int %6680, %int0_7796, %int1_7797 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7798 = torch.constant.int 1920
    %int1280_7799 = torch.constant.int 1280
    %6682 = torch.prim.ListConstruct %int1920_7798, %int1280_7799 : (!torch.int, !torch.int) -> !torch.list<int>
    %6683 = torch.aten.view %6679, %6682 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6684 = torch.aten.mm %6683, %6681 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7800 = torch.constant.int 2
    %int960_7801 = torch.constant.int 960
    %int1280_7802 = torch.constant.int 1280
    %6685 = torch.prim.ListConstruct %int2_7800, %int960_7801, %int1280_7802 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6686 = torch.aten.view %6684, %6685 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16>
    %6687 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7803 = torch.constant.int 0
    %int1_7804 = torch.constant.int 1
    %6688 = torch.aten.transpose.int %6687, %int0_7803, %int1_7804 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7805 = torch.constant.int 32
    %int2048_7806 = torch.constant.int 2048
    %6689 = torch.prim.ListConstruct %int32_7805, %int2048_7806 : (!torch.int, !torch.int) -> !torch.list<int>
    %6690 = torch.aten.view %arg6, %6689 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6691 = torch.aten.mm %6690, %6688 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7807 = torch.constant.int 2
    %int16_7808 = torch.constant.int 16
    %int1280_7809 = torch.constant.int 1280
    %6692 = torch.prim.ListConstruct %int2_7807, %int16_7808, %int1280_7809 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6693 = torch.aten.view %6691, %6692 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16>
    %6694 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_7810 = torch.constant.int 0
    %int1_7811 = torch.constant.int 1
    %6695 = torch.aten.transpose.int %6694, %int0_7810, %int1_7811 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_7812 = torch.constant.int 32
    %int2048_7813 = torch.constant.int 2048
    %6696 = torch.prim.ListConstruct %int32_7812, %int2048_7813 : (!torch.int, !torch.int) -> !torch.list<int>
    %6697 = torch.aten.view %arg6, %6696 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6698 = torch.aten.mm %6697, %6695 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_7814 = torch.constant.int 2
    %int16_7815 = torch.constant.int 16
    %int1280_7816 = torch.constant.int 1280
    %6699 = torch.prim.ListConstruct %int2_7814, %int16_7815, %int1280_7816 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6700 = torch.aten.view %6698, %6699 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_7817 = torch.constant.int 2
    %int-1_7818 = torch.constant.int -1
    %int20_7819 = torch.constant.int 20
    %int64_7820 = torch.constant.int 64
    %6701 = torch.prim.ListConstruct %int2_7817, %int-1_7818, %int20_7819, %int64_7820 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6702 = torch.aten.view %6686, %6701 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7821 = torch.constant.int 1
    %int2_7822 = torch.constant.int 2
    %6703 = torch.aten.transpose.int %6702, %int1_7821, %int2_7822 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7823 = torch.constant.int 2
    %int-1_7824 = torch.constant.int -1
    %int20_7825 = torch.constant.int 20
    %int64_7826 = torch.constant.int 64
    %6704 = torch.prim.ListConstruct %int2_7823, %int-1_7824, %int20_7825, %int64_7826 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6705 = torch.aten.view %6693, %6704 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7827 = torch.constant.int 1
    %int2_7828 = torch.constant.int 2
    %6706 = torch.aten.transpose.int %6705, %int1_7827, %int2_7828 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_7829 = torch.constant.int 2
    %int-1_7830 = torch.constant.int -1
    %int20_7831 = torch.constant.int 20
    %int64_7832 = torch.constant.int 64
    %6707 = torch.prim.ListConstruct %int2_7829, %int-1_7830, %int20_7831, %int64_7832 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6708 = torch.aten.view %6700, %6707 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_7833 = torch.constant.int 1
    %int2_7834 = torch.constant.int 2
    %6709 = torch.aten.transpose.int %6708, %int1_7833, %int2_7834 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_7835 = torch.constant.float 0.000000e+00
    %false_7836 = torch.constant.bool false
    %none_7837 = torch.constant.none
    %none_7838 = torch.constant.none
    %6710:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6703, %6706, %6709, %float0.000000e00_7835, %false_7836, %none_7837, %none_7838) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7839 = torch.constant.int 1
    %int2_7840 = torch.constant.int 2
    %6711 = torch.aten.transpose.int %6710#0, %int1_7839, %int2_7840 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7841 = torch.constant.int 2
    %int-1_7842 = torch.constant.int -1
    %int1280_7843 = torch.constant.int 1280
    %6712 = torch.prim.ListConstruct %int2_7841, %int-1_7842, %int1280_7843 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6713 = torch.aten.view %6711, %6712 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7844 = torch.constant.int 5
    %6714 = torch.prims.convert_element_type %6713, %int5_7844 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7845 = torch.constant.int 1920
    %int1280_7846 = torch.constant.int 1280
    %6715 = torch.prim.ListConstruct %int1920_7845, %int1280_7846 : (!torch.int, !torch.int) -> !torch.list<int>
    %6716 = torch.aten.view %6714, %6715 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6717 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7847 = torch.constant.int 0
    %int1_7848 = torch.constant.int 1
    %6718 = torch.aten.transpose.int %6717, %int0_7847, %int1_7848 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16>
    %6719 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7849 = torch.constant.int 6
    %6720 = torch.prims.convert_element_type %6719, %int6_7849 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7850 = torch.constant.int 6
    %6721 = torch.prims.convert_element_type %6716, %int6_7850 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7851 = torch.constant.int 6
    %6722 = torch.prims.convert_element_type %6718, %int6_7851 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6723 = torch.aten.mm %6721, %6722 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7852 = torch.constant.int 1
    %6724 = torch.aten.mul.Scalar %6723, %int1_7852 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7853 = torch.constant.int 1
    %6725 = torch.aten.mul.Scalar %6720, %int1_7853 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7854 = torch.constant.int 1
    %6726 = torch.aten.add.Tensor %6724, %6725, %int1_7854 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7855 = torch.constant.int 5
    %6727 = torch.prims.convert_element_type %6726, %int5_7855 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7856 = torch.constant.int 2
    %int960_7857 = torch.constant.int 960
    %int1280_7858 = torch.constant.int 1280
    %6728 = torch.prim.ListConstruct %int2_7856, %int960_7857, %int1280_7858 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6729 = torch.aten.view %6727, %6728 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7859 = torch.constant.none
    %6730 = torch.aten.clone %6729, %none_7859 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7860 = torch.constant.float 1.000000e+00
    %6731 = torch.aten.div.Scalar %6730, %float1.000000e00_7860 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7861 = torch.constant.int 1
    %6732 = torch.aten.add.Tensor %6731, %6668, %int1_7861 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7862 = torch.constant.int 6
    %6733 = torch.prims.convert_element_type %6732, %int6_7862 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7863 = torch.constant.int 2
    %6734 = torch.prim.ListConstruct %int2_7863 : (!torch.int) -> !torch.list<int>
    %int0_7864 = torch.constant.int 0
    %true_7865 = torch.constant.bool true
    %result0_7866, %result1_7867 = torch.aten.var_mean.correction %6733, %6734, %int0_7864, %true_7865 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7868 = torch.constant.float 1.000000e-05
    %int1_7869 = torch.constant.int 1
    %6735 = torch.aten.add.Scalar %result0_7866, %float1.000000e-05_7868, %int1_7869 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6736 = torch.aten.rsqrt %6735 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7870 = torch.constant.int 1
    %6737 = torch.aten.sub.Tensor %6732, %result1_7867, %int1_7870 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6738 = torch.aten.mul.Tensor %6737, %6736 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16>
    %6739 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6740 = torch.aten.mul.Tensor %6738, %6739 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16>
    %6741 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7871 = torch.constant.int 1
    %6742 = torch.aten.add.Tensor %6740, %6741, %int1_7871 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7872 = torch.constant.int 5
    %6743 = torch.prims.convert_element_type %6742, %int5_7872 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7873 = torch.constant.int 1920
    %int1280_7874 = torch.constant.int 1280
    %6744 = torch.prim.ListConstruct %int1920_7873, %int1280_7874 : (!torch.int, !torch.int) -> !torch.list<int>
    %6745 = torch.aten.view %6743, %6744 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6746 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_7875 = torch.constant.int 0
    %int1_7876 = torch.constant.int 1
    %6747 = torch.aten.transpose.int %6746, %int0_7875, %int1_7876 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16>
    %6748 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_7877 = torch.constant.int 6
    %6749 = torch.prims.convert_element_type %6748, %int6_7877 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_7878 = torch.constant.int 6
    %6750 = torch.prims.convert_element_type %6745, %int6_7878 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7879 = torch.constant.int 6
    %6751 = torch.prims.convert_element_type %6747, %int6_7879 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6752 = torch.aten.mm %6750, %6751 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_7880 = torch.constant.int 1
    %6753 = torch.aten.mul.Scalar %6752, %int1_7880 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_7881 = torch.constant.int 1
    %6754 = torch.aten.mul.Scalar %6749, %int1_7881 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_7882 = torch.constant.int 1
    %6755 = torch.aten.add.Tensor %6753, %6754, %int1_7882 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_7883 = torch.constant.int 5
    %6756 = torch.prims.convert_element_type %6755, %int5_7883 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_7884 = torch.constant.int 2
    %int960_7885 = torch.constant.int 960
    %int10240_7886 = torch.constant.int 10240
    %6757 = torch.prim.ListConstruct %int2_7884, %int960_7885, %int10240_7886 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6758 = torch.aten.view %6756, %6757 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_7887 = torch.constant.int -1
    %int0_7888 = torch.constant.int 0
    %int5120_7889 = torch.constant.int 5120
    %int1_7890 = torch.constant.int 1
    %6759 = torch.aten.slice.Tensor %6758, %int-1_7887, %int0_7888, %int5120_7889, %int1_7890 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_7891 = torch.constant.int -1
    %int5120_7892 = torch.constant.int 5120
    %int10240_7893 = torch.constant.int 10240
    %int1_7894 = torch.constant.int 1
    %6760 = torch.aten.slice.Tensor %6758, %int-1_7891, %int5120_7892, %int10240_7893, %int1_7894 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_7895 = torch.constant.str "none"
    %6761 = torch.aten.gelu %6760, %str_7895 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %6762 = torch.aten.mul.Tensor %6759, %6761 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_7896 = torch.constant.none
    %6763 = torch.aten.clone %6762, %none_7896 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_7897 = torch.constant.int 1920
    %int5120_7898 = torch.constant.int 5120
    %6764 = torch.prim.ListConstruct %int1920_7897, %int5120_7898 : (!torch.int, !torch.int) -> !torch.list<int>
    %6765 = torch.aten.view %6763, %6764 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16>
    %6766 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_7899 = torch.constant.int 0
    %int1_7900 = torch.constant.int 1
    %6767 = torch.aten.transpose.int %6766, %int0_7899, %int1_7900 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16>
    %6768 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.8.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7901 = torch.constant.int 6
    %6769 = torch.prims.convert_element_type %6768, %int6_7901 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7902 = torch.constant.int 6
    %6770 = torch.prims.convert_element_type %6765, %int6_7902 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_7903 = torch.constant.int 6
    %6771 = torch.prims.convert_element_type %6767, %int6_7903 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6772 = torch.aten.mm %6770, %6771 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7904 = torch.constant.int 1
    %6773 = torch.aten.mul.Scalar %6772, %int1_7904 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7905 = torch.constant.int 1
    %6774 = torch.aten.mul.Scalar %6769, %int1_7905 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7906 = torch.constant.int 1
    %6775 = torch.aten.add.Tensor %6773, %6774, %int1_7906 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7907 = torch.constant.int 5
    %6776 = torch.prims.convert_element_type %6775, %int5_7907 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7908 = torch.constant.int 2
    %int960_7909 = torch.constant.int 960
    %int1280_7910 = torch.constant.int 1280
    %6777 = torch.prim.ListConstruct %int2_7908, %int960_7909, %int1280_7910 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6778 = torch.aten.view %6776, %6777 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_7911 = torch.constant.int 1
    %6779 = torch.aten.add.Tensor %6778, %6732, %int1_7911 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7912 = torch.constant.int 6
    %6780 = torch.prims.convert_element_type %6779, %int6_7912 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7913 = torch.constant.int 2
    %6781 = torch.prim.ListConstruct %int2_7913 : (!torch.int) -> !torch.list<int>
    %int0_7914 = torch.constant.int 0
    %true_7915 = torch.constant.bool true
    %result0_7916, %result1_7917 = torch.aten.var_mean.correction %6780, %6781, %int0_7914, %true_7915 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7918 = torch.constant.float 1.000000e-05
    %int1_7919 = torch.constant.int 1
    %6782 = torch.aten.add.Scalar %result0_7916, %float1.000000e-05_7918, %int1_7919 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6783 = torch.aten.rsqrt %6782 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7920 = torch.constant.int 1
    %6784 = torch.aten.sub.Tensor %6779, %result1_7917, %int1_7920 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6785 = torch.aten.mul.Tensor %6784, %6783 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16>
    %6786 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6787 = torch.aten.mul.Tensor %6785, %6786 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16>
    %6788 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7921 = torch.constant.int 1
    %6789 = torch.aten.add.Tensor %6787, %6788, %int1_7921 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7922 = torch.constant.int 5
    %6790 = torch.prims.convert_element_type %6789, %int5_7922 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16>
    %6791 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7923 = torch.constant.int 0
    %int1_7924 = torch.constant.int 1
    %6792 = torch.aten.transpose.int %6791, %int0_7923, %int1_7924 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7925 = torch.constant.int 1920
    %int1280_7926 = torch.constant.int 1280
    %6793 = torch.prim.ListConstruct %int1920_7925, %int1280_7926 : (!torch.int, !torch.int) -> !torch.list<int>
    %6794 = torch.aten.view %6790, %6793 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6795 = torch.aten.mm %6794, %6792 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7927 = torch.constant.int 2
    %int960_7928 = torch.constant.int 960
    %int1280_7929 = torch.constant.int 1280
    %6796 = torch.prim.ListConstruct %int2_7927, %int960_7928, %int1280_7929 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6797 = torch.aten.view %6795, %6796 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16>
    %6798 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_k.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7930 = torch.constant.int 0
    %int1_7931 = torch.constant.int 1
    %6799 = torch.aten.transpose.int %6798, %int0_7930, %int1_7931 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7932 = torch.constant.int 1920
    %int1280_7933 = torch.constant.int 1280
    %6800 = torch.prim.ListConstruct %int1920_7932, %int1280_7933 : (!torch.int, !torch.int) -> !torch.list<int>
    %6801 = torch.aten.view %6790, %6800 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6802 = torch.aten.mm %6801, %6799 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7934 = torch.constant.int 2
    %int960_7935 = torch.constant.int 960
    %int1280_7936 = torch.constant.int 1280
    %6803 = torch.prim.ListConstruct %int2_7934, %int960_7935, %int1280_7936 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6804 = torch.aten.view %6802, %6803 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16>
    %6805 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_v.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7937 = torch.constant.int 0
    %int1_7938 = torch.constant.int 1
    %6806 = torch.aten.transpose.int %6805, %int0_7937, %int1_7938 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_7939 = torch.constant.int 1920
    %int1280_7940 = torch.constant.int 1280
    %6807 = torch.prim.ListConstruct %int1920_7939, %int1280_7940 : (!torch.int, !torch.int) -> !torch.list<int>
    %6808 = torch.aten.view %6790, %6807 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6809 = torch.aten.mm %6808, %6806 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_7941 = torch.constant.int 2
    %int960_7942 = torch.constant.int 960
    %int1280_7943 = torch.constant.int 1280
    %6810 = torch.prim.ListConstruct %int2_7941, %int960_7942, %int1280_7943 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6811 = torch.aten.view %6809, %6810 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_7944 = torch.constant.int 2
    %int-1_7945 = torch.constant.int -1
    %int20_7946 = torch.constant.int 20
    %int64_7947 = torch.constant.int 64
    %6812 = torch.prim.ListConstruct %int2_7944, %int-1_7945, %int20_7946, %int64_7947 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6813 = torch.aten.view %6797, %6812 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7948 = torch.constant.int 1
    %int2_7949 = torch.constant.int 2
    %6814 = torch.aten.transpose.int %6813, %int1_7948, %int2_7949 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7950 = torch.constant.int 2
    %int-1_7951 = torch.constant.int -1
    %int20_7952 = torch.constant.int 20
    %int64_7953 = torch.constant.int 64
    %6815 = torch.prim.ListConstruct %int2_7950, %int-1_7951, %int20_7952, %int64_7953 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6816 = torch.aten.view %6804, %6815 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7954 = torch.constant.int 1
    %int2_7955 = torch.constant.int 2
    %6817 = torch.aten.transpose.int %6816, %int1_7954, %int2_7955 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_7956 = torch.constant.int 2
    %int-1_7957 = torch.constant.int -1
    %int20_7958 = torch.constant.int 20
    %int64_7959 = torch.constant.int 64
    %6818 = torch.prim.ListConstruct %int2_7956, %int-1_7957, %int20_7958, %int64_7959 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6819 = torch.aten.view %6811, %6818 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_7960 = torch.constant.int 1
    %int2_7961 = torch.constant.int 2
    %6820 = torch.aten.transpose.int %6819, %int1_7960, %int2_7961 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %float0.000000e00_7962 = torch.constant.float 0.000000e+00
    %false_7963 = torch.constant.bool false
    %none_7964 = torch.constant.none
    %none_7965 = torch.constant.none
    %6821:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6814, %6817, %6820, %float0.000000e00_7962, %false_7963, %none_7964, %none_7965) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_7966 = torch.constant.int 1
    %int2_7967 = torch.constant.int 2
    %6822 = torch.aten.transpose.int %6821#0, %int1_7966, %int2_7967 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_7968 = torch.constant.int 2
    %int-1_7969 = torch.constant.int -1
    %int1280_7970 = torch.constant.int 1280
    %6823 = torch.prim.ListConstruct %int2_7968, %int-1_7969, %int1280_7970 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6824 = torch.aten.view %6822, %6823 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_7971 = torch.constant.int 5
    %6825 = torch.prims.convert_element_type %6824, %int5_7971 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_7972 = torch.constant.int 1920
    %int1280_7973 = torch.constant.int 1280
    %6826 = torch.prim.ListConstruct %int1920_7972, %int1280_7973 : (!torch.int, !torch.int) -> !torch.list<int>
    %6827 = torch.aten.view %6825, %6826 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16>
    %6828 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_7974 = torch.constant.int 0
    %int1_7975 = torch.constant.int 1
    %6829 = torch.aten.transpose.int %6828, %int0_7974, %int1_7975 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16>
    %6830 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn1.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_7976 = torch.constant.int 6
    %6831 = torch.prims.convert_element_type %6830, %int6_7976 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_7977 = torch.constant.int 6
    %6832 = torch.prims.convert_element_type %6827, %int6_7977 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_7978 = torch.constant.int 6
    %6833 = torch.prims.convert_element_type %6829, %int6_7978 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6834 = torch.aten.mm %6832, %6833 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_7979 = torch.constant.int 1
    %6835 = torch.aten.mul.Scalar %6834, %int1_7979 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_7980 = torch.constant.int 1
    %6836 = torch.aten.mul.Scalar %6831, %int1_7980 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_7981 = torch.constant.int 1
    %6837 = torch.aten.add.Tensor %6835, %6836, %int1_7981 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_7982 = torch.constant.int 5
    %6838 = torch.prims.convert_element_type %6837, %int5_7982 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_7983 = torch.constant.int 2
    %int960_7984 = torch.constant.int 960
    %int1280_7985 = torch.constant.int 1280
    %6839 = torch.prim.ListConstruct %int2_7983, %int960_7984, %int1280_7985 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6840 = torch.aten.view %6838, %6839 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_7986 = torch.constant.none
    %6841 = torch.aten.clone %6840, %none_7986 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_7987 = torch.constant.float 1.000000e+00
    %6842 = torch.aten.div.Scalar %6841, %float1.000000e00_7987 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_7988 = torch.constant.int 1
    %6843 = torch.aten.add.Tensor %6842, %6779, %int1_7988 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_7989 = torch.constant.int 6
    %6844 = torch.prims.convert_element_type %6843, %int6_7989 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_7990 = torch.constant.int 2
    %6845 = torch.prim.ListConstruct %int2_7990 : (!torch.int) -> !torch.list<int>
    %int0_7991 = torch.constant.int 0
    %true_7992 = torch.constant.bool true
    %result0_7993, %result1_7994 = torch.aten.var_mean.correction %6844, %6845, %int0_7991, %true_7992 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_7995 = torch.constant.float 1.000000e-05
    %int1_7996 = torch.constant.int 1
    %6846 = torch.aten.add.Scalar %result0_7993, %float1.000000e-05_7995, %int1_7996 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6847 = torch.aten.rsqrt %6846 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_7997 = torch.constant.int 1
    %6848 = torch.aten.sub.Tensor %6843, %result1_7994, %int1_7997 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6849 = torch.aten.mul.Tensor %6848, %6847 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16>
    %6850 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6851 = torch.aten.mul.Tensor %6849, %6850 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16>
    %6852 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_7998 = torch.constant.int 1
    %6853 = torch.aten.add.Tensor %6851, %6852, %int1_7998 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_7999 = torch.constant.int 5
    %6854 = torch.prims.convert_element_type %6853, %int5_7999 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16>
    %6855 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_q.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_8000 = torch.constant.int 0
    %int1_8001 = torch.constant.int 1
    %6856 = torch.aten.transpose.int %6855, %int0_8000, %int1_8001 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %int1920_8002 = torch.constant.int 1920
    %int1280_8003 = torch.constant.int 1280
    %6857 = torch.prim.ListConstruct %int1920_8002, %int1280_8003 : (!torch.int, !torch.int) -> !torch.list<int>
    %6858 = torch.aten.view %6854, %6857 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %6859 = torch.aten.mm %6858, %6856 : !torch.vtensor<[1920,1280],f16>, !torch.vtensor<[1280,1280],f16> -> !torch.vtensor<[1920,1280],f16>
    %int2_8004 = torch.constant.int 2
    %int960_8005 = torch.constant.int 960
    %int1280_8006 = torch.constant.int 1280
    %6860 = torch.prim.ListConstruct %int2_8004, %int960_8005, %int1280_8006 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6861 = torch.aten.view %6859, %6860 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16>
    %6862 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_k.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_8007 = torch.constant.int 0
    %int1_8008 = torch.constant.int 1
    %6863 = torch.aten.transpose.int %6862, %int0_8007, %int1_8008 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_8009 = torch.constant.int 32
    %int2048_8010 = torch.constant.int 2048
    %6864 = torch.prim.ListConstruct %int32_8009, %int2048_8010 : (!torch.int, !torch.int) -> !torch.list<int>
    %6865 = torch.aten.view %arg6, %6864 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6866 = torch.aten.mm %6865, %6863 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_8011 = torch.constant.int 2
    %int16_8012 = torch.constant.int 16
    %int1280_8013 = torch.constant.int 1280
    %6867 = torch.prim.ListConstruct %int2_8011, %int16_8012, %int1280_8013 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6868 = torch.aten.view %6866, %6867 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16>
    %6869 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_v.weight : tensor<1280x2048xf16> -> !torch.vtensor<[1280,2048],f16>
    %int0_8014 = torch.constant.int 0
    %int1_8015 = torch.constant.int 1
    %6870 = torch.aten.transpose.int %6869, %int0_8014, %int1_8015 : !torch.vtensor<[1280,2048],f16>, !torch.int, !torch.int -> !torch.vtensor<[2048,1280],f16>
    %int32_8016 = torch.constant.int 32
    %int2048_8017 = torch.constant.int 2048
    %6871 = torch.prim.ListConstruct %int32_8016, %int2048_8017 : (!torch.int, !torch.int) -> !torch.list<int>
    %6872 = torch.aten.view %arg6, %6871 : !torch.vtensor<[2,16,2048],f16>, !torch.list<int> -> !torch.vtensor<[32,2048],f16>
    %6873 = torch.aten.mm %6872, %6870 : !torch.vtensor<[32,2048],f16>, !torch.vtensor<[2048,1280],f16> -> !torch.vtensor<[32,1280],f16>
    %int2_8018 = torch.constant.int 2
    %int16_8019 = torch.constant.int 16
    %int1280_8020 = torch.constant.int 1280
    %6874 = torch.prim.ListConstruct %int2_8018, %int16_8019, %int1280_8020 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6875 = torch.aten.view %6873, %6874 : !torch.vtensor<[32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,1280],f16>
    %int2_8021 = torch.constant.int 2
    %int-1_8022 = torch.constant.int -1
    %int20_8023 = torch.constant.int 20
    %int64_8024 = torch.constant.int 64
    %6876 = torch.prim.ListConstruct %int2_8021, %int-1_8022, %int20_8023, %int64_8024 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6877 = torch.aten.view %6861, %6876 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,20,64],f16>
    %int1_8025 = torch.constant.int 1
    %int2_8026 = torch.constant.int 2
    %6878 = torch.aten.transpose.int %6877, %int1_8025, %int2_8026 : !torch.vtensor<[2,960,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,960,64],f16>
    %int2_8027 = torch.constant.int 2
    %int-1_8028 = torch.constant.int -1
    %int20_8029 = torch.constant.int 20
    %int64_8030 = torch.constant.int 64
    %6879 = torch.prim.ListConstruct %int2_8027, %int-1_8028, %int20_8029, %int64_8030 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6880 = torch.aten.view %6868, %6879 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_8031 = torch.constant.int 1
    %int2_8032 = torch.constant.int 2
    %6881 = torch.aten.transpose.int %6880, %int1_8031, %int2_8032 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %int2_8033 = torch.constant.int 2
    %int-1_8034 = torch.constant.int -1
    %int20_8035 = torch.constant.int 20
    %int64_8036 = torch.constant.int 64
    %6882 = torch.prim.ListConstruct %int2_8033, %int-1_8034, %int20_8035, %int64_8036 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6883 = torch.aten.view %6875, %6882 : !torch.vtensor<[2,16,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,16,20,64],f16>
    %int1_8037 = torch.constant.int 1
    %int2_8038 = torch.constant.int 2
    %6884 = torch.aten.transpose.int %6883, %int1_8037, %int2_8038 : !torch.vtensor<[2,16,20,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,20,16,64],f16>
    %float0.000000e00_8039 = torch.constant.float 0.000000e+00
    %false_8040 = torch.constant.bool false
    %none_8041 = torch.constant.none
    %none_8042 = torch.constant.none
    %6885:2 = torch.operator "torch.aten._scaled_dot_product_flash_attention_for_cpu"(%6878, %6881, %6884, %float0.000000e00_8039, %false_8040, %none_8041, %none_8042) : (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.vtensor<[2,20,16,64],f16>, !torch.float, !torch.bool, !torch.none, !torch.none) -> (!torch.vtensor<[2,20,960,64],f16>, !torch.vtensor<[2,20,960],f32>) 
    %int1_8043 = torch.constant.int 1
    %int2_8044 = torch.constant.int 2
    %6886 = torch.aten.transpose.int %6885#0, %int1_8043, %int2_8044 : !torch.vtensor<[2,20,960,64],f16>, !torch.int, !torch.int -> !torch.vtensor<[2,960,20,64],f16>
    %int2_8045 = torch.constant.int 2
    %int-1_8046 = torch.constant.int -1
    %int1280_8047 = torch.constant.int 1280
    %6887 = torch.prim.ListConstruct %int2_8045, %int-1_8046, %int1280_8047 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6888 = torch.aten.view %6886, %6887 : !torch.vtensor<[2,960,20,64],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int5_8048 = torch.constant.int 5
    %6889 = torch.prims.convert_element_type %6888, %int5_8048 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_8049 = torch.constant.int 1920
    %int1280_8050 = torch.constant.int 1280
    %6890 = torch.prim.ListConstruct %int1920_8049, %int1280_8050 : (!torch.int, !torch.int) -> !torch.list<int>
    %6891 = torch.aten.view %6889, %6890 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16>
    %6892 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_8051 = torch.constant.int 0
    %int1_8052 = torch.constant.int 1
    %6893 = torch.aten.transpose.int %6892, %int0_8051, %int1_8052 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16>
    %6894 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.attn2.to_out.0.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_8053 = torch.constant.int 6
    %6895 = torch.prims.convert_element_type %6894, %int6_8053 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_8054 = torch.constant.int 6
    %6896 = torch.prims.convert_element_type %6891, %int6_8054 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_8055 = torch.constant.int 6
    %6897 = torch.prims.convert_element_type %6893, %int6_8055 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6898 = torch.aten.mm %6896, %6897 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_8056 = torch.constant.int 1
    %6899 = torch.aten.mul.Scalar %6898, %int1_8056 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_8057 = torch.constant.int 1
    %6900 = torch.aten.mul.Scalar %6895, %int1_8057 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_8058 = torch.constant.int 1
    %6901 = torch.aten.add.Tensor %6899, %6900, %int1_8058 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_8059 = torch.constant.int 5
    %6902 = torch.prims.convert_element_type %6901, %int5_8059 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_8060 = torch.constant.int 2
    %int960_8061 = torch.constant.int 960
    %int1280_8062 = torch.constant.int 1280
    %6903 = torch.prim.ListConstruct %int2_8060, %int960_8061, %int1280_8062 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6904 = torch.aten.view %6902, %6903 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %none_8063 = torch.constant.none
    %6905 = torch.aten.clone %6904, %none_8063 : !torch.vtensor<[2,960,1280],f16>, !torch.none -> !torch.vtensor<[2,960,1280],f16>
    %float1.000000e00_8064 = torch.constant.float 1.000000e+00
    %6906 = torch.aten.div.Scalar %6905, %float1.000000e00_8064 : !torch.vtensor<[2,960,1280],f16>, !torch.float -> !torch.vtensor<[2,960,1280],f16>
    %int1_8065 = torch.constant.int 1
    %6907 = torch.aten.add.Tensor %6906, %6843, %int1_8065 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int6_8066 = torch.constant.int 6
    %6908 = torch.prims.convert_element_type %6907, %int6_8066 : !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int2_8067 = torch.constant.int 2
    %6909 = torch.prim.ListConstruct %int2_8067 : (!torch.int) -> !torch.list<int>
    %int0_8068 = torch.constant.int 0
    %true_8069 = torch.constant.bool true
    %result0_8070, %result1_8071 = torch.aten.var_mean.correction %6908, %6909, %int0_8068, %true_8069 : !torch.vtensor<[2,960,1280],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,960,1],f32>, !torch.vtensor<[2,960,1],f32>
    %float1.000000e-05_8072 = torch.constant.float 1.000000e-05
    %int1_8073 = torch.constant.int 1
    %6910 = torch.aten.add.Scalar %result0_8070, %float1.000000e-05_8072, %int1_8073 : !torch.vtensor<[2,960,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,960,1],f32>
    %6911 = torch.aten.rsqrt %6910 : !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1],f32>
    %int1_8074 = torch.constant.int 1
    %6912 = torch.aten.sub.Tensor %6907, %result1_8071, %int1_8074 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %6913 = torch.aten.mul.Tensor %6912, %6911 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[2,960,1],f32> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16>
    %6914 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %6915 = torch.aten.mul.Tensor %6913, %6914 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16> -> !torch.vtensor<[2,960,1280],f32>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16>
    %6916 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.norm3.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_8075 = torch.constant.int 1
    %6917 = torch.aten.add.Tensor %6915, %6916, %int1_8075 : !torch.vtensor<[2,960,1280],f32>, !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f32>
    %int5_8076 = torch.constant.int 5
    %6918 = torch.prims.convert_element_type %6917, %int5_8076 : !torch.vtensor<[2,960,1280],f32>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_8077 = torch.constant.int 1920
    %int1280_8078 = torch.constant.int 1280
    %6919 = torch.prim.ListConstruct %int1920_8077, %int1280_8078 : (!torch.int, !torch.int) -> !torch.list<int>
    %6920 = torch.aten.view %6918, %6919 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16>
    %6921 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.weight : tensor<10240x1280xf16> -> !torch.vtensor<[10240,1280],f16>
    %int0_8079 = torch.constant.int 0
    %int1_8080 = torch.constant.int 1
    %6922 = torch.aten.transpose.int %6921, %int0_8079, %int1_8080 : !torch.vtensor<[10240,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,10240],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16>
    %6923 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.0.proj.bias : tensor<10240xf16> -> !torch.vtensor<[10240],f16>
    %int6_8081 = torch.constant.int 6
    %6924 = torch.prims.convert_element_type %6923, %int6_8081 : !torch.vtensor<[10240],f16>, !torch.int -> !torch.vtensor<[10240],f32>
    %int6_8082 = torch.constant.int 6
    %6925 = torch.prims.convert_element_type %6920, %int6_8082 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_8083 = torch.constant.int 6
    %6926 = torch.prims.convert_element_type %6922, %int6_8083 : !torch.vtensor<[1280,10240],f16>, !torch.int -> !torch.vtensor<[1280,10240],f32>
    %6927 = torch.aten.mm %6925, %6926 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,10240],f32> -> !torch.vtensor<[1920,10240],f32>
    %int1_8084 = torch.constant.int 1
    %6928 = torch.aten.mul.Scalar %6927, %int1_8084 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int1_8085 = torch.constant.int 1
    %6929 = torch.aten.mul.Scalar %6924, %int1_8085 : !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[10240],f32>
    %int1_8086 = torch.constant.int 1
    %6930 = torch.aten.add.Tensor %6928, %6929, %int1_8086 : !torch.vtensor<[1920,10240],f32>, !torch.vtensor<[10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f32>
    %int5_8087 = torch.constant.int 5
    %6931 = torch.prims.convert_element_type %6930, %int5_8087 : !torch.vtensor<[1920,10240],f32>, !torch.int -> !torch.vtensor<[1920,10240],f16>
    %int2_8088 = torch.constant.int 2
    %int960_8089 = torch.constant.int 960
    %int10240_8090 = torch.constant.int 10240
    %6932 = torch.prim.ListConstruct %int2_8088, %int960_8089, %int10240_8090 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6933 = torch.aten.view %6931, %6932 : !torch.vtensor<[1920,10240],f16>, !torch.list<int> -> !torch.vtensor<[2,960,10240],f16>
    %int-1_8091 = torch.constant.int -1
    %int0_8092 = torch.constant.int 0
    %int5120_8093 = torch.constant.int 5120
    %int1_8094 = torch.constant.int 1
    %6934 = torch.aten.slice.Tensor %6933, %int-1_8091, %int0_8092, %int5120_8093, %int1_8094 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %int-1_8095 = torch.constant.int -1
    %int5120_8096 = torch.constant.int 5120
    %int10240_8097 = torch.constant.int 10240
    %int1_8098 = torch.constant.int 1
    %6935 = torch.aten.slice.Tensor %6933, %int-1_8095, %int5120_8096, %int10240_8097, %int1_8098 : !torch.vtensor<[2,960,10240],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,960,5120],f16>
    %str_8099 = torch.constant.str "none"
    %6936 = torch.aten.gelu %6935, %str_8099 : !torch.vtensor<[2,960,5120],f16>, !torch.str -> !torch.vtensor<[2,960,5120],f16>
    %6937 = torch.aten.mul.Tensor %6934, %6936 : !torch.vtensor<[2,960,5120],f16>, !torch.vtensor<[2,960,5120],f16> -> !torch.vtensor<[2,960,5120],f16>
    %none_8100 = torch.constant.none
    %6938 = torch.aten.clone %6937, %none_8100 : !torch.vtensor<[2,960,5120],f16>, !torch.none -> !torch.vtensor<[2,960,5120],f16>
    %int1920_8101 = torch.constant.int 1920
    %int5120_8102 = torch.constant.int 5120
    %6939 = torch.prim.ListConstruct %int1920_8101, %int5120_8102 : (!torch.int, !torch.int) -> !torch.list<int>
    %6940 = torch.aten.view %6938, %6939 : !torch.vtensor<[2,960,5120],f16>, !torch.list<int> -> !torch.vtensor<[1920,5120],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16>
    %6941 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.weight : tensor<1280x5120xf16> -> !torch.vtensor<[1280,5120],f16>
    %int0_8103 = torch.constant.int 0
    %int1_8104 = torch.constant.int 1
    %6942 = torch.aten.transpose.int %6941, %int0_8103, %int1_8104 : !torch.vtensor<[1280,5120],f16>, !torch.int, !torch.int -> !torch.vtensor<[5120,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16>
    %6943 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.transformer_blocks.9.ff.net.2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_8105 = torch.constant.int 6
    %6944 = torch.prims.convert_element_type %6943, %int6_8105 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_8106 = torch.constant.int 6
    %6945 = torch.prims.convert_element_type %6940, %int6_8106 : !torch.vtensor<[1920,5120],f16>, !torch.int -> !torch.vtensor<[1920,5120],f32>
    %int6_8107 = torch.constant.int 6
    %6946 = torch.prims.convert_element_type %6942, %int6_8107 : !torch.vtensor<[5120,1280],f16>, !torch.int -> !torch.vtensor<[5120,1280],f32>
    %6947 = torch.aten.mm %6945, %6946 : !torch.vtensor<[1920,5120],f32>, !torch.vtensor<[5120,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_8108 = torch.constant.int 1
    %6948 = torch.aten.mul.Scalar %6947, %int1_8108 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_8109 = torch.constant.int 1
    %6949 = torch.aten.mul.Scalar %6944, %int1_8109 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_8110 = torch.constant.int 1
    %6950 = torch.aten.add.Tensor %6948, %6949, %int1_8110 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_8111 = torch.constant.int 5
    %6951 = torch.prims.convert_element_type %6950, %int5_8111 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_8112 = torch.constant.int 2
    %int960_8113 = torch.constant.int 960
    %int1280_8114 = torch.constant.int 1280
    %6952 = torch.prim.ListConstruct %int2_8112, %int960_8113, %int1280_8114 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6953 = torch.aten.view %6951, %6952 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int1_8115 = torch.constant.int 1
    %6954 = torch.aten.add.Tensor %6953, %6907, %int1_8115 : !torch.vtensor<[2,960,1280],f16>, !torch.vtensor<[2,960,1280],f16>, !torch.int -> !torch.vtensor<[2,960,1280],f16>
    %int1920_8116 = torch.constant.int 1920
    %int1280_8117 = torch.constant.int 1280
    %6955 = torch.prim.ListConstruct %int1920_8116, %int1280_8117 : (!torch.int, !torch.int) -> !torch.list<int>
    %6956 = torch.aten.view %6954, %6955 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[1920,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.proj_out.weight = util.global.load @__auto.controlnet.mid_block.attentions.0.proj_out.weight : tensor<1280x1280xf16>
    %6957 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.proj_out.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_8118 = torch.constant.int 0
    %int1_8119 = torch.constant.int 1
    %6958 = torch.aten.transpose.int %6957, %int0_8118, %int1_8119 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.attentions.0.proj_out.bias = util.global.load @__auto.controlnet.mid_block.attentions.0.proj_out.bias : tensor<1280xf16>
    %6959 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.attentions.0.proj_out.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_8120 = torch.constant.int 6
    %6960 = torch.prims.convert_element_type %6959, %int6_8120 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_8121 = torch.constant.int 6
    %6961 = torch.prims.convert_element_type %6956, %int6_8121 : !torch.vtensor<[1920,1280],f16>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int6_8122 = torch.constant.int 6
    %6962 = torch.prims.convert_element_type %6958, %int6_8122 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %6963 = torch.aten.mm %6961, %6962 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[1920,1280],f32>
    %int1_8123 = torch.constant.int 1
    %6964 = torch.aten.mul.Scalar %6963, %int1_8123 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int1_8124 = torch.constant.int 1
    %6965 = torch.aten.mul.Scalar %6960, %int1_8124 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_8125 = torch.constant.int 1
    %6966 = torch.aten.add.Tensor %6964, %6965, %int1_8125 : !torch.vtensor<[1920,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f32>
    %int5_8126 = torch.constant.int 5
    %6967 = torch.prims.convert_element_type %6966, %int5_8126 : !torch.vtensor<[1920,1280],f32>, !torch.int -> !torch.vtensor<[1920,1280],f16>
    %int2_8127 = torch.constant.int 2
    %int960_8128 = torch.constant.int 960
    %int1280_8129 = torch.constant.int 1280
    %6968 = torch.prim.ListConstruct %int2_8127, %int960_8128, %int1280_8129 : (!torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6969 = torch.aten.view %6967, %6968 : !torch.vtensor<[1920,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,960,1280],f16>
    %int2_8130 = torch.constant.int 2
    %int30_8131 = torch.constant.int 30
    %int32_8132 = torch.constant.int 32
    %int1280_8133 = torch.constant.int 1280
    %6970 = torch.prim.ListConstruct %int2_8130, %int30_8131, %int32_8132, %int1280_8133 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6971 = torch.aten.view %6969, %6970 : !torch.vtensor<[2,960,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,30,32,1280],f16>
    %int0_8134 = torch.constant.int 0
    %int3_8135 = torch.constant.int 3
    %int1_8136 = torch.constant.int 1
    %int2_8137 = torch.constant.int 2
    %6972 = torch.prim.ListConstruct %int0_8134, %int3_8135, %int1_8136, %int2_8137 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6973 = torch.aten.permute %6971, %6972 : !torch.vtensor<[2,30,32,1280],f16>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f16>
    %int0_8138 = torch.constant.int 0
    %6974 = torch.aten.clone %6973, %int0_8138 : !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_8139 = torch.constant.int 1
    %6975 = torch.aten.add.Tensor %6974, %5169, %int1_8139 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_8140 = torch.constant.int 2
    %int32_8141 = torch.constant.int 32
    %int40_8142 = torch.constant.int 40
    %int960_8143 = torch.constant.int 960
    %6976 = torch.prim.ListConstruct %int2_8140, %int32_8141, %int40_8142, %int960_8143 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6977 = torch.aten.view %6975, %6976 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_8144 = torch.constant.int 6
    %6978 = torch.prims.convert_element_type %6977, %int6_8144 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_8145 = torch.constant.int 2
    %int3_8146 = torch.constant.int 3
    %6979 = torch.prim.ListConstruct %int2_8145, %int3_8146 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8147 = torch.constant.int 0
    %true_8148 = torch.constant.bool true
    %result0_8149, %result1_8150 = torch.aten.var_mean.correction %6978, %6979, %int0_8147, %true_8148 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_8151 = torch.constant.float 1.000000e-05
    %int1_8152 = torch.constant.int 1
    %6980 = torch.aten.add.Scalar %result0_8149, %float1.000000e-05_8151, %int1_8152 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %6981 = torch.aten.rsqrt %6980 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_8153 = torch.constant.int 1
    %6982 = torch.aten.sub.Tensor %6977, %result1_8150, %int1_8153 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %6983 = torch.aten.mul.Tensor %6982, %6981 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_8154 = torch.constant.int 2
    %int1280_8155 = torch.constant.int 1280
    %int30_8156 = torch.constant.int 30
    %int32_8157 = torch.constant.int 32
    %6984 = torch.prim.ListConstruct %int2_8154, %int1280_8155, %int30_8156, %int32_8157 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %6985 = torch.aten.view %6983, %6984 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.mid_block.resnets.1.norm1.bias = util.global.load @__auto.controlnet.mid_block.resnets.1.norm1.bias : tensor<1280xf16>
    %6986 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.norm1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_8158 = torch.constant.int 0
    %6987 = torch.aten.unsqueeze %6986, %int0_8158 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_8159 = torch.constant.int 2
    %6988 = torch.aten.unsqueeze %6987, %int2_8159 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_8160 = torch.constant.int 3
    %6989 = torch.aten.unsqueeze %6988, %int3_8160 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.mid_block.resnets.1.norm1.weight = util.global.load @__auto.controlnet.mid_block.resnets.1.norm1.weight : tensor<1280xf16>
    %6990 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.norm1.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_8161 = torch.constant.int 0
    %6991 = torch.aten.unsqueeze %6990, %int0_8161 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_8162 = torch.constant.int 2
    %6992 = torch.aten.unsqueeze %6991, %int2_8162 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_8163 = torch.constant.int 3
    %6993 = torch.aten.unsqueeze %6992, %int3_8163 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %6994 = torch.aten.mul.Tensor %6985, %6993 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_8164 = torch.constant.int 1
    %6995 = torch.aten.add.Tensor %6994, %6989, %int1_8164 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_8165 = torch.constant.int 5
    %6996 = torch.prims.convert_element_type %6995, %int5_8165 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %6997 = torch.aten.silu %6996 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.mid_block.resnets.1.conv1.weight = util.global.load @__auto.controlnet.mid_block.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16>
    %6998 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.conv1.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.mid_block.resnets.1.conv1.bias = util.global.load @__auto.controlnet.mid_block.resnets.1.conv1.bias : tensor<1280xf16>
    %6999 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.conv1.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_8166 = torch.constant.int 1
    %int1_8167 = torch.constant.int 1
    %7000 = torch.prim.ListConstruct %int1_8166, %int1_8167 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8168 = torch.constant.int 1
    %int1_8169 = torch.constant.int 1
    %7001 = torch.prim.ListConstruct %int1_8168, %int1_8169 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8170 = torch.constant.int 1
    %int1_8171 = torch.constant.int 1
    %7002 = torch.prim.ListConstruct %int1_8170, %int1_8171 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8172 = torch.constant.bool false
    %int0_8173 = torch.constant.int 0
    %int0_8174 = torch.constant.int 0
    %7003 = torch.prim.ListConstruct %int0_8173, %int0_8174 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8175 = torch.constant.int 1
    %7004 = torch.aten.convolution %6997, %6998, %6999, %7000, %7001, %7002, %false_8172, %7003, %int1_8175 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %7005 = torch.aten.silu %100 : !torch.vtensor<[2,1280],f16> -> !torch.vtensor<[2,1280],f16>
    %__auto.controlnet.mid_block.resnets.1.time_emb_proj.weight = util.global.load @__auto.controlnet.mid_block.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16>
    %7006 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.time_emb_proj.weight : tensor<1280x1280xf16> -> !torch.vtensor<[1280,1280],f16>
    %int0_8176 = torch.constant.int 0
    %int1_8177 = torch.constant.int 1
    %7007 = torch.aten.transpose.int %7006, %int0_8176, %int1_8177 : !torch.vtensor<[1280,1280],f16>, !torch.int, !torch.int -> !torch.vtensor<[1280,1280],f16>
    %__auto.controlnet.mid_block.resnets.1.time_emb_proj.bias = util.global.load @__auto.controlnet.mid_block.resnets.1.time_emb_proj.bias : tensor<1280xf16>
    %7008 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.time_emb_proj.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int6_8178 = torch.constant.int 6
    %7009 = torch.prims.convert_element_type %7008, %int6_8178 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1280],f32>
    %int6_8179 = torch.constant.int 6
    %7010 = torch.prims.convert_element_type %7005, %int6_8179 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int6_8180 = torch.constant.int 6
    %7011 = torch.prims.convert_element_type %7007, %int6_8180 : !torch.vtensor<[1280,1280],f16>, !torch.int -> !torch.vtensor<[1280,1280],f32>
    %7012 = torch.aten.mm %7010, %7011 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280,1280],f32> -> !torch.vtensor<[2,1280],f32>
    %int1_8181 = torch.constant.int 1
    %7013 = torch.aten.mul.Scalar %7012, %int1_8181 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int1_8182 = torch.constant.int 1
    %7014 = torch.aten.mul.Scalar %7009, %int1_8182 : !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[1280],f32>
    %int1_8183 = torch.constant.int 1
    %7015 = torch.aten.add.Tensor %7013, %7014, %int1_8183 : !torch.vtensor<[2,1280],f32>, !torch.vtensor<[1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f32>
    %int5_8184 = torch.constant.int 5
    %7016 = torch.prims.convert_element_type %7015, %int5_8184 : !torch.vtensor<[2,1280],f32>, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int0_8185 = torch.constant.int 0
    %int0_8186 = torch.constant.int 0
    %int9223372036854775807_8187 = torch.constant.int 9223372036854775807
    %int1_8188 = torch.constant.int 1
    %7017 = torch.aten.slice.Tensor %7016, %int0_8185, %int0_8186, %int9223372036854775807_8187, %int1_8188 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int1_8189 = torch.constant.int 1
    %int0_8190 = torch.constant.int 0
    %int9223372036854775807_8191 = torch.constant.int 9223372036854775807
    %int1_8192 = torch.constant.int 1
    %7018 = torch.aten.slice.Tensor %7017, %int1_8189, %int0_8190, %int9223372036854775807_8191, %int1_8192 : !torch.vtensor<[2,1280],f16>, !torch.int, !torch.int, !torch.int, !torch.int -> !torch.vtensor<[2,1280],f16>
    %int2_8193 = torch.constant.int 2
    %7019 = torch.aten.unsqueeze %7018, %int2_8193 : !torch.vtensor<[2,1280],f16>, !torch.int -> !torch.vtensor<[2,1280,1],f16>
    %int3_8194 = torch.constant.int 3
    %7020 = torch.aten.unsqueeze %7019, %int3_8194 : !torch.vtensor<[2,1280,1],f16>, !torch.int -> !torch.vtensor<[2,1280,1,1],f16>
    %int1_8195 = torch.constant.int 1
    %7021 = torch.aten.add.Tensor %7004, %7020, %int1_8195 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int2_8196 = torch.constant.int 2
    %int32_8197 = torch.constant.int 32
    %int40_8198 = torch.constant.int 40
    %int960_8199 = torch.constant.int 960
    %7022 = torch.prim.ListConstruct %int2_8196, %int32_8197, %int40_8198, %int960_8199 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7023 = torch.aten.view %7021, %7022 : !torch.vtensor<[2,1280,30,32],f16>, !torch.list<int> -> !torch.vtensor<[2,32,40,960],f16>
    %int6_8200 = torch.constant.int 6
    %7024 = torch.prims.convert_element_type %7023, %int6_8200 : !torch.vtensor<[2,32,40,960],f16>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %int2_8201 = torch.constant.int 2
    %int3_8202 = torch.constant.int 3
    %7025 = torch.prim.ListConstruct %int2_8201, %int3_8202 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8203 = torch.constant.int 0
    %true_8204 = torch.constant.bool true
    %result0_8205, %result1_8206 = torch.aten.var_mean.correction %7024, %7025, %int0_8203, %true_8204 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int>, !torch.int, !torch.bool -> !torch.vtensor<[2,32,1,1],f32>, !torch.vtensor<[2,32,1,1],f32>
    %float1.000000e-05_8207 = torch.constant.float 1.000000e-05
    %int1_8208 = torch.constant.int 1
    %7026 = torch.aten.add.Scalar %result0_8205, %float1.000000e-05_8207, %int1_8208 : !torch.vtensor<[2,32,1,1],f32>, !torch.float, !torch.int -> !torch.vtensor<[2,32,1,1],f32>
    %7027 = torch.aten.rsqrt %7026 : !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,1,1],f32>
    %int1_8209 = torch.constant.int 1
    %7028 = torch.aten.sub.Tensor %7023, %result1_8206, %int1_8209 : !torch.vtensor<[2,32,40,960],f16>, !torch.vtensor<[2,32,1,1],f32>, !torch.int -> !torch.vtensor<[2,32,40,960],f32>
    %7029 = torch.aten.mul.Tensor %7028, %7027 : !torch.vtensor<[2,32,40,960],f32>, !torch.vtensor<[2,32,1,1],f32> -> !torch.vtensor<[2,32,40,960],f32>
    %int2_8210 = torch.constant.int 2
    %int1280_8211 = torch.constant.int 1280
    %int30_8212 = torch.constant.int 30
    %int32_8213 = torch.constant.int 32
    %7030 = torch.prim.ListConstruct %int2_8210, %int1280_8211, %int30_8212, %int32_8213 : (!torch.int, !torch.int, !torch.int, !torch.int) -> !torch.list<int>
    %7031 = torch.aten.view %7029, %7030 : !torch.vtensor<[2,32,40,960],f32>, !torch.list<int> -> !torch.vtensor<[2,1280,30,32],f32>
    %__auto.controlnet.mid_block.resnets.1.norm2.bias = util.global.load @__auto.controlnet.mid_block.resnets.1.norm2.bias : tensor<1280xf16>
    %7032 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.norm2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_8214 = torch.constant.int 0
    %7033 = torch.aten.unsqueeze %7032, %int0_8214 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_8215 = torch.constant.int 2
    %7034 = torch.aten.unsqueeze %7033, %int2_8215 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_8216 = torch.constant.int 3
    %7035 = torch.aten.unsqueeze %7034, %int3_8216 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %__auto.controlnet.mid_block.resnets.1.norm2.weight = util.global.load @__auto.controlnet.mid_block.resnets.1.norm2.weight : tensor<1280xf16>
    %7036 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.norm2.weight : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int0_8217 = torch.constant.int 0
    %7037 = torch.aten.unsqueeze %7036, %int0_8217 : !torch.vtensor<[1280],f16>, !torch.int -> !torch.vtensor<[1,1280],f16>
    %int2_8218 = torch.constant.int 2
    %7038 = torch.aten.unsqueeze %7037, %int2_8218 : !torch.vtensor<[1,1280],f16>, !torch.int -> !torch.vtensor<[1,1280,1],f16>
    %int3_8219 = torch.constant.int 3
    %7039 = torch.aten.unsqueeze %7038, %int3_8219 : !torch.vtensor<[1,1280,1],f16>, !torch.int -> !torch.vtensor<[1,1280,1,1],f16>
    %7040 = torch.aten.mul.Tensor %7031, %7039 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16> -> !torch.vtensor<[2,1280,30,32],f32>
    %int1_8220 = torch.constant.int 1
    %7041 = torch.aten.add.Tensor %7040, %7035, %int1_8220 : !torch.vtensor<[2,1280,30,32],f32>, !torch.vtensor<[1,1280,1,1],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f32>
    %int5_8221 = torch.constant.int 5
    %7042 = torch.prims.convert_element_type %7041, %int5_8221 : !torch.vtensor<[2,1280,30,32],f32>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %7043 = torch.aten.silu %7042 : !torch.vtensor<[2,1280,30,32],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %none_8222 = torch.constant.none
    %7044 = torch.aten.clone %7043, %none_8222 : !torch.vtensor<[2,1280,30,32],f16>, !torch.none -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.mid_block.resnets.1.conv2.weight = util.global.load @__auto.controlnet.mid_block.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16>
    %7045 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.conv2.weight : tensor<1280x1280x3x3xf16> -> !torch.vtensor<[1280,1280,3,3],f16>
    %__auto.controlnet.mid_block.resnets.1.conv2.bias = util.global.load @__auto.controlnet.mid_block.resnets.1.conv2.bias : tensor<1280xf16>
    %7046 = torch_c.from_builtin_tensor %__auto.controlnet.mid_block.resnets.1.conv2.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_8223 = torch.constant.int 1
    %int1_8224 = torch.constant.int 1
    %7047 = torch.prim.ListConstruct %int1_8223, %int1_8224 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8225 = torch.constant.int 1
    %int1_8226 = torch.constant.int 1
    %7048 = torch.prim.ListConstruct %int1_8225, %int1_8226 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8227 = torch.constant.int 1
    %int1_8228 = torch.constant.int 1
    %7049 = torch.prim.ListConstruct %int1_8227, %int1_8228 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8229 = torch.constant.bool false
    %int0_8230 = torch.constant.int 0
    %int0_8231 = torch.constant.int 0
    %7050 = torch.prim.ListConstruct %int0_8230, %int0_8231 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8232 = torch.constant.int 1
    %7051 = torch.aten.convolution %7044, %7045, %7046, %7047, %7048, %7049, %false_8229, %7050, %int1_8232 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,3,3],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_8233 = torch.constant.int 1
    %7052 = torch.aten.add.Tensor %6975, %7051, %int1_8233 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %int1_8234 = torch.constant.int 1
    %7053 = torch.aten.div.Scalar %7052, %int1_8234 : !torch.vtensor<[2,1280,30,32],f16>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.controlnet_down_blocks.0.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.0.weight : tensor<320x320x1x1xf16>
    %7054 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.0.weight : tensor<320x320x1x1xf16> -> !torch.vtensor<[320,320,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.0.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.0.bias : tensor<320xf16>
    %7055 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.0.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_8235 = torch.constant.int 1
    %int1_8236 = torch.constant.int 1
    %7056 = torch.prim.ListConstruct %int1_8235, %int1_8236 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8237 = torch.constant.int 0
    %int0_8238 = torch.constant.int 0
    %7057 = torch.prim.ListConstruct %int0_8237, %int0_8238 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8239 = torch.constant.int 1
    %int1_8240 = torch.constant.int 1
    %7058 = torch.prim.ListConstruct %int1_8239, %int1_8240 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8241 = torch.constant.bool false
    %int0_8242 = torch.constant.int 0
    %int0_8243 = torch.constant.int 0
    %7059 = torch.prim.ListConstruct %int0_8242, %int0_8243 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8244 = torch.constant.int 1
    %7060 = torch.aten.convolution %171, %7054, %7055, %7056, %7057, %7058, %false_8241, %7059, %int1_8244 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.controlnet_down_blocks.1.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.1.weight : tensor<320x320x1x1xf16>
    %7061 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.1.weight : tensor<320x320x1x1xf16> -> !torch.vtensor<[320,320,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.1.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.1.bias : tensor<320xf16>
    %7062 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.1.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_8245 = torch.constant.int 1
    %int1_8246 = torch.constant.int 1
    %7063 = torch.prim.ListConstruct %int1_8245, %int1_8246 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8247 = torch.constant.int 0
    %int0_8248 = torch.constant.int 0
    %7064 = torch.prim.ListConstruct %int0_8247, %int0_8248 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8249 = torch.constant.int 1
    %int1_8250 = torch.constant.int 1
    %7065 = torch.prim.ListConstruct %int1_8249, %int1_8250 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8251 = torch.constant.bool false
    %int0_8252 = torch.constant.int 0
    %int0_8253 = torch.constant.int 0
    %7066 = torch.prim.ListConstruct %int0_8252, %int0_8253 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8254 = torch.constant.int 1
    %7067 = torch.aten.convolution %249, %7061, %7062, %7063, %7064, %7065, %false_8251, %7066, %int1_8254 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.controlnet_down_blocks.2.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.2.weight : tensor<320x320x1x1xf16>
    %7068 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.2.weight : tensor<320x320x1x1xf16> -> !torch.vtensor<[320,320,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.2.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.2.bias : tensor<320xf16>
    %7069 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.2.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_8255 = torch.constant.int 1
    %int1_8256 = torch.constant.int 1
    %7070 = torch.prim.ListConstruct %int1_8255, %int1_8256 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8257 = torch.constant.int 0
    %int0_8258 = torch.constant.int 0
    %7071 = torch.prim.ListConstruct %int0_8257, %int0_8258 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8259 = torch.constant.int 1
    %int1_8260 = torch.constant.int 1
    %7072 = torch.prim.ListConstruct %int1_8259, %int1_8260 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8261 = torch.constant.bool false
    %int0_8262 = torch.constant.int 0
    %int0_8263 = torch.constant.int 0
    %7073 = torch.prim.ListConstruct %int0_8262, %int0_8263 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8264 = torch.constant.int 1
    %7074 = torch.aten.convolution %327, %7068, %7069, %7070, %7071, %7072, %false_8261, %7073, %int1_8264 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[320,320,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,120,128],f16>
    %__auto.controlnet.controlnet_down_blocks.3.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.3.weight : tensor<320x320x1x1xf16>
    %7075 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.3.weight : tensor<320x320x1x1xf16> -> !torch.vtensor<[320,320,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.3.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.3.bias : tensor<320xf16>
    %7076 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.3.bias : tensor<320xf16> -> !torch.vtensor<[320],f16>
    %int1_8265 = torch.constant.int 1
    %int1_8266 = torch.constant.int 1
    %7077 = torch.prim.ListConstruct %int1_8265, %int1_8266 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8267 = torch.constant.int 0
    %int0_8268 = torch.constant.int 0
    %7078 = torch.prim.ListConstruct %int0_8267, %int0_8268 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8269 = torch.constant.int 1
    %int1_8270 = torch.constant.int 1
    %7079 = torch.prim.ListConstruct %int1_8269, %int1_8270 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8271 = torch.constant.bool false
    %int0_8272 = torch.constant.int 0
    %int0_8273 = torch.constant.int 0
    %7080 = torch.prim.ListConstruct %int0_8272, %int0_8273 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8274 = torch.constant.int 1
    %7081 = torch.aten.convolution %334, %7075, %7076, %7077, %7078, %7079, %false_8271, %7080, %int1_8274 : !torch.vtensor<[2,320,60,64],f16>, !torch.vtensor<[320,320,1,1],f16>, !torch.vtensor<[320],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,320,60,64],f16>
    %__auto.controlnet.controlnet_down_blocks.4.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.4.weight : tensor<640x640x1x1xf16>
    %7082 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.4.weight : tensor<640x640x1x1xf16> -> !torch.vtensor<[640,640,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.4.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.4.bias : tensor<640xf16>
    %7083 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.4.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_8275 = torch.constant.int 1
    %int1_8276 = torch.constant.int 1
    %7084 = torch.prim.ListConstruct %int1_8275, %int1_8276 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8277 = torch.constant.int 0
    %int0_8278 = torch.constant.int 0
    %7085 = torch.prim.ListConstruct %int0_8277, %int0_8278 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8279 = torch.constant.int 1
    %int1_8280 = torch.constant.int 1
    %7086 = torch.prim.ListConstruct %int1_8279, %int1_8280 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8281 = torch.constant.bool false
    %int0_8282 = torch.constant.int 0
    %int0_8283 = torch.constant.int 0
    %7087 = torch.prim.ListConstruct %int0_8282, %int0_8283 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8284 = torch.constant.int 1
    %7088 = torch.aten.convolution %825, %7082, %7083, %7084, %7085, %7086, %false_8281, %7087, %int1_8284 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[640,640,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.controlnet_down_blocks.5.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.5.weight : tensor<640x640x1x1xf16>
    %7089 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.5.weight : tensor<640x640x1x1xf16> -> !torch.vtensor<[640,640,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.5.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.5.bias : tensor<640xf16>
    %7090 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.5.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_8285 = torch.constant.int 1
    %int1_8286 = torch.constant.int 1
    %7091 = torch.prim.ListConstruct %int1_8285, %int1_8286 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8287 = torch.constant.int 0
    %int0_8288 = torch.constant.int 0
    %7092 = torch.prim.ListConstruct %int0_8287, %int0_8288 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8289 = torch.constant.int 1
    %int1_8290 = torch.constant.int 1
    %7093 = torch.prim.ListConstruct %int1_8289, %int1_8290 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8291 = torch.constant.bool false
    %int0_8292 = torch.constant.int 0
    %int0_8293 = torch.constant.int 0
    %7094 = torch.prim.ListConstruct %int0_8292, %int0_8293 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8294 = torch.constant.int 1
    %7095 = torch.aten.convolution %1309, %7089, %7090, %7091, %7092, %7093, %false_8291, %7094, %int1_8294 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[640,640,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,60,64],f16>
    %__auto.controlnet.controlnet_down_blocks.6.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.6.weight : tensor<640x640x1x1xf16>
    %7096 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.6.weight : tensor<640x640x1x1xf16> -> !torch.vtensor<[640,640,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.6.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.6.bias : tensor<640xf16>
    %7097 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.6.bias : tensor<640xf16> -> !torch.vtensor<[640],f16>
    %int1_8295 = torch.constant.int 1
    %int1_8296 = torch.constant.int 1
    %7098 = torch.prim.ListConstruct %int1_8295, %int1_8296 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8297 = torch.constant.int 0
    %int0_8298 = torch.constant.int 0
    %7099 = torch.prim.ListConstruct %int0_8297, %int0_8298 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8299 = torch.constant.int 1
    %int1_8300 = torch.constant.int 1
    %7100 = torch.prim.ListConstruct %int1_8299, %int1_8300 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8301 = torch.constant.bool false
    %int0_8302 = torch.constant.int 0
    %int0_8303 = torch.constant.int 0
    %7101 = torch.prim.ListConstruct %int0_8302, %int0_8303 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8304 = torch.constant.int 1
    %7102 = torch.aten.convolution %1316, %7096, %7097, %7098, %7099, %7100, %false_8301, %7101, %int1_8304 : !torch.vtensor<[2,640,30,32],f16>, !torch.vtensor<[640,640,1,1],f16>, !torch.vtensor<[640],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,640,30,32],f16>
    %__auto.controlnet.controlnet_down_blocks.7.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.7.weight : tensor<1280x1280x1x1xf16>
    %7103 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.7.weight : tensor<1280x1280x1x1xf16> -> !torch.vtensor<[1280,1280,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.7.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.7.bias : tensor<1280xf16>
    %7104 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.7.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_8305 = torch.constant.int 1
    %int1_8306 = torch.constant.int 1
    %7105 = torch.prim.ListConstruct %int1_8305, %int1_8306 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8307 = torch.constant.int 0
    %int0_8308 = torch.constant.int 0
    %7106 = torch.prim.ListConstruct %int0_8307, %int0_8308 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8309 = torch.constant.int 1
    %int1_8310 = torch.constant.int 1
    %7107 = torch.prim.ListConstruct %int1_8309, %int1_8310 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8311 = torch.constant.bool false
    %int0_8312 = torch.constant.int 0
    %int0_8313 = torch.constant.int 0
    %7108 = torch.prim.ListConstruct %int0_8312, %int0_8313 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8314 = torch.constant.int 1
    %7109 = torch.aten.convolution %3207, %7103, %7104, %7105, %7106, %7107, %false_8311, %7108, %int1_8314 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.controlnet_down_blocks.8.weight = util.global.load @__auto.controlnet.controlnet_down_blocks.8.weight : tensor<1280x1280x1x1xf16>
    %7110 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.8.weight : tensor<1280x1280x1x1xf16> -> !torch.vtensor<[1280,1280,1,1],f16>
    %__auto.controlnet.controlnet_down_blocks.8.bias = util.global.load @__auto.controlnet.controlnet_down_blocks.8.bias : tensor<1280xf16>
    %7111 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_down_blocks.8.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_8315 = torch.constant.int 1
    %int1_8316 = torch.constant.int 1
    %7112 = torch.prim.ListConstruct %int1_8315, %int1_8316 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8317 = torch.constant.int 0
    %int0_8318 = torch.constant.int 0
    %7113 = torch.prim.ListConstruct %int0_8317, %int0_8318 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8319 = torch.constant.int 1
    %int1_8320 = torch.constant.int 1
    %7114 = torch.prim.ListConstruct %int1_8319, %int1_8320 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8321 = torch.constant.bool false
    %int0_8322 = torch.constant.int 0
    %int0_8323 = torch.constant.int 0
    %7115 = torch.prim.ListConstruct %int0_8322, %int0_8323 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8324 = torch.constant.int 1
    %7116 = torch.aten.convolution %5091, %7110, %7111, %7112, %7113, %7114, %false_8321, %7115, %int1_8324 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %__auto.controlnet.controlnet_mid_block.weight = util.global.load @__auto.controlnet.controlnet_mid_block.weight : tensor<1280x1280x1x1xf16>
    %7117 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_mid_block.weight : tensor<1280x1280x1x1xf16> -> !torch.vtensor<[1280,1280,1,1],f16>
    %__auto.controlnet.controlnet_mid_block.bias = util.global.load @__auto.controlnet.controlnet_mid_block.bias : tensor<1280xf16>
    %7118 = torch_c.from_builtin_tensor %__auto.controlnet.controlnet_mid_block.bias : tensor<1280xf16> -> !torch.vtensor<[1280],f16>
    %int1_8325 = torch.constant.int 1
    %int1_8326 = torch.constant.int 1
    %7119 = torch.prim.ListConstruct %int1_8325, %int1_8326 : (!torch.int, !torch.int) -> !torch.list<int>
    %int0_8327 = torch.constant.int 0
    %int0_8328 = torch.constant.int 0
    %7120 = torch.prim.ListConstruct %int0_8327, %int0_8328 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8329 = torch.constant.int 1
    %int1_8330 = torch.constant.int 1
    %7121 = torch.prim.ListConstruct %int1_8329, %int1_8330 : (!torch.int, !torch.int) -> !torch.list<int>
    %false_8331 = torch.constant.bool false
    %int0_8332 = torch.constant.int 0
    %int0_8333 = torch.constant.int 0
    %7122 = torch.prim.ListConstruct %int0_8332, %int0_8333 : (!torch.int, !torch.int) -> !torch.list<int>
    %int1_8334 = torch.constant.int 1
    %7123 = torch.aten.convolution %7053, %7117, %7118, %7119, %7120, %7121, %false_8331, %7122, %int1_8334 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1280,1280,1,1],f16>, !torch.vtensor<[1280],f16>, !torch.list<int>, !torch.list<int>, !torch.list<int>, !torch.bool, !torch.list<int>, !torch.int -> !torch.vtensor<[2,1280,30,32],f16>
    %7124 = torch.aten.mul.Tensor %7060, %arg7 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %7125 = torch.aten.mul.Tensor %7067, %arg7 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %7126 = torch.aten.mul.Tensor %7074, %arg7 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,320,120,128],f16>
    %7127 = torch.aten.mul.Tensor %7081, %arg7 : !torch.vtensor<[2,320,60,64],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,320,60,64],f16>
    %7128 = torch.aten.mul.Tensor %7088, %arg7 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,640,60,64],f16>
    %7129 = torch.aten.mul.Tensor %7095, %arg7 : !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,640,60,64],f16>
    %7130 = torch.aten.mul.Tensor %7102, %arg7 : !torch.vtensor<[2,640,30,32],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,640,30,32],f16>
    %7131 = torch.aten.mul.Tensor %7109, %arg7 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %7132 = torch.aten.mul.Tensor %7116, %arg7 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    %7133 = torch.aten.mul.Tensor %7123, %arg7 : !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[1],f16> -> !torch.vtensor<[2,1280,30,32],f16>
    return %7124, %7125, %7126, %7127, %7128, %7129, %7130, %7131, %7132, %7133 : !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,120,128],f16>, !torch.vtensor<[2,320,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,60,64],f16>, !torch.vtensor<[2,640,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>, !torch.vtensor<[2,1280,30,32],f16>
  }
}
